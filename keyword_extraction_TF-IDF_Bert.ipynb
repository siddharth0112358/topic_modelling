{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/keyword-extraction-python-tf-idf-textrank-topicrank-yake-bert-7405d51cd839"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install trafilatura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install summa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/LIAAD/yake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keyBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sdeshpande/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/sdeshpande/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/sdeshpande/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We extract the articles' texts from each webpage and perform some basic text cleaning. We take only the first 5000 characters.\n",
    "import trafilatura\n",
    "array_links = [\n",
    "    \"https://en.wikipedia.org/wiki/Coronavirus_disease_2019\", \n",
    "    \"https://en.wikipedia.org/wiki/Recession\", \n",
    "    \"https://en.wikipedia.org/wiki/Vienna\", \n",
    "    \"https://en.wikipedia.org/wiki/Machine_learning\", \n",
    "    \"https://en.wikipedia.org/wiki/Graph_database\"\n",
    "]\n",
    "array_text = []\n",
    "for l in array_links:\n",
    "    html = trafilatura.fetch_url(l)\n",
    "    text = trafilatura.extract(html)\n",
    "    text_clean = text.replace(\"\\n\", \" \").replace(\"\\'\", \"\")\n",
    "    array_text.append(text_clean[0:5000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keywords Extraction with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "from tqdm.notebook import tqdm\n",
    "from re import sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_lines = sum(1 for line in open(\"/Users/sdeshpande/Desktop/text_analysis_scripts/SmartDataAnalytics-Wikipedia_TF_IDF_Dataset-a614a46/en/wiki_tfidf_terms.csv\"))\n",
    "with open(\"/Users/sdeshpande/Desktop/text_analysis_scripts/SmartDataAnalytics-Wikipedia_TF_IDF_Dataset-a614a46/en/wiki_tfidf_terms.csv\") as file:\n",
    "    dict_idf = {}\n",
    "    with tqdm(total=num_lines) as pbar:\n",
    "        for i, line in tqdm(islice(enumerate(file), 1, None)):\n",
    "            try: \n",
    "                cells = line.split(\",\")\n",
    "                idf = float(sub(\"[^0-9.]\", \"\", cells[3]))\n",
    "                dict_idf[cells[0]] = idf\n",
    "            except: \n",
    "                print(\"Error on: \" + line)\n",
    "            finally:\n",
    "                pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from numpy import array, log\n",
    "vectorizer = CountVectorizer()\n",
    "tf = vectorizer.fit_transform([x.lower() for x in array_text])\n",
    "tf = tf.toarray()\n",
    "tf = log(tf + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = tf.copy()\n",
    "words = array(vectorizer.get_feature_names())\n",
    "for k in tqdm(dict_idf.keys()):\n",
    "    if k in words:\n",
    "        tfidf[:, words == k] = tfidf[:, words == k] * dict_idf[k]\n",
    "    pbar.update(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(tfidf.shape[0]):\n",
    "    print(\"Keywords of article\", str(j+1), words[tfidf[j, :].argsort()[-5:][::-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keywords Extraction with TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from summa import keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords of article 1 \n",
      " ['symptoms', 'disease', 'covid', 'syndrome', 'fever']\n",
      "Keywords of article 2 \n",
      " ['recession', 'recessions', 'economics', 'economic', 'policies', 'policy', 'governments', 'government', 'shapes', 'shape']\n",
      "Keywords of article 3 \n",
      " ['vienna', 'viennas', 'city', 'cities', 'citys', 'mi', 'rank', 'ranked', 'cultural', 'culture']\n",
      "Keywords of article 4 \n",
      " ['data', 'algorithms', 'algorithm', 'tasks', 'task', 'computational', 'computers', 'computing', 'problems', 'problem']\n",
      "Keywords of article 5 \n",
      " ['graphs', 'graph database', 'databases', 'model', 'models', 'data', 'relates', 'relational', 'relation']\n"
     ]
    }
   ],
   "source": [
    "for j in range(len(array_text)):\n",
    "    print(\"Keywords of article\", str(j+1), \"\\n\", (keywords.keywords(array_text[j], words=5)).split(\"\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keywords Extraction with YAKE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yake import KeywordExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords of article 1 \n",
      " ['symptoms', 'pneumonia', 'coronavirus', 'people', 'syndrome']\n",
      "Keywords of article 2 \n",
      " ['economic', 'recession', 'recessions', 'gdp', 'united']\n",
      "Keywords of article 3 \n",
      " ['vienna', 'city', 'state', 'capital', 'german']\n",
      "Keywords of article 4 \n",
      " ['learning', 'machine', 'computer', 'tasks', 'computers']\n",
      "Keywords of article 5 \n",
      " ['graph', 'databases', 'database', 'data', 'relationships']\n"
     ]
    }
   ],
   "source": [
    "kw_extractor = KeywordExtractor(lan=\"en\", n=1, top=5)\n",
    "for j in range(len(array_text)):\n",
    "    keywords = kw_extractor.extract_keywords(text=array_text[j])\n",
    "    keywords = [x for x, y in keywords]\n",
    "    print(\"Keywords of article\", str(j+1), \"\\n\", keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keywords Extraction with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords of article 1 \n",
      " ['coronavirus', 'pneumonia', 'vaccines', 'virus', '2019']\n",
      "Keywords of article 2 \n",
      " ['recessions', 'recession', 'unemployment', 'pandemic', 'worsening']\n",
      "Keywords of article 3 \n",
      " ['austrias', 'vienna', 'austria', 'austrian', 'viennas']\n",
      "Keywords of article 4 \n",
      " ['algorithms', 'algorithm', 'computational', 'computers', 'mathematical']\n",
      "Keywords of article 5 \n",
      " ['graphs', 'graph', 'databases', 'web', 'database']\n"
     ]
    }
   ],
   "source": [
    "from keybert import KeyBERT\n",
    "kw_extractor = KeyBERT('distilbert-base-nli-mean-tokens')\n",
    "for j in range(len(array_text)):\n",
    "    keywords = kw_extractor.extract_keywords(array_text[j], keyphrase_length=1, stop_words='english')\n",
    "    print(\"Keywords of article\", str(j+1), \"\\n\", keywords)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
