{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download basics for BERT topic modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.utils import check_array\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "\n",
    "class ClassTFIDF(TfidfTransformer):\n",
    "    \"\"\"\n",
    "    A Class-based TF-IDF procedure using scikit-learns TfidfTransformer as a base.\n",
    "    ![](../img/ctfidf.png)\n",
    "    C-TF-IDF can best be explained as a TF-IDF formula adopted for multiple classes\n",
    "    by joining all documents per class. Thus, each class is converted to a single document\n",
    "    instead of set of documents. Then, the frequency of words **t** are extracted for\n",
    "    each class **i** and divided by the total number of words **w**.\n",
    "    Next, the total, unjoined, number of documents across all classes **m** is divided by the total\n",
    "    sum of word **i** across all classes.\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(ClassTFIDF, self).__init__(*args, **kwargs)\n",
    "        self.df = None\n",
    "\n",
    "    def fit(self, X: sp.csr_matrix, n_samples: int):\n",
    "        \"\"\"Learn the idf vector (global term weights).\n",
    "        Arguments:\n",
    "            X: A matrix of term/token counts.\n",
    "            n_samples: Number of total documents\n",
    "        \"\"\"\n",
    "        X = check_array(X, accept_sparse=('csr', 'csc'))\n",
    "        if not sp.issparse(X):\n",
    "            X = sp.csr_matrix(X)\n",
    "        dtype = np.float64\n",
    "\n",
    "        if self.use_idf:\n",
    "            _, n_features = X.shape\n",
    "            self.df = np.squeeze(np.asarray(X.sum(axis=0)))\n",
    "            idf = np.log(n_samples / self.df)\n",
    "            self._idf_diag = sp.diags(idf, offsets=0,\n",
    "                                      shape=(n_features, n_features),\n",
    "                                      format='csr',\n",
    "                                      dtype=dtype)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: sp.csr_matrix, copy=True):\n",
    "        \"\"\"Transform a count-based matrix to c-TF-IDF\n",
    "        Arguments:\n",
    "            X (sparse matrix): A matrix of term/token counts.\n",
    "        Returns:\n",
    "            X (sparse matrix): A c-TF-IDF matrix\n",
    "        \"\"\"\n",
    "        if self.use_idf:\n",
    "            X = normalize(X, axis=1, norm='l1', copy=False)\n",
    "            X = X * self._idf_diag\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = ['afrikaans', 'albanian', 'amharic', 'arabic', 'armenian', 'assamese',\n",
    "             'azerbaijani', 'basque', 'belarusian', 'bengali', 'bengali romanize',\n",
    "             'bosnian', 'breton', 'bulgarian', 'burmese', 'burmese zawgyi font', 'catalan',\n",
    "             'chinese (simplified)', 'chinese (traditional)', 'croatian', 'czech', 'danish',\n",
    "             'dutch', 'english', 'esperanto', 'estonian', 'filipino', 'finnish', 'french',\n",
    "             'galician', 'georgian', 'german', 'greek', 'gujarati', 'hausa', 'hebrew', 'hindi',\n",
    "             'hindi romanize', 'hungarian', 'icelandic', 'indonesian', 'irish', 'italian', 'japanese',\n",
    "             'javanese', 'kannada', 'kazakh', 'khmer', 'korean', 'kurdish (kurmanji)', 'kyrgyz',\n",
    "             'lao', 'latin', 'latvian', 'lithuanian', 'macedonian', 'malagasy', 'malay', 'malayalam',\n",
    "             'marathi', 'mongolian', 'nepali', 'norwegian', 'oriya', 'oromo', 'pashto', 'persian',\n",
    "             'polish', 'portuguese', 'punjabi', 'romanian', 'russian', 'sanskrit', 'scottish gaelic',\n",
    "             'serbian', 'sindhi', 'sinhala', 'slovak', 'slovenian', 'somali', 'spanish', 'sundanese',\n",
    "             'swahili', 'swedish', 'tamil', 'tamil romanize', 'telugu', 'telugu romanize', 'thai',\n",
    "             'turkish', 'ukrainian', 'urdu', 'urdu romanize', 'uyghur', 'uzbek', 'vietnamese',\n",
    "             'welsh', 'western frisian', 'xhosa', 'yiddish']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def mmr(doc_embedding: np.ndarray,\n",
    "        word_embeddings: np.ndarray,\n",
    "        words: List[str],\n",
    "        top_n: int = 5,\n",
    "        diversity: float = 0.8) -> List[str]:\n",
    "    \"\"\" Calculate Maximal Marginal Relevance (MMR)\n",
    "    between candidate keywords and the document.\n",
    "    MMR considers the similarity of keywords/keyphrases with the\n",
    "    document, along with the similarity of already selected\n",
    "    keywords and keyphrases. This results in a selection of keywords\n",
    "    that maximize their within diversity with respect to the document.\n",
    "    Arguments:\n",
    "        doc_embedding: The document embeddings\n",
    "        word_embeddings: The embeddings of the selected candidate keywords/phrases\n",
    "        words: The selected candidate keywords/keyphrases\n",
    "        top_n: The number of keywords/keyhprases to return\n",
    "        diversity: How diverse the select keywords/keyphrases are.\n",
    "                   Values between 0 and 1 with 0 being not diverse at all\n",
    "                   and 1 being most diverse.\n",
    "    Returns:\n",
    "         List[str]: The selected keywords/keyphrases\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract similarity within words, and between words and the document\n",
    "    word_doc_similarity = cosine_similarity(word_embeddings, doc_embedding)\n",
    "    word_similarity = cosine_similarity(word_embeddings)\n",
    "\n",
    "    # Initialize candidates and already choose best keyword/keyphras\n",
    "    keywords_idx = [np.argmax(word_doc_similarity)]\n",
    "    candidates_idx = [i for i in range(len(words)) if i != keywords_idx[0]]\n",
    "\n",
    "    for _ in range(top_n - 1):\n",
    "        # Extract similarities within candidates and\n",
    "        # between candidates and selected keywords/phrases\n",
    "        candidate_similarities = word_doc_similarity[candidates_idx, :]\n",
    "        target_similarities = np.max(word_similarity[candidates_idx][:, keywords_idx], axis=1)\n",
    "\n",
    "        # Calculate MMR\n",
    "        mmr = (1-diversity) * candidate_similarities - diversity * target_similarities.reshape(-1, 1)\n",
    "        mmr_idx = candidates_idx[np.argmax(mmr)]\n",
    "\n",
    "        # Update keywords & candidates\n",
    "        keywords_idx.append(mmr_idx)\n",
    "        candidates_idx.remove(mmr_idx)\n",
    "\n",
    "    return [words[idx] for idx in keywords_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import logging\n",
    "from collections.abc import Iterable\n",
    "from scipy.sparse.csr import csr_matrix\n",
    "\n",
    "\n",
    "class MyLogger:\n",
    "    def __init__(self, level):\n",
    "        self.logger = logging.getLogger('BERTopic')\n",
    "        self.set_level(level)\n",
    "        self._add_handler()\n",
    "        self.logger.propagate = False\n",
    "\n",
    "    def info(self, message):\n",
    "        self.logger.info(\"{}\".format(message))\n",
    "\n",
    "    def set_level(self, level):\n",
    "        levels = [\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\", \"CRITICAL\"]\n",
    "        if level in levels:\n",
    "            self.logger.setLevel(level)\n",
    "\n",
    "    def _add_handler(self):\n",
    "        sh = logging.StreamHandler()\n",
    "        sh.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(message)s'))\n",
    "        self.logger.addHandler(sh)\n",
    "\n",
    "        # Remove duplicate handlers\n",
    "        if len(self.logger.handlers) > 1:\n",
    "            self.logger.handlers = [self.logger.handlers[0]]\n",
    "\n",
    "\n",
    "def check_documents_type(documents):\n",
    "    \"\"\" Check whether the input documents are indeed a list of strings \"\"\"\n",
    "    if isinstance(documents, Iterable) and not isinstance(documents, str):\n",
    "        if not any([isinstance(doc, str) for doc in documents]):\n",
    "            raise TypeError(\"Make sure that the iterable only contains strings.\")\n",
    "\n",
    "    else:\n",
    "        raise TypeError(\"Make sure that the documents variable is an iterable containing strings only.\")\n",
    "\n",
    "\n",
    "def check_embeddings_shape(embeddings, docs):\n",
    "    \"\"\" Check if the embeddings have the correct shape \"\"\"\n",
    "    if embeddings is not None:\n",
    "        if not any([isinstance(embeddings, np.ndarray), isinstance(embeddings, csr_matrix)]):\n",
    "            raise ValueError(\"Make sure to input embeddings as a numpy array or scipy.sparse.csr.csr_matrix. \")\n",
    "        else:\n",
    "            if embeddings.shape[0] != len(docs):\n",
    "                raise ValueError(\"Make sure that the embeddings are a numpy array with shape: \"\n",
    "                                 \"(len(docs), vector_dim) where vector_dim is the dimensionality \"\n",
    "                                 \"of the vector embeddings. \")\n",
    "\n",
    "\n",
    "def check_is_fitted(model):\n",
    "    \"\"\" Checks if the model was fitted by verifying the presence of self.matches\n",
    "    Arguments:\n",
    "        model: BERTopic instance for which the check is performed.\n",
    "    Returns:\n",
    "        None\n",
    "    Raises:\n",
    "        ValueError: If the matches were not found.\n",
    "    \"\"\"\n",
    "    msg = (\"This %(name)s instance is not fitted yet. Call 'fit' with \"\n",
    "           \"appropriate arguments before using this estimator.\")\n",
    "\n",
    "    if not model.topics:\n",
    "        raise ValueError(msg % {'name': type(model).__name__})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import re\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse.csr import csr_matrix\n",
    "from typing import List, Tuple, Dict, Union\n",
    "\n",
    "# Models\n",
    "import umap\n",
    "import hdbscan\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "# Additional dependencies\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import plotly.express as px\n",
    "    _HAS_VIZ = True\n",
    "except ModuleNotFoundError as e:\n",
    "    _HAS_VIZ = False\n",
    "\n",
    "logger = MyLogger(\"WARNING\")\n",
    "\n",
    "\n",
    "class BERTopic:\n",
    "    \"\"\"BERTopic is a topic modeling technique that leverages BERT embeddings and\n",
    "    c-TF-IDF to create dense clusters allowing for easily interpretable topics\n",
    "    whilst keeping important words in the topic descriptions.\n",
    "    Usage:\n",
    "    ```python\n",
    "    from bertopic import BERTopic\n",
    "    from sklearn.datasets import fetch_20newsgroups\n",
    "    docs = fetch_20newsgroups(subset='all')['data']\n",
    "    model = BERTopic(\"distilbert-base-nli-mean-tokens\", verbose=True)\n",
    "    topics = model.fit_transform(docs)\n",
    "    ```\n",
    "    If you want to use your own embeddings, use it as follows:\n",
    "    ```python\n",
    "    from bertopic import BERTopic\n",
    "    from sklearn.datasets import fetch_20newsgroups\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    # Create embeddings\n",
    "    docs = fetch_20newsgroups(subset='all')['data']\n",
    "    sentence_model = SentenceTransformer(\"distilbert-base-nli-mean-tokens\")\n",
    "    embeddings = sentence_model.encode(docs, show_progress_bar=True)\n",
    "    # Create topic model\n",
    "    model = BERTopic(verbose=True)\n",
    "    topics = model.fit_transform(docs, embeddings)\n",
    "    ```\n",
    "    Due to the stochastisch nature of UMAP, the results from BERTopic might differ\n",
    "    and the quality can degrade. Using your own embeddings allows you to\n",
    "    try out BERTopic several times until you find the topics that suit\n",
    "    you best.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 language: str = \"english\",\n",
    "                 embedding_model: str = None,\n",
    "                 top_n_words: int = 10,\n",
    "                 nr_topics: Union[int, str] = None,\n",
    "                 n_gram_range: Tuple[int, int] = (1, 1),\n",
    "                 min_topic_size: int = 10,\n",
    "                 n_neighbors: int = 15,\n",
    "                 n_components: int = 5,\n",
    "                 stop_words: Union[str, List[str]] = None,\n",
    "                 verbose: bool = False,\n",
    "                 vectorizer: CountVectorizer = None,\n",
    "                 calculate_probabilities: bool = True,\n",
    "                 allow_st_model: bool = True):\n",
    "        \"\"\"BERTopic initialization\n",
    "        Args:\n",
    "            language: The main language used in your documents. For a full overview of supported languages\n",
    "                      see bertopic.embeddings.languages. Select \"multilingual\" to load in a model that\n",
    "                      support 50+ languages.\n",
    "            embedding_model: Model to use. Overview of options can be found here\n",
    "                            https://www.sbert.net/docs/pretrained_models.html\n",
    "            top_n_words: The number of words per topic to extract\n",
    "            nr_topics: Specifying the number of topics will reduce the initial\n",
    "                       number of topics to the value specified. This reduction can take\n",
    "                       a while as each reduction in topics (-1) activates a c-TF-IDF calculation.\n",
    "                       IF this is set to None, no reduction is applied. Use \"auto\" to automatically\n",
    "                       reduce topics that have a similarity of at least 0.9, do not maps all others.\n",
    "            n_gram_range: The n-gram range for the CountVectorizer.\n",
    "                          Advised to keep high values between 1 and 3.\n",
    "                          More would likely lead to memory issues.\n",
    "                          Note that this will not be used if you pass in your own CountVectorizer.\n",
    "            min_topic_size: The minimum size of the topic.\n",
    "            n_neighbors: The size of local neighborhood (in terms of number of neighboring sample points) used\n",
    "                         for manifold approximation (UMAP).\n",
    "            n_components: The dimension of the space to embed into when reducing dimensionality with UMAP.\n",
    "            stop_words: Stopwords that can be used as either a list of strings, or the name of the\n",
    "                        language as a string. For example: 'english' or ['the', 'and', 'I'].\n",
    "                        Note that this will not be used if you pass in your own CountVectorizer.\n",
    "            verbose: Changes the verbosity of the model, Set to True if you want\n",
    "                     to track the stages of the model.\n",
    "            vectorizer: Pass in your own CountVectorizer from scikit-learn\n",
    "            calculate_probabilities: Whether to calculate the topic probabilities. This could slow down\n",
    "                                     extraction of topics if you have many documents (>100_000). If so,\n",
    "                                     set this to False to increase speed.\n",
    "            allow_st_model: This allows BERTopic to use a multi-lingual version of SentenceTransformer\n",
    "                            to be used to fine-tune the topic words extracted from the c-TF-IDF representation.\n",
    "                            Moreover, it will allow you to search for topics based on search queries.\n",
    "        Usage:\n",
    "        ```python\n",
    "        from bertopic import BERTopic\n",
    "        model = BERTopic(language = \"english\",\n",
    "                         embedding_model = None,\n",
    "                         top_n_words = 10,\n",
    "                         nr_topics = 30,\n",
    "                         n_gram_range = (1, 1),\n",
    "                         min_topic_size = 10,\n",
    "                         n_neighbors = 15,\n",
    "                         n_components = 5,\n",
    "                         stop_words = None,\n",
    "                         verbose = True,\n",
    "                         vectorizer = None,\n",
    "                         allow_st_model = True)\n",
    "        ```\n",
    "        \"\"\"\n",
    "\n",
    "        # Embedding model\n",
    "        self.language = language\n",
    "        self.embedding_model = embedding_model\n",
    "        self.allow_st_model = allow_st_model\n",
    "\n",
    "        # Topic-based parameters\n",
    "        if top_n_words > 30:\n",
    "            raise ValueError(\"top_n_words should be lower or equal to 30. The preferred value is 10.\")\n",
    "        self.top_n_words = top_n_words\n",
    "        self.nr_topics = nr_topics\n",
    "        self.min_topic_size = min_topic_size\n",
    "        self.calculate_probabilities = calculate_probabilities\n",
    "\n",
    "        # Umap parameters\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.n_components = n_components\n",
    "\n",
    "        # Vectorizer parameters\n",
    "        self.stop_words = stop_words\n",
    "        self.n_gram_range = n_gram_range\n",
    "        self.vectorizer = vectorizer or CountVectorizer(ngram_range=self.n_gram_range, stop_words=self.stop_words)\n",
    "\n",
    "        self.umap_model = None\n",
    "        self.cluster_model = None\n",
    "        self.topics = None\n",
    "        self.topic_sizes = None\n",
    "        self.reduced_topics_mapped = None\n",
    "        self.mapped_topics = None\n",
    "        self.topic_embeddings = None\n",
    "        self.topic_sim_matrix = None\n",
    "        self.custom_embeddings = False\n",
    "\n",
    "        if verbose:\n",
    "            logger.set_level(\"DEBUG\")\n",
    "\n",
    "    def fit(self,\n",
    "            documents: List[str],\n",
    "            embeddings: np.ndarray = None):\n",
    "        \"\"\" Fit the models (Bert, UMAP, and, HDBSCAN) on a collection of documents and generate topics\n",
    "        Arguments:\n",
    "            documents: A list of documents to fit on\n",
    "            embeddings: Pre-trained document embeddings. These can be used\n",
    "                        instead of the sentence-transformer model\n",
    "        Usage:\n",
    "        ```python\n",
    "        from bertopic import BERTopic\n",
    "        from sklearn.datasets import fetch_20newsgroups\n",
    "        docs = fetch_20newsgroups(subset='all')['data']\n",
    "        model = BERTopic(\"distilbert-base-nli-mean-tokens\", verbose=True).fit(docs)\n",
    "        ```\n",
    "        If you want to use your own embeddings, use it as follows:\n",
    "        ```python\n",
    "        from bertopic import BERTopic\n",
    "        from sklearn.datasets import fetch_20newsgroups\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        # Create embeddings\n",
    "        docs = fetch_20newsgroups(subset='all')['data']\n",
    "        sentence_model = SentenceTransformer(\"distilbert-base-nli-mean-tokens\")\n",
    "        embeddings = sentence_model.encode(docs, show_progress_bar=True)\n",
    "        # Create topic model\n",
    "        model = BERTopic(None, verbose=True).fit(docs, embeddings)\n",
    "        ```\n",
    "        \"\"\"\n",
    "        self.fit_transform(documents, embeddings)\n",
    "        return self\n",
    "\n",
    "    def fit_transform(self,\n",
    "                      documents: List[str],\n",
    "                      embeddings: np.ndarray = None) -> Tuple[List[int],\n",
    "                                                              Union[np.ndarray, None]]:\n",
    "        \"\"\" Fit the models on a collection of documents, generate topics, and return the docs with topics\n",
    "        Arguments:\n",
    "            documents: A list of documents to fit on\n",
    "            embeddings: Pre-trained document embeddings. These can be used\n",
    "                        instead of the sentence-transformer model\n",
    "        Returns:\n",
    "            predictions: Topic predictions for each documents\n",
    "            probabilities: The topic probability distribution\n",
    "        Usage:\n",
    "        ```python\n",
    "        from bertopic import BERTopic\n",
    "        from sklearn.datasets import fetch_20newsgroups\n",
    "        docs = fetch_20newsgroups(subset='all')['data']\n",
    "        model = BERTopic(\"distilbert-base-nli-mean-tokens\", verbose=True)\n",
    "        topics = model.fit_transform(docs)\n",
    "        ```\n",
    "        If you want to use your own embeddings, use it as follows:\n",
    "        ```python\n",
    "        from bertopic import BERTopic\n",
    "        from sklearn.datasets import fetch_20newsgroups\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        # Create embeddings\n",
    "        docs = fetch_20newsgroups(subset='all')['data']\n",
    "        sentence_model = SentenceTransformer(\"distilbert-base-nli-mean-tokens\")\n",
    "        embeddings = sentence_model.encode(docs, show_progress_bar=True)\n",
    "        # Create topic model\n",
    "        model = BERTopic(None, verbose=True)\n",
    "        topics = model.fit_transform(docs, embeddings)\n",
    "        ```\n",
    "        \"\"\"\n",
    "        check_documents_type(documents)\n",
    "        check_embeddings_shape(embeddings, documents)\n",
    "\n",
    "        documents = pd.DataFrame({\"Document\": documents,\n",
    "                                  \"ID\": range(len(documents)),\n",
    "                                  \"Topic\": None})\n",
    "\n",
    "        # Extract embeddings\n",
    "        if not any([isinstance(embeddings, np.ndarray), isinstance(embeddings, csr_matrix)]):\n",
    "            embeddings = self._extract_embeddings(documents.Document)\n",
    "        else:\n",
    "            self.custom_embeddings = True\n",
    "\n",
    "        # Reduce dimensionality with UMAP\n",
    "        umap_embeddings = self._reduce_dimensionality(embeddings)\n",
    "\n",
    "        # Cluster UMAP embeddings with HDBSCAN\n",
    "        documents, probabilities = self._cluster_embeddings(umap_embeddings, documents)\n",
    "\n",
    "        # Extract topics by calculating c-TF-IDF\n",
    "        self._extract_topics(documents)\n",
    "\n",
    "        if self.nr_topics:\n",
    "            documents = self._reduce_topics(documents)\n",
    "            probabilities = self._map_probabilities(probabilities)\n",
    "\n",
    "        predictions = documents.Topic.to_list()\n",
    "\n",
    "        return predictions, probabilities\n",
    "\n",
    "    def transform(self,\n",
    "                  documents: Union[str, List[str]],\n",
    "                  embeddings: np.ndarray = None) -> Tuple[List[int], np.ndarray]:\n",
    "        \"\"\" After having fit a model, use transform to predict new instances\n",
    "        Arguments:\n",
    "            documents: A single document or a list of documents to fit on\n",
    "            embeddings: Pre-trained document embeddings. These can be used\n",
    "                        instead of the sentence-transformer model.\n",
    "        Returns:\n",
    "            predictions: Topic predictions for each documents\n",
    "            probabilities: The topic probability distribution\n",
    "        Usage:\n",
    "        ```python\n",
    "        from bertopic import BERTopic\n",
    "        from sklearn.datasets import fetch_20newsgroups\n",
    "        docs = fetch_20newsgroups(subset='all')['data']\n",
    "        model = BERTopic(\"distilbert-base-nli-mean-tokens\", verbose=True).fit(docs)\n",
    "        topics = model.transform(docs)\n",
    "        ```\n",
    "        If you want to use your own embeddings:\n",
    "        ```python\n",
    "        from bertopic import BERTopic\n",
    "        from sklearn.datasets import fetch_20newsgroups\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        # Create embeddings\n",
    "        docs = fetch_20newsgroups(subset='all')['data']\n",
    "        sentence_model = SentenceTransformer(\"distilbert-base-nli-mean-tokens\")\n",
    "        embeddings = sentence_model.encode(docs, show_progress_bar=True)\n",
    "        # Create topic model\n",
    "        model = BERTopic(None, verbose=True).fit(docs, embeddings)\n",
    "        topics = model.transform(docs, embeddings)\n",
    "        ```\n",
    "        \"\"\"\n",
    "        check_is_fitted(self)\n",
    "        check_embeddings_shape(embeddings, documents)\n",
    "\n",
    "        if isinstance(documents, str):\n",
    "            documents = [documents]\n",
    "\n",
    "        if not isinstance(embeddings, np.ndarray):\n",
    "            embeddings = self._extract_embeddings(documents)\n",
    "\n",
    "        umap_embeddings = self.umap_model.transform(embeddings)\n",
    "        predictions, _ = hdbscan.approximate_predict(self.cluster_model, umap_embeddings)\n",
    "\n",
    "        if self.calculate_probabilities:\n",
    "            probabilities = hdbscan.membership_vector(self.cluster_model, umap_embeddings)\n",
    "            if len(documents) == 1:\n",
    "                probabilities = probabilities.flatten()\n",
    "        else:\n",
    "            probabilities = None\n",
    "\n",
    "        if self.mapped_topics:\n",
    "            predictions = self._map_predictions(predictions)\n",
    "            probabilities = self._map_probabilities(probabilities)\n",
    "\n",
    "        return predictions, probabilities\n",
    "\n",
    "    def find_topics(self,\n",
    "                    search_term: str,\n",
    "                    top_n: int = 5) -> Tuple[List[int], List[float]]:\n",
    "        \"\"\" Find topics most similar to a search_term\n",
    "        Creates an embedding for search_term and compares that with\n",
    "        the topic embeddings. The most similar topics are returned\n",
    "        along with their similarity values.\n",
    "        The search_term can be of any size but since it compares\n",
    "        with the topic representation it is advised to keep it\n",
    "        below 5 words.\n",
    "        Args:\n",
    "            search_term: the term you want to use to search for topics\n",
    "            top_n: the number of topics to return\n",
    "        Returns:\n",
    "            similar_topics: the most similar topics from high to low\n",
    "            similarity: the similarity scores from high to low\n",
    "        \"\"\"\n",
    "        if self.custom_embeddings and not self.allow_st_model:\n",
    "            raise Exception(\"This method can only be used if you set `allow_st_model` to True when \"\n",
    "                            \"using custom embeddings.\")\n",
    "\n",
    "        topic_list = list(self.topics.keys())\n",
    "        topic_list.sort()\n",
    "\n",
    "        # Extract search_term embeddings and compare with topic embeddings\n",
    "        search_embedding = self._extract_embeddings([search_term]).flatten()\n",
    "        sims = cosine_similarity(search_embedding.reshape(1, -1), self.topic_embeddings).flatten()\n",
    "\n",
    "        # Extract topics most similar to search_term\n",
    "        ids = np.argsort(sims)[-top_n:]\n",
    "        similarity = [sims[i] for i in ids][::-1]\n",
    "        similar_topics = [topic_list[index] for index in ids][::-1]\n",
    "\n",
    "        return similar_topics, similarity\n",
    "\n",
    "    def update_topics(self,\n",
    "                      docs: List[str],\n",
    "                      topics: List[int],\n",
    "                      n_gram_range: Tuple[int, int] = None,\n",
    "                      stop_words: str = None,\n",
    "                      vectorizer: CountVectorizer = None):\n",
    "        \"\"\" Updates the topic representation by recalculating c-TF-IDF with the new\n",
    "        parameters as defined in this function.\n",
    "        When you have trained a model and viewed the topics and the words that represent them,\n",
    "        you might not be satisfied with the representation. Perhaps you forgot to remove\n",
    "        stop_words or you want to try out a different n_gram_range. This function allows you\n",
    "        to update the topic representation after they have been formed.\n",
    "        Args:\n",
    "            docs: The docs you used when calling either `fit` or `fit_transform`\n",
    "            topics: The topics that were returned when calling either `fit` or `fit_transform`\n",
    "            n_gram_range: The n-gram range for the CountVectorizer.\n",
    "            stop_words: Stopwords that can be used as either a list of strings, or the name of the\n",
    "                        language as a string. For example: 'english' or ['the', 'and', 'I'].\n",
    "                        Note that this will not be used if you pass in your own CountVectorizer.\n",
    "            vectorizer: Pass in your own CountVectorizer from scikit-learn\n",
    "        Usage:\n",
    "        ```python\n",
    "        from bertopic import BERTopic\n",
    "        from sklearn.datasets import fetch_20newsgroups\n",
    "        # Create topics\n",
    "        docs = fetch_20newsgroups(subset='train')['data']\n",
    "        model = BERTopic(n_gram_range=(1, 1), stop_words=None)\n",
    "        topics, probs = model.fit_transform(docs)\n",
    "        # Update topic representation\n",
    "        model.update_topics(docs, topics, n_gram_range=(2, 3), stop_words=\"english\")\n",
    "        ```\n",
    "        \"\"\"\n",
    "        check_is_fitted(self)\n",
    "        if not n_gram_range:\n",
    "            n_gram_range = self.n_gram_range\n",
    "\n",
    "        if not stop_words:\n",
    "            stop_words = self.stop_words\n",
    "\n",
    "        self.vectorizer = vectorizer or CountVectorizer(ngram_range=n_gram_range, stop_words=stop_words)\n",
    "        documents = pd.DataFrame({\"Document\": docs, \"Topic\": topics})\n",
    "        self._extract_topics(documents)\n",
    "\n",
    "    def get_topics(self) -> Dict[str, Tuple[str, float]]:\n",
    "        \"\"\" Return topics with top n words and their c-TF-IDF score\n",
    "        Usage:\n",
    "        ```python\n",
    "        all_topics = model.get_topics()\n",
    "        ```\n",
    "        \"\"\"\n",
    "        check_is_fitted(self)\n",
    "        return self.topics\n",
    "\n",
    "    def get_topic(self, topic: int) -> Union[Dict[str, Tuple[str, float]], bool]:\n",
    "        \"\"\" Return top n words for a specific topic and their c-TF-IDF scores\n",
    "        Usage:\n",
    "        ```python\n",
    "        topic = model.get_topic(12)\n",
    "        ```\n",
    "        \"\"\"\n",
    "        check_is_fitted(self)\n",
    "        if self.topics.get(topic):\n",
    "            return self.topics[topic]\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def get_topic_freq(self, topic: int = None) -> Union[pd.DataFrame, int]:\n",
    "        \"\"\" Return the the size of topics (descending order)\n",
    "        Usage:\n",
    "        To extract the frequency of all topics:\n",
    "        ```python\n",
    "        frequency = model.get_topic_freq()\n",
    "        ```\n",
    "        To get the frequency of a single topic:\n",
    "        ```python\n",
    "        frequency = model.get_topic_freq(12)\n",
    "        ```\n",
    "        \"\"\"\n",
    "        check_is_fitted(self)\n",
    "        if isinstance(topic, int):\n",
    "            return self.topic_sizes[topic]\n",
    "        else:\n",
    "            return pd.DataFrame(self.topic_sizes.items(), columns=['Topic', 'Count']).sort_values(\"Count\",\n",
    "                                                                                                  ascending=False)\n",
    "\n",
    "    def reduce_topics(self,\n",
    "                      docs: List[str],\n",
    "                      topics: List[int],\n",
    "                      probabilities: np.ndarray = None,\n",
    "                      nr_topics: int = 20) -> Tuple[List[int], np.ndarray]:\n",
    "        \"\"\" Further reduce the number of topics to nr_topics.\n",
    "        The number of topics is further reduced by calculating the c-TF-IDF matrix\n",
    "        of the documents and then reducing them by iteratively merging the least\n",
    "        frequent topic with the most similar one based on their c-TF-IDF matrices.\n",
    "        The topics, their sizes, and representations are updated.\n",
    "        The reasoning for putting `docs`, `topics`, and `probs` as parameters is that\n",
    "        these values are not saved within BERTopic on purpose. If you were to have a\n",
    "        million documents, it seems very inefficient to save those in BERTopic\n",
    "        instead of a dedicated database.\n",
    "        Arguments:\n",
    "            docs: The docs you used when calling either `fit` or `fit_transform`\n",
    "            topics: The topics that were returned when calling either `fit` or `fit_transform`\n",
    "            nr_topics: The number of topics you want reduced to\n",
    "            probabilities: The probabilities that were returned when calling either `fit` or `fit_transform`\n",
    "        Returns:\n",
    "            new_topics: Updated topics\n",
    "            new_probabilities: Updated probabilities\n",
    "        Usage:\n",
    "        ```python\n",
    "        from bertopic import BERTopic\n",
    "        from sklearn.datasets import fetch_20newsgroups\n",
    "        # Create topics -> Typically over 50 topics\n",
    "        docs = fetch_20newsgroups(subset='train')['data']\n",
    "        model = BERTopic()\n",
    "        topics, probs = model.fit_transform(docs)\n",
    "        # Further reduce topics\n",
    "        new_topics, new_probs = model.reduce_topics(docs, topics, probs, nr_topics=30)\n",
    "        ```\n",
    "        \"\"\"\n",
    "        check_is_fitted(self)\n",
    "        self.nr_topics = nr_topics\n",
    "        documents = pd.DataFrame({\"Document\": docs, \"Topic\": topics})\n",
    "\n",
    "        # Reduce number of topics\n",
    "        self._extract_topics(documents)\n",
    "        documents = self._reduce_topics(documents)\n",
    "        new_topics = documents.Topic.to_list()\n",
    "        new_probabilities = self._map_probabilities(probabilities)\n",
    "\n",
    "        return new_topics, new_probabilities\n",
    "\n",
    "    def visualize_topics(self):\n",
    "        \"\"\" Visualize topics, their sizes, and their corresponding words\n",
    "        This visualization is highly inspired by LDAvis, a great visualization\n",
    "        technique typically reserved for LDA.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self)\n",
    "        if not _HAS_VIZ:\n",
    "            raise ModuleNotFoundError(f\"In order to use this function you'll need to install \"\n",
    "                                      f\"additional dependencies;\\npip install bertopic[visualization]\")\n",
    "\n",
    "        # Extract topic words and their frequencies\n",
    "        topic_list = sorted(list(self.topics.keys()))\n",
    "        frequencies = [self.topic_sizes[topic] for topic in topic_list]\n",
    "        words = [\" | \".join([word[0] for word in self.get_topic(topic)[:5]]) for topic in topic_list]\n",
    "\n",
    "        # Embed c-TF-IDF into 2D\n",
    "        embeddings = MinMaxScaler().fit_transform(self.c_tf_idf.toarray())\n",
    "        embeddings = umap.UMAP(n_neighbors=2, n_components=2, metric='hellinger').fit_transform(embeddings)\n",
    "\n",
    "        # Visualize with plotly\n",
    "        df = pd.DataFrame({\"x\": embeddings[1:, 0], \"y\": embeddings[1:, 1],\n",
    "                           \"Topic\": topic_list[1:], \"Words\": words[1:], \"Size\": frequencies[1:]})\n",
    "        self._plotly_topic_visualization(df, topic_list)\n",
    "\n",
    "    def visualize_distribution(self,\n",
    "                               probabilities: np.ndarray,\n",
    "                               min_probability: float = 0.015,\n",
    "                               figsize: tuple = (10, 5),\n",
    "                               save: bool = False):\n",
    "        \"\"\" Visualize the distribution of topic probabilities\n",
    "        Arguments:\n",
    "            probabilities: An array of probability scores\n",
    "            min_probability: The minimum probability score to visualize.\n",
    "                             All others are ignored.\n",
    "            figsize: The size of the figure\n",
    "            save: Whether to save the resulting graph to probility.png\n",
    "        Usage:\n",
    "        Make sure to fit the model before and only input the\n",
    "        probabilities of a single document:\n",
    "        ```python\n",
    "        model.visualize_distribution(probabilities[0])\n",
    "        ```\n",
    "        ![](../img/probabilities.png)\n",
    "        \"\"\"\n",
    "        check_is_fitted(self)\n",
    "        if not _HAS_VIZ:\n",
    "            raise ModuleNotFoundError(f\"In order to use this function you'll need to install \"\n",
    "                                      f\"additional dependencies;\\npip install bertopic[visualization]\")\n",
    "        if len(probabilities[probabilities > min_probability]) == 0:\n",
    "            raise ValueError(\"There are no values where `min_probability` is higher than the \"\n",
    "                             \"probabilities that were supplied. Lower `min_probability` to prevent this error.\")\n",
    "        if not self.calculate_probabilities:\n",
    "            raise ValueError(\"This visualization cannot be used if you have set `calculate_probabilities` to False \"\n",
    "                             \"as it uses the topic probabilities. \")\n",
    "\n",
    "        # Get values and indices equal or exceed the minimum probability\n",
    "        labels_idx = np.argwhere(probabilities >= min_probability).flatten()\n",
    "        vals = probabilities[labels_idx].tolist()\n",
    "\n",
    "        # Create labels\n",
    "        labels = []\n",
    "        for idx in labels_idx:\n",
    "            label = []\n",
    "            words = self.get_topic(idx)\n",
    "            if words:\n",
    "                for word in words[:5]:\n",
    "                    label.append(word[0])\n",
    "                label = str(r\"$\\bf{Topic }$ \" +\n",
    "                            r\"$\\bf{\" + str(idx) + \":}$ \" +\n",
    "                            \" \".join(label))\n",
    "                labels.append(label)\n",
    "            else:\n",
    "                print(idx, probabilities[idx])\n",
    "                vals.remove(probabilities[idx])\n",
    "        pos = range(len(vals))\n",
    "\n",
    "        # Create figure\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        plt.hlines(y=pos, xmin=0, xmax=vals, color='#333F4B', alpha=0.2, linewidth=15)\n",
    "        plt.hlines(y=np.argmax(vals), xmin=0, xmax=max(vals), color='#333F4B', alpha=1, linewidth=15)\n",
    "\n",
    "        # Set ticks and labels\n",
    "        ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "        ax.set_xlabel('Probability', fontsize=15, fontweight='black', color='#333F4B')\n",
    "        ax.set_ylabel('')\n",
    "        plt.yticks(pos, labels)\n",
    "        fig.text(0, 1, 'Topic Probability Distribution', fontsize=15, fontweight='black', color='#333F4B')\n",
    "\n",
    "        # Update spine style\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['left'].set_bounds(pos[0], pos[-1])\n",
    "        ax.spines['bottom'].set_bounds(0, max(vals))\n",
    "        ax.spines['bottom'].set_position(('axes', -0.02))\n",
    "        ax.spines['left'].set_position(('axes', 0.02))\n",
    "\n",
    "        fig.tight_layout()\n",
    "\n",
    "        if save:\n",
    "            fig.savefig(\"probability.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "    def save(self, path: str) -> None:\n",
    "        \"\"\" Saves the model to the specified path\n",
    "        Arguments:\n",
    "            path: the location and name of the file you want to save\n",
    "        Usage:\n",
    "        ```python\n",
    "        model.save(\"my_model\")\n",
    "        ```\n",
    "        \"\"\"\n",
    "        with open(path, 'wb') as file:\n",
    "            joblib.dump(self, file)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path: str):\n",
    "        \"\"\" Loads the model from the specified path\n",
    "        Arguments:\n",
    "            path: the location and name of the BERTopic file you want to load\n",
    "        Usage:\n",
    "        ```python\n",
    "        BERTopic.load(\"my_model\")\n",
    "        ```\n",
    "        \"\"\"\n",
    "        with open(path, 'rb') as file:\n",
    "            return joblib.load(file)\n",
    "\n",
    "    def _extract_embeddings(self, documents: List[str]) -> np.ndarray:\n",
    "        \"\"\" Extract sentence/document embeddings through pre-trained embeddings\n",
    "        For an overview of pre-trained models: https://www.sbert.net/docs/pretrained_models.html\n",
    "        Arguments:\n",
    "            documents: Dataframe with documents and their corresponding IDs\n",
    "        Returns:\n",
    "            embeddings: The extracted embeddings using the sentence transformer\n",
    "                        module. Typically uses pre-trained huggingface models.\n",
    "        \"\"\"\n",
    "        model = self._select_embedding_model()\n",
    "        logger.info(\"Loaded embedding model\")\n",
    "        embeddings = model.encode(documents, show_progress_bar=False)\n",
    "        logger.info(\"Transformed documents to Embeddings\")\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    def _map_predictions(self, predictions):\n",
    "        \"\"\" Map predictions to the correct topics if topics were reduced \"\"\"\n",
    "        mapped_predictions = []\n",
    "        for prediction in predictions:\n",
    "            while self.mapped_topics.get(prediction):\n",
    "                prediction = self.mapped_topics[prediction]\n",
    "            mapped_predictions.append(prediction)\n",
    "        return mapped_predictions\n",
    "\n",
    "    def _reduce_dimensionality(self, embeddings: Union[np.ndarray, csr_matrix]) -> np.ndarray:\n",
    "        \"\"\" Reduce dimensionality of embeddings using UMAP and train a UMAP model\n",
    "        Arguments:\n",
    "            embeddings: The extracted embeddings using the sentence transformer module.\n",
    "        Returns:\n",
    "            umap_embeddings: The reduced embeddings\n",
    "        \"\"\"\n",
    "        if isinstance(embeddings, csr_matrix):\n",
    "            self.umap_model = umap.UMAP(n_neighbors=self.n_neighbors,\n",
    "                                        n_components=self.n_components,\n",
    "                                        metric='hellinger').fit(embeddings)\n",
    "        else:\n",
    "            self.umap_model = umap.UMAP(n_neighbors=self.n_neighbors,\n",
    "                                        n_components=self.n_components,\n",
    "                                        min_dist=0.0,\n",
    "                                        metric='cosine').fit(embeddings)\n",
    "        umap_embeddings = self.umap_model.transform(embeddings)\n",
    "        logger.info(\"Reduced dimensionality with UMAP\")\n",
    "        return umap_embeddings\n",
    "\n",
    "    def _cluster_embeddings(self,\n",
    "                            umap_embeddings: np.ndarray,\n",
    "                            documents: pd.DataFrame) -> Tuple[pd.DataFrame,\n",
    "                                                              np.ndarray]:\n",
    "        \"\"\" Cluster UMAP embeddings with HDBSCAN\n",
    "        Arguments:\n",
    "            umap_embeddings: The reduced sentence embeddings with UMAP\n",
    "            documents: Dataframe with documents and their corresponding IDs\n",
    "        Returns:\n",
    "            documents: Updated dataframe with documents and their corresponding IDs\n",
    "                       and newly added Topics\n",
    "            probabilities: The distribution of probabilities\n",
    "        \"\"\"\n",
    "        self.cluster_model = hdbscan.HDBSCAN(min_cluster_size=self.min_topic_size,\n",
    "                                             metric='euclidean',\n",
    "                                             cluster_selection_method='eom',\n",
    "                                             prediction_data=True).fit(umap_embeddings)\n",
    "        documents['Topic'] = self.cluster_model.labels_\n",
    "\n",
    "        if self.calculate_probabilities:\n",
    "            probabilities = hdbscan.all_points_membership_vectors(self.cluster_model)\n",
    "        else:\n",
    "            probabilities = None\n",
    "\n",
    "        self._update_topic_size(documents)\n",
    "        logger.info(\"Clustered UMAP embeddings with HDBSCAN\")\n",
    "        return documents, probabilities\n",
    "\n",
    "    def _extract_topics(self, documents: pd.DataFrame):\n",
    "        \"\"\" Extract topics from the clusters using a class-based TF-IDF\n",
    "        Arguments:\n",
    "            documents: Dataframe with documents and their corresponding IDs\n",
    "        Returns:\n",
    "            c_tf_idf: The resulting matrix giving a value (importance score) for each word per topic\n",
    "        \"\"\"\n",
    "        documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "        self.c_tf_idf, words = self._c_tf_idf(documents_per_topic, m=len(documents))\n",
    "        self._extract_words_per_topic(words)\n",
    "        self._create_topic_vectors()\n",
    "\n",
    "    def _create_topic_vectors(self):\n",
    "        \"\"\" Creates embeddings per topics based on their topic representation\n",
    "        We start by creating embeddings out of the topic representation. This\n",
    "        results in a number of embeddings per topic. Then, we take the weighted\n",
    "        average of embeddings in a topic by their c-TF-IDF score. This will put\n",
    "        more emphasis to words that represent a topic best.\n",
    "        Only allow topic vectors to be created if there are no custom embeddings and therefore\n",
    "        a sentence-transformer model to be used or there are custom embeddings but it is allowed\n",
    "        to use a different multi-lingual sentence-transformer model\n",
    "        \"\"\"\n",
    "        if not self.custom_embeddings or all([self.custom_embeddings and self.allow_st_model]):\n",
    "            topic_list = list(self.topics.keys())\n",
    "            topic_list.sort()\n",
    "            n = self.top_n_words\n",
    "\n",
    "            # Extract embeddings for all words in all topics\n",
    "            topic_words = [self.get_topic(topic) for topic in topic_list]\n",
    "            topic_words = [word[0] for topic in topic_words for word in topic]\n",
    "            embeddings = self._extract_embeddings(topic_words)\n",
    "\n",
    "            # Take the weighted average of word embeddings in a topic based on their c-TF-IDF value\n",
    "            # The embeddings var is a single numpy matrix and therefore slicing is necessary to\n",
    "            # access the words per topic\n",
    "            topic_embeddings = []\n",
    "            for i, topic in enumerate(topic_list):\n",
    "                word_importance = [val[1] for val in self.get_topic(topic)]\n",
    "                if sum(word_importance) == 0:\n",
    "                    word_importance = [1 for _ in range(len(self.get_topic(topic)))]\n",
    "                topic_embedding = np.average(embeddings[i * n: n + (i * n)], weights=word_importance, axis=0)\n",
    "                topic_embeddings.append(topic_embedding)\n",
    "\n",
    "            self.topic_embeddings = topic_embeddings\n",
    "\n",
    "    def _c_tf_idf(self, documents_per_topic: pd.DataFrame, m: int) -> Tuple[csr_matrix, List[str]]:\n",
    "        \"\"\" Calculate a class-based TF-IDF where m is the number of total documents.\n",
    "        Arguments:\n",
    "            documents_per_topic: The joined documents per topic such that each topic has a single\n",
    "                                 string made out of multiple documents\n",
    "            m: The total number of documents (unjoined)\n",
    "        Returns:\n",
    "            tf_idf: The resulting matrix giving a value (importance score) for each word per topic\n",
    "            words: The names of the words to which values were given\n",
    "        \"\"\"\n",
    "        documents = self._preprocess_text(documents_per_topic.Document.values)\n",
    "        count = self.vectorizer.fit(documents)\n",
    "        words = count.get_feature_names()\n",
    "        X = count.transform(documents)\n",
    "        transformer = ClassTFIDF().fit(X, n_samples=m)\n",
    "        c_tf_idf = transformer.transform(X)\n",
    "        self.topic_sim_matrix = cosine_similarity(c_tf_idf)\n",
    "\n",
    "        return c_tf_idf, words\n",
    "\n",
    "    def _update_topic_size(self, documents: pd.DataFrame) -> None:\n",
    "        \"\"\" Calculate the topic sizes\n",
    "        Arguments:\n",
    "            documents: Updated dataframe with documents and their corresponding IDs and newly added Topics\n",
    "        \"\"\"\n",
    "        sizes = documents.groupby(['Topic']).count().sort_values(\"Document\", ascending=False).reset_index()\n",
    "        self.topic_sizes = dict(zip(sizes.Topic, sizes.Document))\n",
    "\n",
    "    def _extract_words_per_topic(self, words: List[str]):\n",
    "        \"\"\" Based on tf_idf scores per topic, extract the top n words per topic\n",
    "        Arguments:\n",
    "        words: List of all words (sorted according to tf_idf matrix position)\n",
    "        \"\"\"\n",
    "\n",
    "        # Get top 30 words per topic based on c-TF-IDF score\n",
    "        c_tf_idf = self.c_tf_idf.toarray()\n",
    "        labels = sorted(list(self.topic_sizes.keys()))\n",
    "        indices = c_tf_idf.argsort()[:, -30:]\n",
    "        self.topics = {label: [(words[j], c_tf_idf[i][j])\n",
    "                               for j in indices[i]][::-1]\n",
    "                       for i, label in enumerate(labels)}\n",
    "\n",
    "        # Extract word embeddings for the top 30 words per topic and compare it\n",
    "        # with the topic embedding to keep only the words most similar to the topic embedding\n",
    "        if not self.custom_embeddings or all([self.custom_embeddings and self.allow_st_model]):\n",
    "            model = self._select_embedding_model()\n",
    "\n",
    "            for topic, topic_words in self.topics.items():\n",
    "                words = [word[0] for word in topic_words]\n",
    "                word_embeddings = model.encode(words)\n",
    "                topic_embedding = model.encode(\" \".join(words)).reshape(1, -1)\n",
    "                topic_words = mmr(topic_embedding, word_embeddings, words, top_n=self.top_n_words, diversity=0)\n",
    "                self.topics[topic] = [(word, value) for word, value in self.topics[topic] if word in topic_words]\n",
    "\n",
    "    def _select_embedding_model(self) -> SentenceTransformer:\n",
    "        \"\"\" Select an embedding model based on language or a specific sentence transformer models.\n",
    "        When selecting a language, we choose distilbert-base-nli-stsb-mean-tokens for English and\n",
    "        xlm-r-bert-base-nli-stsb-mean-tokens for all other languages as it support 100+ languages.\n",
    "        \"\"\"\n",
    "\n",
    "        # Used for fine-tuning the topic representation\n",
    "        # If a custom embeddings are used, we use the multi-langual model\n",
    "        # to extract word embeddings\n",
    "        if self.custom_embeddings and self.allow_st_model:\n",
    "            return SentenceTransformer(\"xlm-r-bert-base-nli-stsb-mean-tokens\")\n",
    "\n",
    "        # Select embedding model based on specific sentence transformer model\n",
    "        elif self.embedding_model:\n",
    "            return SentenceTransformer(self.embedding_model)\n",
    "\n",
    "        # Select embedding model based on language\n",
    "        elif self.language:\n",
    "            if self.language.lower() in [\"English\", \"english\", \"en\"]:\n",
    "                return SentenceTransformer(\"distilbert-base-nli-stsb-mean-tokens\")\n",
    "\n",
    "            elif self.language.lower() in languages:\n",
    "                return SentenceTransformer(\"xlm-r-bert-base-nli-stsb-mean-tokens\")\n",
    "\n",
    "            elif self.language == \"multilingual\":\n",
    "                return SentenceTransformer(\"xlm-r-bert-base-nli-stsb-mean-tokens\")\n",
    "\n",
    "            else:\n",
    "                raise ValueError(f\"{self.language} is currently not supported. However, you can \"\n",
    "                                 f\"create any embeddings yourself and pass it through fit_transform(docs, embeddings)\\n\"\n",
    "                                 \"Else, please select a language from the following list:\\n\"\n",
    "                                 f\"{languages}\")\n",
    "\n",
    "        return SentenceTransformer(\"xlm-r-bert-base-nli-stsb-mean-tokens\")\n",
    "\n",
    "    def _reduce_topics(self, documents: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\" Reduce topics to self.nr_topics\n",
    "        Arguments:\n",
    "            documents: Dataframe with documents and their corresponding IDs and Topics\n",
    "        Returns:\n",
    "            documents: Updated dataframe with documents and the reduced number of Topics\n",
    "        \"\"\"\n",
    "        if isinstance(self.nr_topics, int):\n",
    "            documents = self._reduce_to_n_topics(documents)\n",
    "        elif isinstance(self.nr_topics, str):\n",
    "            documents = self._auto_reduce_topics(documents)\n",
    "        else:\n",
    "            raise ValueError(\"nr_topics needs to be an int or 'auto'! \")\n",
    "\n",
    "        return documents\n",
    "\n",
    "    def _reduce_to_n_topics(self, documents):\n",
    "        \"\"\" Reduce topics to self.nr_topics\n",
    "        Arguments:\n",
    "            documents: Dataframe with documents and their corresponding IDs and Topics\n",
    "        Returns:\n",
    "            documents: Updated dataframe with documents and the reduced number of Topics\n",
    "        \"\"\"\n",
    "        if not self.mapped_topics:\n",
    "            self.mapped_topics = {}\n",
    "        initial_nr_topics = len(self.get_topics())\n",
    "\n",
    "        # Create topic similarity matrix\n",
    "        similarities = cosine_similarity(self.c_tf_idf)\n",
    "        np.fill_diagonal(similarities, 0)\n",
    "\n",
    "        while len(self.get_topic_freq()) > self.nr_topics + 1:\n",
    "            # Find most similar topic to least common topic\n",
    "            topic_to_merge = self.get_topic_freq().iloc[-1].Topic\n",
    "            topic_to_merge_into = np.argmax(similarities[topic_to_merge + 1]) - 1\n",
    "            similarities[:, topic_to_merge + 1] = -1\n",
    "\n",
    "            # Update Topic labels\n",
    "            documents.loc[documents.Topic == topic_to_merge, \"Topic\"] = topic_to_merge_into\n",
    "            self.mapped_topics[topic_to_merge] = topic_to_merge_into\n",
    "\n",
    "            # Update new topic content\n",
    "            self._update_topic_size(documents)\n",
    "\n",
    "        self._extract_topics(documents)\n",
    "\n",
    "        if initial_nr_topics <= self.nr_topics:\n",
    "            logger.info(f\"Since {initial_nr_topics} were found, they could not be reduced to {self.nr_topics}\")\n",
    "        else:\n",
    "            logger.info(f\"Reduced number of topics from {initial_nr_topics} to {len(self.get_topic_freq())}\")\n",
    "\n",
    "        return documents\n",
    "\n",
    "    def _auto_reduce_topics(self, documents):\n",
    "        \"\"\" Reduce the number of topics as long as it exceeds a minimum similarity of 0.9\n",
    "        Arguments:\n",
    "            documents: Dataframe with documents and their corresponding IDs and Topics\n",
    "        Returns:\n",
    "            documents: Updated dataframe with documents and the reduced number of Topics\n",
    "        \"\"\"\n",
    "        initial_nr_topics = len(self.get_topics())\n",
    "        has_mapped = []\n",
    "        if not self.mapped_topics:\n",
    "            self.mapped_topics = {}\n",
    "\n",
    "        # Create topic similarity matrix\n",
    "        similarities = cosine_similarity(self.c_tf_idf)\n",
    "        np.fill_diagonal(similarities, 0)\n",
    "\n",
    "        # Do not map the top 10% most frequent topics\n",
    "        not_mapped = int(np.ceil(len(self.get_topic_freq()) * 0.1))\n",
    "        to_map = self.get_topic_freq().Topic.values[not_mapped:][::-1]\n",
    "\n",
    "        for topic_to_merge in to_map:\n",
    "            # Find most similar topic to least common topic\n",
    "            similarity = np.max(similarities[topic_to_merge + 1])\n",
    "            topic_to_merge_into = np.argmax(similarities[topic_to_merge + 1]) - 1\n",
    "\n",
    "            # Only map topics if they have a high similarity\n",
    "            if (similarity > 0.915) & (topic_to_merge_into not in has_mapped):\n",
    "                # Update Topic labels\n",
    "                documents.loc[documents.Topic == topic_to_merge, \"Topic\"] = topic_to_merge_into\n",
    "                self.mapped_topics[topic_to_merge] = topic_to_merge_into\n",
    "                similarities[:, topic_to_merge + 1] = -1\n",
    "\n",
    "                # Update new topic content\n",
    "                self._update_topic_size(documents)\n",
    "                has_mapped.append(topic_to_merge)\n",
    "\n",
    "        _ = self._extract_topics(documents)\n",
    "\n",
    "        logger.info(f\"Reduced number of topics from {initial_nr_topics} to {len(self.get_topic_freq())}\")\n",
    "\n",
    "        return documents\n",
    "\n",
    "    def _map_probabilities(self, probabilities: Union[np.ndarray, None]) -> Union[np.ndarray, None]:\n",
    "        \"\"\" Map the probabilities to the reduced topics.\n",
    "        This is achieved by adding the probabilities together\n",
    "        of all topics that were mapped to the same topic. Then,\n",
    "        the topics that were mapped from were set to 0 as they\n",
    "        were reduced.\n",
    "        Arguments:\n",
    "            probabilities: An array containing probabilities\n",
    "        Returns:\n",
    "            probabilities: Updated probabilities\n",
    "        \"\"\"\n",
    "        if isinstance(probabilities, np.ndarray):\n",
    "            for from_topic, to_topic in self.mapped_topics.items():\n",
    "                probabilities[:, to_topic] += probabilities[:, from_topic]\n",
    "                probabilities[:, from_topic] = 0\n",
    "\n",
    "            return probabilities.round(3)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    @staticmethod\n",
    "    def _plotly_topic_visualization(df: pd.DataFrame,\n",
    "                                    topic_list: List[str]):\n",
    "        \"\"\" Create plotly-based visualization of topics with a slider for topic selection \"\"\"\n",
    "\n",
    "        def get_color(topic_selected):\n",
    "            if topic_selected == -1:\n",
    "                marker_color = [\"#B0BEC5\" for _ in topic_list[1:]]\n",
    "            else:\n",
    "                marker_color = [\"red\" if topic == topic_selected else \"#B0BEC5\" for topic in topic_list[1:]]\n",
    "            return [{'marker.color': [marker_color]}]\n",
    "\n",
    "        # Prepare figure range\n",
    "        x_range = (df.x.min() - abs((df.x.min()) * .15), df.x.max() + abs((df.x.max()) * .15))\n",
    "        y_range = (df.y.min() - abs((df.y.min()) * .15), df.y.max() + abs((df.y.max()) * .15))\n",
    "\n",
    "        # Plot topics\n",
    "        fig = px.scatter(df, x=\"x\", y=\"y\", size=\"Size\", size_max=40, template=\"simple_white\", labels={\"x\": \"\", \"y\": \"\"},\n",
    "                         hover_data={\"x\": False, \"y\": False, \"Topic\": True, \"Words\": True, \"Size\": True})\n",
    "        fig.update_traces(marker=dict(color=\"#B0BEC5\", line=dict(width=2, color='DarkSlateGrey')))\n",
    "\n",
    "        # Update hover order\n",
    "        fig.update_traces(hovertemplate=\"<br>\".join([\"<b>Topic %{customdata[2]}</b>\",\n",
    "                                                     \"Words: %{customdata[3]}\",\n",
    "                                                     \"Size: %{customdata[4]}\"]))\n",
    "\n",
    "        # Create a slider for topic selection\n",
    "        steps = [dict(label=f\"Topic {topic}\", method=\"update\", args=get_color(topic)) for topic in topic_list[1:]]\n",
    "        sliders = [dict(active=0, pad={\"t\": 50}, steps=steps)]\n",
    "\n",
    "        # Stylize layout\n",
    "        fig.update_layout(\n",
    "            title={\n",
    "                'text': \"<b>Intertopic Distance Map\",\n",
    "                'y': .95,\n",
    "                'x': 0.5,\n",
    "                'xanchor': 'center',\n",
    "                'yanchor': 'top',\n",
    "                'font': dict(\n",
    "                    size=22,\n",
    "                    color=\"Black\")\n",
    "            },\n",
    "            width=650,\n",
    "            height=650,\n",
    "            hoverlabel=dict(\n",
    "                bgcolor=\"white\",\n",
    "                font_size=16,\n",
    "                font_family=\"Rockwell\"\n",
    "            ),\n",
    "            xaxis={\"visible\": False},\n",
    "            yaxis={\"visible\": False},\n",
    "            sliders=sliders\n",
    "        )\n",
    "\n",
    "        # Update axes ranges\n",
    "        fig.update_xaxes(range=x_range)\n",
    "        fig.update_yaxes(range=y_range)\n",
    "\n",
    "        # Add grid in a 'plus' shape\n",
    "        fig.add_shape(type=\"line\",\n",
    "                      x0=sum(x_range) / 2, y0=y_range[0], x1=sum(x_range) / 2, y1=y_range[1],\n",
    "                      line=dict(color=\"#CFD8DC\", width=2))\n",
    "        fig.add_shape(type=\"line\",\n",
    "                      x0=x_range[0], y0=sum(y_range) / 2, x1=y_range[1], y1=sum(y_range) / 2,\n",
    "                      line=dict(color=\"#9E9E9E\", width=2))\n",
    "        fig.add_annotation(x=x_range[0], y=sum(y_range) / 2, text=\"D1\", showarrow=False, yshift=10)\n",
    "        fig.add_annotation(y=y_range[1], x=sum(x_range) / 2, text=\"D2\", showarrow=False, xshift=10)\n",
    "        fig.data = fig.data[::-1]\n",
    "\n",
    "        fig.show()\n",
    "\n",
    "    def _preprocess_text(self, documents: np.ndarray) -> List[str]:\n",
    "        \"\"\" Basic preprocessing of text\n",
    "        Steps:\n",
    "            * Lower text\n",
    "            * Replace \\n and \\t with whitespace\n",
    "            * Only keep alpha-numerical characters\n",
    "        \"\"\"\n",
    "        cleaned_documents = [doc.lower() for doc in documents]\n",
    "        cleaned_documents = [doc.replace(\"\\n\", \" \") for doc in cleaned_documents]\n",
    "        cleaned_documents = [doc.replace(\"\\t\", \" \") for doc in cleaned_documents]\n",
    "        if self.language == \"english\":\n",
    "            cleaned_documents = [re.sub(r'[^A-Za-z0-9 ]+', '', doc) for doc in cleaned_documents]\n",
    "        cleaned_documents = [doc if doc != \"\" else \"emptydoc\" for doc in cleaned_documents]\n",
    "        return cleaned_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run bert topic modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from bertopic import BERTopic\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    " \n",
    "docs = fetch_20newsgroups(subset='all',  remove=('headers', 'footers', 'quotes'))['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nI am sure some bashers of Pens fans are pretty confused about the lack\\nof any kind of posts about the recent Pens massacre of the Devils. Actually,\\nI am  bit puzzled too and a bit relieved. However, I am going to put an end\\nto non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\\nare killing those Devils worse than I thought. Jagr just showed you why\\nhe is much better than his regular season stats. He is also a lot\\nfo fun to watch in the playoffs. Bowman should let JAgr have a lot of\\nfun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final\\nregular season game.          PENS RULE!!!\\n\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = BERTopic(language=\"english\")\n",
    "# topics, probs = model.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-1 refers to all outliers and should typically be ignored. Next, let's take a look at the most frequent topic that was generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.get_topic_freq().head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.get_topic(49)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bertopic import languages\n",
    "# print(languages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can select any model from sentence-transformers and use it instead of the preselected models by simply passing the model through\n",
    "BERTopic with embedding_model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.sbert.net/docs/pretrained_models.html for a list of supported sentence transformers models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_model = BERTopic(embedding_model=\"xlm-r-bert-base-nli-stsb-mean-tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics, probs = st_model.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_model.get_topic_freq().head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_model.get_topic(49)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.visualize_distribution(probs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can also reduce the number of topics after having trained a BERTopic model. The advantage of doing so, is that you can decide the number of topics after knowing how many are actually created. It is difficult to predict before training your model how many topics that are in your documents and how many will be extracted. Instead, we can decide afterwards how many topics seems realistic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_topics, new_probs = model.reduce_topics(docs, topics, probs, nr_topics=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you have trained a model and viewed the topics and the words that represent them, you might not be satisfied with the representation. Perhaps you forgot to remove stop_words or you want to try out a different n_gram_range. We can use the function update_topics to update the topic representation with new parameters for c-TF-IDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.update_topics(docs, topics, n_gram_range=(1, 3), stop_words=\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_topics, similarity = model.find_topics(\"vehicle\", top_n=5); similar_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_topic(28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save(\"my_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "my_model = BERTopic.load(\"my_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
