{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: keybert in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (0.1.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from keybert) (1.20.1)\n",
      "Requirement already satisfied: scikit-learn>=0.22.2 in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from keybert) (0.24.1)\n",
      "Requirement already satisfied: sentence-transformers>=0.3.8 in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from keybert) (0.3.9)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn>=0.22.2->keybert) (1.0.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn>=0.22.2->keybert) (1.6.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn>=0.22.2->keybert) (2.1.0)\n",
      "Collecting transformers<3.6.0,>=3.1.0\n",
      "  Downloading transformers-3.5.1-py3-none-any.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 2.9 MB/s \n",
      "\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from sentence-transformers>=0.3.8->keybert) (1.7.1)\n",
      "Requirement already satisfied: nltk in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from sentence-transformers>=0.3.8->keybert) (3.4.5)\n",
      "Requirement already satisfied: tqdm in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from sentence-transformers>=0.3.8->keybert) (4.56.0)\n",
      "Requirement already satisfied: sentencepiece==0.1.91 in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from transformers<3.6.0,>=3.1.0->sentence-transformers>=0.3.8->keybert) (0.1.91)\n",
      "Requirement already satisfied: sacremoses in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from transformers<3.6.0,>=3.1.0->sentence-transformers>=0.3.8->keybert) (0.0.43)\n",
      "Collecting tokenizers==0.9.3\n",
      "  Downloading tokenizers-0.9.3-cp38-cp38-macosx_10_11_x86_64.whl (2.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.0 MB 31.8 MB/s \n",
      "\u001b[?25hRequirement already satisfied: protobuf in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from transformers<3.6.0,>=3.1.0->sentence-transformers>=0.3.8->keybert) (3.13.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from transformers<3.6.0,>=3.1.0->sentence-transformers>=0.3.8->keybert) (2020.11.13)\n",
      "Requirement already satisfied: filelock in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from transformers<3.6.0,>=3.1.0->sentence-transformers>=0.3.8->keybert) (3.0.12)\n",
      "Requirement already satisfied: requests in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from transformers<3.6.0,>=3.1.0->sentence-transformers>=0.3.8->keybert) (2.25.1)\n",
      "Requirement already satisfied: packaging in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from transformers<3.6.0,>=3.1.0->sentence-transformers>=0.3.8->keybert) (20.9)\n",
      "Requirement already satisfied: typing-extensions in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (3.7.4.2)\n",
      "Requirement already satisfied: six in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from nltk->sentence-transformers>=0.3.8->keybert) (1.15.0)\n",
      "Requirement already satisfied: click in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from sacremoses->transformers<3.6.0,>=3.1.0->sentence-transformers>=0.3.8->keybert) (7.1.2)\n",
      "Requirement already satisfied: setuptools in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from protobuf->transformers<3.6.0,>=3.1.0->sentence-transformers>=0.3.8->keybert) (53.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from requests->transformers<3.6.0,>=3.1.0->sentence-transformers>=0.3.8->keybert) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from requests->transformers<3.6.0,>=3.1.0->sentence-transformers>=0.3.8->keybert) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from requests->transformers<3.6.0,>=3.1.0->sentence-transformers>=0.3.8->keybert) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from requests->transformers<3.6.0,>=3.1.0->sentence-transformers>=0.3.8->keybert) (1.26.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from packaging->transformers<3.6.0,>=3.1.0->sentence-transformers>=0.3.8->keybert) (2.4.7)\n",
      "\u001b[31mERROR: tner 0.0.0 has requirement jinja2==2.11.2, but you'll have jinja2 2.11.3 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: textattack 0.2.14 has requirement numpy<1.19.0, but you'll have numpy 1.20.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: textattack 0.2.14 has requirement scipy==1.4.1, but you'll have scipy 1.6.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: textattack 0.2.14 has requirement tokenizers==0.8.1-rc2, but you'll have tokenizers 0.9.3 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: textattack 0.2.14 has requirement tqdm<4.50.0,>=4.27, but you'll have tqdm 4.56.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: textattack 0.2.14 has requirement transformers==3.3.0, but you'll have transformers 3.5.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: simpletransformers 0.51.13 has requirement transformers>=4.0.0, but you'll have transformers 3.5.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: medacy 1.0.0 has requirement gensim==3.8.0, but you'll have gensim 3.8.3 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: medacy 1.0.0 has requirement msgpack<0.6,>=0.3.0, but you'll have msgpack 1.0.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: medacy 1.0.0 has requirement spacy==2.2.2, but you'll have spacy 3.0.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: medacy 1.0.0 has requirement transformers==2.3.0, but you'll have transformers 3.5.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: allennlp 2.0.1 has requirement spacy<2.4,>=2.1.0, but you'll have spacy 3.0.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: allennlp 2.0.1 has requirement transformers<4.3,>=4.1, but you'll have transformers 3.5.1 which is incompatible.\u001b[0m\n",
      "Installing collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.9.4\n",
      "    Uninstalling tokenizers-0.9.4:\n",
      "      Successfully uninstalled tokenizers-0.9.4\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.2.2\n",
      "    Uninstalling transformers-4.2.2:\n",
      "      Successfully uninstalled transformers-4.2.2\n",
      "Successfully installed tokenizers-0.9.3 transformers-3.5.1\n"
     ]
    }
   ],
   "source": [
    "#!pip install keybert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "def mmr(doc_embedding: np.ndarray,\n",
    "        word_embeddings: np.ndarray,\n",
    "        words: List[str],\n",
    "        top_n: int = 5,\n",
    "        diversity: float = 0.8) -> List[Tuple[str, float]]:\n",
    "    \"\"\" Calculate Maximal Marginal Relevance (MMR)\n",
    "    between candidate keywords and the document.\n",
    "    MMR considers the similarity of keywords/keyphrases with the\n",
    "    document, along with the similarity of already selected\n",
    "    keywords and keyphrases. This results in a selection of keywords\n",
    "    that maximize their within diversity with respect to the document.\n",
    "    Arguments:\n",
    "        doc_embedding: The document embeddings\n",
    "        word_embeddings: The embeddings of the selected candidate keywords/phrases\n",
    "        words: The selected candidate keywords/keyphrases\n",
    "        top_n: The number of keywords/keyhprases to return\n",
    "        diversity: How diverse the select keywords/keyphrases are.\n",
    "                   Values between 0 and 1 with 0 being not diverse at all\n",
    "                   and 1 being most diverse.\n",
    "    Returns:\n",
    "         List[Tuple[str, float]]: The selected keywords/keyphrases with their distances\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract similarity within words, and between words and the document\n",
    "    word_doc_similarity = cosine_similarity(word_embeddings, doc_embedding)\n",
    "    word_similarity = cosine_similarity(word_embeddings)\n",
    "\n",
    "    # Initialize candidates and already choose best keyword/keyphras\n",
    "    keywords_idx = [np.argmax(word_doc_similarity)]\n",
    "    candidates_idx = [i for i in range(len(words)) if i != keywords_idx[0]]\n",
    "\n",
    "    for _ in range(top_n - 1):\n",
    "        # Extract similarities within candidates and\n",
    "        # between candidates and selected keywords/phrases\n",
    "        candidate_similarities = word_doc_similarity[candidates_idx, :]\n",
    "        target_similarities = np.max(word_similarity[candidates_idx][:, keywords_idx], axis=1)\n",
    "\n",
    "        # Calculate MMR\n",
    "        mmr = (1-diversity) * candidate_similarities - diversity * target_similarities.reshape(-1, 1)\n",
    "        mmr_idx = candidates_idx[np.argmax(mmr)]\n",
    "\n",
    "        # Update keywords & candidates\n",
    "        keywords_idx.append(mmr_idx)\n",
    "        candidates_idx.remove(mmr_idx)\n",
    "\n",
    "    return [(words[idx], round(float(word_doc_similarity.reshape(1, -1)[0][idx]), 4)) for idx in keywords_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "def max_sum_similarity(doc_embedding: np.ndarray,\n",
    "                       word_embeddings: np.ndarray,\n",
    "                       words: List[str],\n",
    "                       top_n: int,\n",
    "                       nr_candidates: int) -> List[Tuple[str, float]]:\n",
    "    \"\"\" Calculate Max Sum Distance for extraction of keywords\n",
    "    We take the 2 x top_n most similar words/phrases to the document.\n",
    "    Then, we take all top_n combinations from the 2 x top_n words and\n",
    "    extract the combination that are the least similar to each other\n",
    "    by cosine similarity.\n",
    "    NOTE:\n",
    "        This is O(n^2) and therefore not advised if you use a large top_n\n",
    "    Arguments:\n",
    "        doc_embedding: The document embeddings\n",
    "        word_embeddings: The embeddings of the selected candidate keywords/phrases\n",
    "        words: The selected candidate keywords/keyphrases\n",
    "        top_n: The number of keywords/keyhprases to return\n",
    "        nr_candidates: The number of candidates to consider\n",
    "    Returns:\n",
    "         List[Tuple[str, float]]: The selected keywords/keyphrases with their distances\n",
    "    \"\"\"\n",
    "    if nr_candidates < top_n:\n",
    "        raise Exception(\"Make sure that the number of candidates exceeds the number \"\n",
    "                        \"of keywords to return.\")\n",
    "\n",
    "    # Calculate distances and extract keywords\n",
    "    distances = cosine_similarity(doc_embedding, word_embeddings)\n",
    "    distances_words = cosine_similarity(word_embeddings, word_embeddings)\n",
    "\n",
    "    # Get 2*top_n words as candidates based on cosine similarity\n",
    "    words_idx = list(distances.argsort()[0][-nr_candidates:])\n",
    "    words_vals = [words[index] for index in words_idx]\n",
    "    candidates = distances_words[np.ix_(words_idx, words_idx)]\n",
    "\n",
    "    # Calculate the combination of words that are the least similar to each other\n",
    "    min_sim = 100_000\n",
    "    candidate = None\n",
    "    for combination in itertools.combinations(range(len(words_idx)), top_n):\n",
    "        sim = sum([candidates[i][j] for i in combination for j in combination if i != j])\n",
    "        if sim < min_sim:\n",
    "            candidate = combination\n",
    "            min_sim = sim\n",
    "\n",
    "    return [(words_vals[idx], round(float(distances[0][idx]), 4)) for idx in candidate]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from typing import List, Union, Tuple\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Flair\n",
    "try:\n",
    "    from flair.embeddings import DocumentEmbeddings, TokenEmbeddings, DocumentPoolEmbeddings\n",
    "    from flair.data import Sentence\n",
    "    _HAS_FLAIR = True\n",
    "except ModuleNotFoundError as e:\n",
    "    DocumentEmbeddings, TokenEmbeddings, DocumentPoolEmbeddings = None, None, None\n",
    "    _HAS_FLAIR = False\n",
    "\n",
    "\n",
    "class KeyBERT:\n",
    "    \"\"\"\n",
    "    A minimal method for keyword extraction with BERT\n",
    "    The keyword extraction is done by finding the sub-phrases in\n",
    "    a document that are the most similar to the document itself.\n",
    "    First, document embeddings are extracted with BERT to get a\n",
    "    document-level representation. Then, word embeddings are extracted\n",
    "    for N-gram words/phrases. Finally, we use cosine similarity to find the\n",
    "    words/phrases that are the most similar to the document.\n",
    "    The most similar words could then be identified as the words that\n",
    "    best describe the entire document.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 model: Union[str,\n",
    "                              SentenceTransformer,\n",
    "                              DocumentEmbeddings,\n",
    "                              TokenEmbeddings] = 'distilbert-base-nli-mean-tokens'):\n",
    "        \"\"\" KeyBERT initialization\n",
    "        Arguments:\n",
    "            model: Use a custom embedding model. You can pass in a string related\n",
    "                   to one of the following models:\n",
    "                   https://www.sbert.net/docs/pretrained_models.html\n",
    "                   You can also pass in a SentenceTransformer() model or a Flair\n",
    "                   DocumentEmbedding model.\n",
    "        \"\"\"\n",
    "        self.model = self._select_embedding_model(model)\n",
    "\n",
    "    def extract_keywords(self,\n",
    "                         docs: Union[str, List[str]],\n",
    "                         keyphrase_ngram_range: Tuple[int, int] = (1, 1),\n",
    "                         stop_words: Union[str, List[str]] = 'english',\n",
    "                         top_n: int = 5,\n",
    "                         min_df: int = 1,\n",
    "                         use_maxsum: bool = False,\n",
    "                         use_mmr: bool = False,\n",
    "                         diversity: float = 0.5,\n",
    "                         nr_candidates: int = 20,\n",
    "                         vectorizer: CountVectorizer = None) -> Union[List[Tuple[str, float]],\n",
    "                                                                      List[List[Tuple[str, float]]]]:\n",
    "        \"\"\" Extract keywords/keyphrases\n",
    "        NOTE:\n",
    "            I would advise you to iterate over single documents as they\n",
    "            will need the least amount of memory. Even though this is slower,\n",
    "            you are not likely to run into memory errors.\n",
    "        Multiple Documents:\n",
    "            There is an option to extract keywords for multiple documents\n",
    "            that is faster than extraction for multiple single documents.\n",
    "            However...this method assumes that you can keep the word embeddings\n",
    "            for all words in the vocabulary in memory which might be troublesome.\n",
    "            I would advise against using this option and simply iterating\n",
    "            over documents instead if you have limited hardware.\n",
    "        Arguments:\n",
    "            docs: The document(s) for which to extract keywords/keyphrases\n",
    "            keyphrase_ngram_range: Length, in words, of the extracted keywords/keyphrases\n",
    "            stop_words: Stopwords to remove from the document\n",
    "            top_n: Return the top n keywords/keyphrases\n",
    "            min_df: Minimum document frequency of a word across all documents\n",
    "                    if keywords for multiple documents need to be extracted\n",
    "            use_maxsum: Whether to use Max Sum Similarity for the selection\n",
    "                        of keywords/keyphrases\n",
    "            use_mmr: Whether to use Maximal Marginal Relevance (MMR) for the\n",
    "                     selection of keywords/keyphrases\n",
    "            diversity: The diversity of the results between 0 and 1 if use_mmr\n",
    "                       is set to True\n",
    "            nr_candidates: The number of candidates to consider if use_maxsum is\n",
    "                           set to True\n",
    "            vectorizer: Pass in your own CountVectorizer from scikit-learn\n",
    "        Returns:\n",
    "            keywords: the top n keywords for a document with their respective distances\n",
    "                      to the input document\n",
    "        \"\"\"\n",
    "\n",
    "        if isinstance(docs, str):\n",
    "            return self._extract_keywords_single_doc(docs,\n",
    "                                                     keyphrase_ngram_range,\n",
    "                                                     stop_words,\n",
    "                                                     top_n,\n",
    "                                                     use_maxsum,\n",
    "                                                     use_mmr,\n",
    "                                                     diversity,\n",
    "                                                     nr_candidates,\n",
    "                                                     vectorizer)\n",
    "        elif isinstance(docs, list):\n",
    "            warnings.warn(\"Although extracting keywords for multiple documents is faster \"\n",
    "                          \"than iterating over single documents, it requires significantly more memory \"\n",
    "                          \"to hold all word embeddings. Use this at your own discretion!\")\n",
    "            return self._extract_keywords_multiple_docs(docs,\n",
    "                                                        keyphrase_ngram_range,\n",
    "                                                        stop_words,\n",
    "                                                        top_n,\n",
    "                                                        min_df,\n",
    "                                                        vectorizer)\n",
    "\n",
    "    def _extract_keywords_single_doc(self,\n",
    "                                     doc: str,\n",
    "                                     keyphrase_ngram_range: Tuple[int, int] = (1, 1),\n",
    "                                     stop_words: Union[str, List[str]] = 'english',\n",
    "                                     top_n: int = 5,\n",
    "                                     use_maxsum: bool = False,\n",
    "                                     use_mmr: bool = False,\n",
    "                                     diversity: float = 0.5,\n",
    "                                     nr_candidates: int = 20,\n",
    "                                     vectorizer: CountVectorizer = None) -> List[Tuple[str, float]]:\n",
    "        \"\"\" Extract keywords/keyphrases for a single document\n",
    "        Arguments:\n",
    "            doc: The document for which to extract keywords/keyphrases\n",
    "            keyphrase_ngram_range: Length, in words, of the extracted keywords/keyphrases\n",
    "            stop_words: Stopwords to remove from the document\n",
    "            top_n: Return the top n keywords/keyphrases\n",
    "            use_mmr: Whether to use Max Sum Similarity\n",
    "            use_mmr: Whether to use MMR\n",
    "            diversity: The diversity of results between 0 and 1 if use_mmr is True\n",
    "            nr_candidates: The number of candidates to consider if use_maxsum is set to True\n",
    "            vectorizer: Pass in your own CountVectorizer from scikit-learn\n",
    "        Returns:\n",
    "            keywords: the top n keywords for a document with their respective distances\n",
    "                      to the input document\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Extract Words\n",
    "            if vectorizer:\n",
    "                count = vectorizer.fit([doc])\n",
    "            else:\n",
    "                count = CountVectorizer(ngram_range=keyphrase_ngram_range, stop_words=stop_words).fit([doc])\n",
    "            words = count.get_feature_names()\n",
    "\n",
    "            # Extract Embeddings\n",
    "            doc_embedding = self._extract_embeddings([doc])\n",
    "            word_embeddings = self._extract_embeddings(words)\n",
    "            # doc_embedding = self.model.encode([doc])\n",
    "            # word_embeddings = self.model.encode(words)\n",
    "\n",
    "            # Calculate distances and extract keywords\n",
    "            if use_mmr:\n",
    "                keywords = mmr(doc_embedding, word_embeddings, words, top_n, diversity)\n",
    "            elif use_maxsum:\n",
    "                keywords = max_sum_similarity(doc_embedding, word_embeddings, words, top_n, nr_candidates)\n",
    "            else:\n",
    "                distances = cosine_similarity(doc_embedding, word_embeddings)\n",
    "                keywords = [(words[index], round(float(distances[0][index]), 4))\n",
    "                            for index in distances.argsort()[0][-top_n:]][::-1]\n",
    "\n",
    "            return keywords\n",
    "        except ValueError:\n",
    "            return []\n",
    "\n",
    "    def _extract_keywords_multiple_docs(self,\n",
    "                                        docs: List[str],\n",
    "                                        keyphrase_ngram_range: Tuple[int, int] = (1, 1),\n",
    "                                        stop_words: str = 'english',\n",
    "                                        top_n: int = 5,\n",
    "                                        min_df: int = 1,\n",
    "                                        vectorizer: CountVectorizer = None) -> List[List[Tuple[str, float]]]:\n",
    "        \"\"\" Extract keywords/keyphrases for a multiple documents\n",
    "        This currently does not use MMR as\n",
    "        Arguments:\n",
    "            docs: The document for which to extract keywords/keyphrases\n",
    "            keyphrase_ngram_range: Length, in words, of the extracted keywords/keyphrases\n",
    "            stop_words: Stopwords to remove from the document\n",
    "            top_n: Return the top n keywords/keyphrases\n",
    "            min_df: The minimum frequency of words\n",
    "            vectorizer: Pass in your own CountVectorizer from scikit-learn\n",
    "        Returns:\n",
    "            keywords: the top n keywords for a document with their respective distances\n",
    "                      to the input document\n",
    "        \"\"\"\n",
    "        # Extract words\n",
    "        if vectorizer:\n",
    "            count = vectorizer.fit(docs)\n",
    "        else:\n",
    "            count = CountVectorizer(ngram_range=keyphrase_ngram_range, stop_words=stop_words, min_df=min_df).fit(docs)\n",
    "        words = count.get_feature_names()\n",
    "        df = count.transform(docs)\n",
    "\n",
    "        # Extract embeddings\n",
    "        word_embeddings = self._extract_embeddings(words)\n",
    "        doc_embeddings = self._extract_embeddings(docs)\n",
    "        # word_embeddings = self.model.encode(words, show_progress_bar=True)\n",
    "        # doc_embeddings = self.model.encode(docs, show_progress_bar=True)\n",
    "\n",
    "        # Extract keywords\n",
    "        keywords = []\n",
    "        for index, doc in tqdm(enumerate(docs)):\n",
    "            doc_words = [words[i] for i in df[index].nonzero()[1]]\n",
    "\n",
    "            if doc_words:\n",
    "                doc_word_embeddings = np.array([word_embeddings[i] for i in df[index].nonzero()[1]])\n",
    "                distances = cosine_similarity([doc_embeddings[index]], doc_word_embeddings)[0]\n",
    "                doc_keywords = [(doc_words[i], round(float(distances[i]), 4)) for i in distances.argsort()[-top_n:]]\n",
    "                keywords.append(doc_keywords)\n",
    "            else:\n",
    "                keywords.append([\"None Found\"])\n",
    "\n",
    "        return keywords\n",
    "\n",
    "    def _extract_embeddings(self, documents: Union[List[str], str]) -> np.ndarray:\n",
    "        \"\"\" Extract sentence/document embeddings through pre-trained embeddings\n",
    "        For an overview of pre-trained models: https://www.sbert.net/docs/pretrained_models.html\n",
    "        Arguments:\n",
    "            documents: Dataframe with documents and their corresponding IDs\n",
    "        Returns:\n",
    "            embeddings: The extracted embeddings using the sentence transformer\n",
    "                        module. Typically uses pre-trained huggingface models.\n",
    "        \"\"\"\n",
    "        if isinstance(documents, str):\n",
    "            documents = [documents]\n",
    "\n",
    "        # Infer embeddings with SentenceTransformer\n",
    "        if isinstance(self.model, SentenceTransformer):\n",
    "            embeddings = self.model.encode(documents)\n",
    "\n",
    "        # Infer embeddings with Flair\n",
    "        elif isinstance(self.model, DocumentEmbeddings):\n",
    "            embeddings = []\n",
    "            for index, document in enumerate(documents):\n",
    "                try:\n",
    "                    sentence = Sentence(document) if document else Sentence(\"an empty document\")\n",
    "                    self.model.embed(sentence)\n",
    "                except RuntimeError:\n",
    "                    sentence = Sentence(\"an empty document\")\n",
    "                    self.model.embed(sentence)\n",
    "                embedding = sentence.embedding.detach().cpu().numpy()\n",
    "                embeddings.append(embedding)\n",
    "            embeddings = np.asarray(embeddings)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"An incorrect embedding model type was selected.\")\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    def _select_embedding_model(self, model: Union[str,\n",
    "                                                   SentenceTransformer,\n",
    "                                                   DocumentEmbeddings,\n",
    "                                                   TokenEmbeddings]) -> Union[SentenceTransformer,\n",
    "                                                                              DocumentEmbeddings]:\n",
    "        \"\"\" Select an embedding model based on language or a specific sentence transformer models.\n",
    "        When selecting a language, we choose distilbert-base-nli-stsb-mean-tokens for English and\n",
    "        xlm-r-bert-base-nli-stsb-mean-tokens for all other languages as it support 100+ languages.\n",
    "        Arguments:\n",
    "            model: Use a custom embedding model. You can pass in a string related\n",
    "                   to one of the following models:\n",
    "                   https://www.sbert.net/docs/pretrained_models.html\n",
    "                   You can also pass in a SentenceTransformer() model or a Flair\n",
    "                   DocumentEmbedding model.\n",
    "        Returns:\n",
    "            model: Either a Sentence-Transformer or Flair model\n",
    "        \"\"\"\n",
    "\n",
    "        # Sentence Transformer embeddings\n",
    "        if isinstance(model, SentenceTransformer):\n",
    "            return model\n",
    "\n",
    "        # Flair word embeddings\n",
    "        elif _HAS_FLAIR and isinstance(model, TokenEmbeddings):\n",
    "            return DocumentPoolEmbeddings([model])\n",
    "\n",
    "        # Flair document embeddings + disable fine tune to prevent CUDA OOM\n",
    "        # https://github.com/flairNLP/flair/issues/1719\n",
    "        elif _HAS_FLAIR and isinstance(model, DocumentEmbeddings):\n",
    "            if \"fine_tune\" in model.__dict__:\n",
    "                model.fine_tune = False\n",
    "            return model\n",
    "\n",
    "        # Select embedding model based on specific sentence transformer model\n",
    "        elif isinstance(model, str):\n",
    "            return SentenceTransformer(model)\n",
    "\n",
    "        return SentenceTransformer(\"xlm-r-bert-base-nli-stsb-mean-tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"\"\"\n",
    "         Supervised learning is the machine learning task of learning a function that\n",
    "         maps an input to an output based on example input-output pairs.[1] It infers a\n",
    "         function from labeled training data consisting of a set of training examples.[2]\n",
    "         In supervised learning, each example is a pair consisting of an input object\n",
    "         (typically a vector) and a desired output value (also called the supervisory signal). \n",
    "         A supervised learning algorithm analyzes the training data and produces an inferred function, \n",
    "         which can be used for mapping new examples. An optimal scenario will allow for the \n",
    "         algorithm to correctly determine the class labels for unseen instances. This requires \n",
    "         the learning algorithm to generalize from the training data to unseen situations in a \n",
    "         'reasonable' way (see inductive bias).\n",
    "      \"\"\"\n",
    "model = KeyBERT('distilbert-base-nli-mean-tokens')\n",
    "keywords = model.extract_keywords(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('learning', 0.4762),\n",
       " ('training', 0.4679),\n",
       " ('algorithm', 0.4562),\n",
       " ('class', 0.4259),\n",
       " ('mapping', 0.3733)]"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "model.extract_keywords(doc, stop_words=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('learning algorithm', 0.7061),\n",
       " ('machine learning', 0.6435),\n",
       " ('supervised learning', 0.6028),\n",
       " ('learning function', 0.5982),\n",
       " ('algorithm analyzes', 0.59)]"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "model.extract_keywords(doc, keyphrase_ngram_range=(1, 2), stop_words=None)"
   ]
  },
  {
   "source": [
    "# Max Sum Similarity"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('set training examples', 0.7632),\n",
       " ('generalize training data', 0.7825),\n",
       " ('requires learning algorithm', 0.2892),\n",
       " ('supervised learning algorithm', 0.3836),\n",
       " ('learning machine learning', 0.3873)]"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "model.extract_keywords(doc, keyphrase_ngram_range=(3, 3), stop_words='english', use_maxsum=True, nr_candidates=20, top_n=5)"
   ]
  },
  {
   "source": [
    "# Maximal Marginal Relevance"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('algorithm generalize training', 0.7825),\n",
       " ('labels unseen instances', 0.1559),\n",
       " ('new examples optimal', 0.4188),\n",
       " ('determine class labels', 0.4855),\n",
       " ('supervised learning algorithm', 0.7513)]"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "model.extract_keywords(doc, keyphrase_ngram_range=(3, 3), stop_words='english', use_mmr=True, diversity=0.7) # Diversity high "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('algorithm generalize training', 0.7825),\n",
       " ('learning machine learning', 0.7717),\n",
       " ('learning algorithm analyzes', 0.767),\n",
       " ('supervised learning algorithm', 0.7513),\n",
       " ('algorithm analyzes training', 0.7632)]"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "model.extract_keywords(doc, keyphrase_ngram_range=(3, 3), stop_words='english', use_mmr=True, diversity=0.2) # Diversity low"
   ]
  },
  {
   "source": [
    "# Embedding Models "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('algorithm generalize training', 0.7825),\n",
       " ('labels unseen instances', 0.1559),\n",
       " ('new examples optimal', 0.4188),\n",
       " ('determine class labels', 0.4855),\n",
       " ('supervised learning algorithm', 0.7513)]"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "sentence_model = SentenceTransformer(\"distilbert-base-nli-mean-tokens\", device=\"cpu\")\n",
    "model = KeyBERT(model=sentence_model)\n",
    "model.extract_keywords(doc, keyphrase_ngram_range=(3, 3), stop_words='english', use_mmr=True, diversity=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Downloading: 100%|██████████| 481/481 [00:00<00:00, 133kB/s]\n",
      "Downloading: 100%|██████████| 899k/899k [00:00<00:00, 1.51MB/s]\n",
      "Downloading: 100%|██████████| 456k/456k [00:00<00:00, 922kB/s]\n",
      "Downloading: 100%|██████████| 501M/501M [00:49<00:00, 10.1MB/s]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('supervised learning algorithm', 0.9317),\n",
       " ('used mapping new', 0.9279),\n",
       " ('output pairs infers', 0.9302),\n",
       " ('examples optimal scenario', 0.9308),\n",
       " ('labeled training data', 0.9315)]"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "from flair.embeddings import TransformerDocumentEmbeddings\n",
    "\n",
    "roberta = TransformerDocumentEmbeddings('roberta-base')\n",
    "model = KeyBERT(model=roberta)\n",
    "model.extract_keywords(doc, keyphrase_ngram_range=(3, 3), stop_words='english', use_mmr=True, diversity=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}