{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('anaconda3': virtualenv)",
   "metadata": {
    "interpreter": {
     "hash": "17f60a5376c77425fc92555942c601d199d1b6f4d9fe48b17446cfeffaeccd2f"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Topic Modelling - Genism"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "import numpy as np\n",
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.models import CoherenceModel, LdaModel, LsiModel, HdpModel\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "from gensim.corpora import Dictionary\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "import os, re, operator, warnings\n",
    "warnings.filterwarnings('ignore')  # Let's not pay heed to them right now\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the text \n",
    "def proprocess_text(string_list):\n",
    "    final_str_list = []\n",
    "    for s in string_list:\n",
    "        if len(s) > 15:\n",
    "            doc = nlp(s)\n",
    "            new_list =[]\n",
    "            for token in doc:\n",
    "                if token.pos_ == 'ADJ' or token.pos_ == 'ADV' or token.pos_ == 'NOUN' or token.pos_ == 'NUM' or token.pos_ == 'PROPN' or token.pos_ == 'VERB':\n",
    "                    new_list.append(token.text)\n",
    "            final_str_list.append(new_list)\n",
    "    return final_str_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(21053, 7)\n(20140, 7)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   Unnamed: 0                                  paper_id  \\\n",
       "0           0  0001418189999fea7f7cbe3e82703d71c85a6fe5   \n",
       "1           1  000affa746a03f1fe4e3b3ef1a62fdfa9b9ac52a   \n",
       "2           2  000e754142ba65ef77c6fdffcbcbe824e141ea7b   \n",
       "3           3  000eec3f1e93c3792454ac59415c928ce3a6b4ad   \n",
       "4           4  001259ae6d9bfa9376894f61aa6b6c5f18be2177   \n",
       "\n",
       "                                               title  \\\n",
       "0  Absence of surface expression of feline infect...   \n",
       "1  Correlation between antimicrobial consumption ...   \n",
       "2  Laboratory-based surveillance of hospital-acqu...   \n",
       "3  Pneumonie virale sévère de l'immunocompétent V...   \n",
       "4  Microheterogeneity of S-glycoprotein of mouse ...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  Feline infectious peritonitis virus (FIPV) pos...   \n",
       "1  Objectives: This study was conducted to invest...   \n",
       "2  Of 7,772 laboratory-confirmed cases of respira...   \n",
       "3  Reçu et accepté le 7 février 2004 Les infectio...   \n",
       "4  IEF, isoelectric focusing; NC, nitrocellulose;...   \n",
       "\n",
       "                                           body_text  \\\n",
       "0  Feline infectious peritonitis (FIP) is a fatal...   \n",
       "1  The incidence of health-care-associated infect...   \n",
       "2  The human respiratory viruses include adenovir...   \n",
       "3  Les pathologies infectieuses respiratoires son...   \n",
       "4  (Accepted 10 January 1992) is a neurotropic co...   \n",
       "\n",
       "                                           doi  \\\n",
       "0  http://doi.org/10.1016/j.vetmic.2006.11.026   \n",
       "1    http://doi.org/10.1016/j.jmii.2013.10.008   \n",
       "2    http://doi.org/10.1016/j.ajic.2017.01.009   \n",
       "3  http://doi.org/10.1016/j.reaurg.2004.02.009   \n",
       "4  http://doi.org/10.1016/0166-0934(92)90173-B   \n",
       "\n",
       "                                 title_abstract_body  \n",
       "0  Absence of surface expression of feline infect...  \n",
       "1  Correlation between antimicrobial consumption ...  \n",
       "2  Laboratory-based surveillance of hospital-acqu...  \n",
       "3  Pneumonie virale sévère de l'immunocompétent V...  \n",
       "4  Microheterogeneity of S-glycoprotein of mouse ...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>paper_id</th>\n      <th>title</th>\n      <th>abstract</th>\n      <th>body_text</th>\n      <th>doi</th>\n      <th>title_abstract_body</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0001418189999fea7f7cbe3e82703d71c85a6fe5</td>\n      <td>Absence of surface expression of feline infect...</td>\n      <td>Feline infectious peritonitis virus (FIPV) pos...</td>\n      <td>Feline infectious peritonitis (FIP) is a fatal...</td>\n      <td>http://doi.org/10.1016/j.vetmic.2006.11.026</td>\n      <td>Absence of surface expression of feline infect...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>000affa746a03f1fe4e3b3ef1a62fdfa9b9ac52a</td>\n      <td>Correlation between antimicrobial consumption ...</td>\n      <td>Objectives: This study was conducted to invest...</td>\n      <td>The incidence of health-care-associated infect...</td>\n      <td>http://doi.org/10.1016/j.jmii.2013.10.008</td>\n      <td>Correlation between antimicrobial consumption ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>000e754142ba65ef77c6fdffcbcbe824e141ea7b</td>\n      <td>Laboratory-based surveillance of hospital-acqu...</td>\n      <td>Of 7,772 laboratory-confirmed cases of respira...</td>\n      <td>The human respiratory viruses include adenovir...</td>\n      <td>http://doi.org/10.1016/j.ajic.2017.01.009</td>\n      <td>Laboratory-based surveillance of hospital-acqu...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>000eec3f1e93c3792454ac59415c928ce3a6b4ad</td>\n      <td>Pneumonie virale sévère de l'immunocompétent V...</td>\n      <td>Reçu et accepté le 7 février 2004 Les infectio...</td>\n      <td>Les pathologies infectieuses respiratoires son...</td>\n      <td>http://doi.org/10.1016/j.reaurg.2004.02.009</td>\n      <td>Pneumonie virale sévère de l'immunocompétent V...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>001259ae6d9bfa9376894f61aa6b6c5f18be2177</td>\n      <td>Microheterogeneity of S-glycoprotein of mouse ...</td>\n      <td>IEF, isoelectric focusing; NC, nitrocellulose;...</td>\n      <td>(Accepted 10 January 1992) is a neurotropic co...</td>\n      <td>http://doi.org/10.1016/0166-0934(92)90173-B</td>\n      <td>Microheterogeneity of S-glycoprotein of mouse ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "my_df = pd.read_csv(\"/Users/sdeshpande/Desktop/bioinformatices/full_articlesLDA.csv\")\n",
    "print(my_df.shape)\n",
    "my_df.dropna(subset = ['title'], inplace = True)\n",
    "print(my_df.shape)\n",
    "my_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "health_titles = my_df['title'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_titles = proprocess_text(health_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Absence',\n",
       " 'surface',\n",
       " 'expression',\n",
       " 'feline',\n",
       " 'infectious',\n",
       " 'peritonitis',\n",
       " 'virus',\n",
       " 'FIPV',\n",
       " 'antigens',\n",
       " 'infected',\n",
       " 'cells',\n",
       " 'isolated',\n",
       " 'cats',\n",
       " 'FIP']"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "processed_titles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "20140\n20047\n"
     ]
    }
   ],
   "source": [
    "print(len(health_titles))\n",
    "print(len(processed_titles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = gensim.models.Phrases(processed_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [bigram[line] for line in processed_titles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Absence',\n",
       " 'surface',\n",
       " 'expression',\n",
       " 'feline_infectious',\n",
       " 'peritonitis_virus',\n",
       " 'FIPV',\n",
       " 'antigens',\n",
       " 'infected_cells',\n",
       " 'isolated',\n",
       " 'cats',\n",
       " 'FIP']"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.760*\"virus\" + 0.282*\"infection\" + 0.139*\"coronavirus\" + 0.130*\"respiratory_syndrome\" + 0.119*\"protein\" + 0.108*\"viral\" + 0.101*\"human\" + 0.099*\"based\" + 0.098*\"RNA\" + 0.096*\"Virus\"'),\n",
       " (1,\n",
       "  '0.556*\"C\" + -0.359*\"virus\" + 0.327*\"Virus\" + 0.223*\"N\" + 0.198*\"A\" + 0.188*\"E\" + 0.168*\"R\" + 0.129*\"1\" + 0.121*\"infection\" + 0.093*\"S\"'),\n",
       " (2,\n",
       "  '0.545*\"Virus\" + -0.444*\"C\" + 0.334*\"infection\" + -0.301*\"virus\" + -0.122*\"R\" + -0.121*\"N\" + -0.113*\"E\" + 0.109*\"Middle_East\" + -0.106*\"A\" + 0.101*\"coronavirus\"'),\n",
       " (3,\n",
       "  '-0.624*\"infection\" + 0.565*\"Virus\" + 0.302*\"virus\" + -0.159*\"coronavirus\" + -0.127*\"viral\" + -0.105*\"respiratory_syndrome\" + 0.091*\"RNA\" + -0.080*\"human\" + -0.077*\"Middle_East\" + 0.065*\"Protein\"'),\n",
       " (4,\n",
       "  '-0.416*\"infection\" + -0.304*\"C\" + 0.267*\"N\" + -0.264*\"Virus\" + 0.248*\"1\" + 0.240*\"Middle_East\" + 0.219*\"coronavirus\" + 0.192*\"respiratory_syndrome\" + 0.168*\"Respiratory_Syndrome\" + 0.164*\"A\"')]"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "lsimodel = LsiModel(corpus=corpus, num_topics=10, id2word=dictionary)\n",
    "lsimodel.show_topics(num_topics=5)  # Showing only the top 5 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.001*virus + 0.001*infection + 0.001*analysis + 0.001*using + 0.001*specific + 0.001*cells + 0.001*coronavirus + 0.001*human + 0.001*1 + 0.001*detection + 0.000*Virus + 0.000*patients + 0.000*disease + 0.000*based + 0.000*induced + 0.000*2 + 0.000*\\u202bفي\\u202c + 0.000*China + 0.000*Novel + 0.000*respiratory_syndrome'),\n",
       " (1,\n",
       "  '0.001*virus + 0.001*infection + 0.001*E + 0.001*Virus + 0.001*2 + 0.001*Clinical + 0.001*coronavirus + 0.001*Infection + 0.001*children + 0.001*respiratory_syndrome + 0.001*trial + 0.001*influenza + 0.001*protein + 0.001*expression + 0.000*based + 0.000*Human + 0.000*Cells + 0.000*using + 0.000*RNA + 0.000*Respiratory'),\n",
       " (2,\n",
       "  '0.001*virus + 0.001*1 + 0.001*coronavirus + 0.001*viral + 0.001*novel + 0.001*N + 0.001*A + 0.001*Virus + 0.001*infection + 0.001*2 + 0.001*RNA + 0.001*3 + 0.001*analysis + 0.001*Human + 0.001*Protein + 0.001*cells + 0.000*characterization + 0.000*ovalbumin + 0.000*protein + 0.000*human'),\n",
       " (3,\n",
       "  '0.002*Respiratory_Syndrome + 0.001*Middle_East + 0.001*infection + 0.001*Virus + 0.001*virus + 0.001*Coronavirus + 0.001*Replication + 0.001*Infection + 0.001*based + 0.001*Hospital + 0.001*Management_Large + 0.001*RNA + 0.001*analysis + 0.000*human + 0.000*viral + 0.000*viruses + 0.000*Human + 0.000*coronavirus + 0.000*Sequence + 0.000*molecules'),\n",
       " (4,\n",
       "  '0.001*infection + 0.001*virus + 0.001*respiratory_syndrome + 0.001*transmission + 0.001*coronavirus + 0.001*China + 0.001*Score + 0.001*respiratory + 0.000*production + 0.000*pathogen + 0.000*using + 0.000*based + 0.000*study + 0.000*analysis + 0.000*potential + 0.000*viral + 0.000*adults + 0.000*COVID-19 + 0.000*Public_Health + 0.000*Health'),\n",
       " (5,\n",
       "  '0.001*N + 0.001*E + 0.001*A + 0.001*virus + 0.001*Virus + 0.001*D + 0.001*S + 0.001*R + 0.001*G + 0.001*O + 0.001*4 + 0.001*infection + 0.000*Y + 0.000*B + 0.000*evaluation + 0.000*Infections + 0.000*I_C + 0.000*E_M + 0.000*F + 0.000*Synthesis'),\n",
       " (6,\n",
       "  '0.001*virus + 0.001*infection + 0.001*based + 0.001*infections + 0.001*replication + 0.001*patients + 0.001*Virus + 0.000*viral + 0.000*human + 0.000*surveillance + 0.000*respiratory_syndrome + 0.000*using + 0.000*viruses + 0.000*acute_respiratory + 0.000*analysis + 0.000*Infection + 0.000*Patients + 0.000*protein + 0.000*associated + 0.000*cells'),\n",
       " (7,\n",
       "  '0.001*virus + 0.001*infection + 0.001*coronavirus + 0.001*based + 0.001*Virus + 0.001*using + 0.000*nucleocapsid_protein + 0.000*protein + 0.000*Role + 0.000*Detection + 0.000*human + 0.000*cells + 0.000*RNA + 0.000*detection + 0.000*viral + 0.000*tumor + 0.000*analysis + 0.000*viruses + 0.000*three + 0.000*vaccine'),\n",
       " (8,\n",
       "  '0.002*C + 0.001*virus + 0.001*R + 0.001*E + 0.001*infection + 0.000*China + 0.000*5 + 0.000*analysis + 0.000*RNA + 0.000*risk + 0.000*protein + 0.000*Virus + 0.000*viral + 0.000*interferon + 0.000*NE + 0.000*A + 0.000*Management + 0.000*approach + 0.000*N + 0.000*Abortion'),\n",
       " (9,\n",
       "  '0.001*virus + 0.001*infection + 0.001*viral + 0.001*analysis + 0.001*protein + 0.001*SARS + 0.001*Virus + 0.000*coronavirus + 0.000*human + 0.000*Enhanced + 0.000*expression + 0.000*respiratory_syndrome + 0.000*RNA + 0.000*cells + 0.000*Europe_PMC + 0.000*activity + 0.000*High + 0.000*PMC_Funders + 0.000*associated + 0.000*Human'),\n",
       " (10,\n",
       "  '0.001*Virus + 0.001*Viral + 0.001*virus + 0.001*Protein + 0.001*RNA + 0.000*infection + 0.000*using + 0.000*Proteins + 0.000*Human + 0.000*Viruses + 0.000*Injury + 0.000*Enzymes + 0.000*Lysosomal + 0.000*based + 0.000*Cells + 0.000*Inhibition + 0.000*automated + 0.000*viral + 0.000*potential + 0.000*respiratory_infections'),\n",
       " (11,\n",
       "  '0.001*virus + 0.001*human + 0.001*infection + 0.001*Respiratory_Syndrome + 0.001*cancer + 0.001*Middle_East + 0.001*Virus + 0.001*coronavirus + 0.001*study + 0.001*protein + 0.000*respiratory_syndrome + 0.000*analysis + 0.000*Respiratory + 0.000*expression + 0.000*SARS + 0.000*New + 0.000*novel + 0.000*characterisation + 0.000*using + 0.000*based'),\n",
       " (12,\n",
       "  '0.001*virus + 0.001*infection + 0.001*Virus + 0.001*human + 0.001*based + 0.000*Infection + 0.000*coronavirus + 0.000*disease + 0.000*induced + 0.000*1 + 0.000*viruses + 0.000*China + 0.000*protein + 0.000*SARS + 0.000*diarrhea_virus + 0.000*C + 0.000*analysis + 0.000*respiratory_syndrome + 0.000*emerging + 0.000*A'),\n",
       " (13,\n",
       "  '0.001*virus + 0.001*analysis + 0.001*infection + 0.001*Virus + 0.000*protein + 0.000*based + 0.000*cells + 0.000*Respiratory + 0.000*Impact + 0.000*pathogen + 0.000*viral + 0.000*SARS + 0.000*Pathogenesis + 0.000*human + 0.000*transmission + 0.000*coronavirus + 0.000*associated + 0.000*operties + 0.000*lung + 0.000*Infection'),\n",
       " (14,\n",
       "  '0.001*virus + 0.001*infection + 0.001*Virus + 0.001*1 + 0.000*Clinical + 0.000*based + 0.000*coronavirus + 0.000*Evaluation + 0.000*porcine + 0.000*respiratory + 0.000*Using + 0.000*SARS_CoV-2 + 0.000*protein + 0.000*formulations + 0.000*molecular + 0.000*children + 0.000*RNA + 0.000*Coronavirus + 0.000*detection + 0.000*viral'),\n",
       " (15,\n",
       "  '0.001*virus + 0.001*Virus + 0.001*RNA + 0.001*infection + 0.001*Infection + 0.000*Vaccine + 0.000*Sessions + 0.000*Educational + 0.000*based + 0.000*China + 0.000*Session + 0.000*human + 0.000*long_term + 0.000*disease + 0.000*associated + 0.000*Postgraduate + 0.000*Human + 0.000*Programme + 0.000*using + 0.000*Evaluation'),\n",
       " (16,\n",
       "  '0.002*virus + 0.001*infection + 0.001*Virus + 0.001*using + 0.001*coronavirus + 0.001*respiratory + 0.001*viral + 0.001*study + 0.001*avian_influenza + 0.001*human + 0.000*based + 0.000*infections + 0.000*associated + 0.000*Human + 0.000*virus_infection + 0.000*Middle_East + 0.000*host + 0.000*RNA + 0.000*protein + 0.000*analysis'),\n",
       " (17,\n",
       "  '0.001*virus + 0.001*infection + 0.001*viral + 0.001*RNA + 0.000*Coronavirus + 0.000*based + 0.000*gene + 0.000*Virus + 0.000*coronavirus + 0.000*2 + 0.000*detection + 0.000*1 + 0.000*data + 0.000*Infection + 0.000*receptor + 0.000*interferon + 0.000*system + 0.000*☆ + 0.000*barrier + 0.000*respiratory_syndrome'),\n",
       " (18,\n",
       "  '0.001*infection + 0.001*virus + 0.001*Virus + 0.000*Comparison + 0.000*Infection + 0.000*respiratory_viruses + 0.000*Respiratory_Syndrome + 0.000*detection + 0.000*Middle_East + 0.000*review + 0.000*Coronavirus + 0.000*based + 0.000*0 + 0.000*outcomes + 0.000*Copyright + 0.000*Human + 0.000*Host + 0.000*coronavirus + 0.000*Canine + 0.000*Analysis'),\n",
       " (19,\n",
       "  '0.001*virus + 0.001*infection + 0.001*dogs + 0.001*Virus + 0.000*pneumonia + 0.000*associated + 0.000*respiratory_syndrome + 0.000*disease + 0.000*ESCG + 0.000*protein + 0.000*analysis + 0.000*community_acquired + 0.000*bacterial + 0.000*Viral + 0.000*oral + 0.000*coronavirus + 0.000*Human + 0.000*infections + 0.000*using + 0.000*fecal')]"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "hdpmodel = HdpModel(corpus=corpus, id2word=dictionary)\n",
    "hdpmodel.show_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.014*\"novel\" + 0.014*\"vaccination\" + 0.013*\"Global\" + 0.012*\"Different\" + 0.010*\"Systematic_Review\" + 0.010*\"water\" + 0.009*\"Epidemics\" + 0.009*\"affects\" + 0.009*\"epidemiological\" + 0.009*\"air\"'),\n",
       " (1,\n",
       "  '0.016*\"MACROPHAGES\" + 0.012*\"virus\" + 0.011*\"respiratory_syncytial\" + 0.011*\"replication\" + 0.010*\"patients\" + 0.010*\"VIRUS\" + 0.010*\"outcomes\" + 0.009*\"Active\" + 0.009*\"associated\" + 0.009*\"Wuhan_China\"'),\n",
       " (2,\n",
       "  '0.012*\"virus\" + 0.009*\"virus_infection\" + 0.009*\"antiviral\" + 0.008*\"2009_H1N1\" + 0.007*\"Respiratory_Infections\" + 0.007*\"Human_Coronavirus\" + 0.007*\"Regions\" + 0.007*\"antigen\" + 0.007*\"experimental_infection\" + 0.007*\"PCR\"'),\n",
       " (3,\n",
       "  '0.017*\"respiratory_syndrome\" + 0.013*\"0_0\" + 0.012*\"virus\" + 0.011*\"human\" + 0.011*\"coronavirus\" + 0.009*\"disease\" + 0.009*\"evolution\" + 0.008*\"porcine_reproductive\" + 0.008*\"protein\" + 0.008*\"host\"'),\n",
       " (4,\n",
       "  '0.013*\"Singapore\" + 0.012*\"Pandemic\" + 0.012*\"Reporting\" + 0.010*\"infection\" + 0.010*\"response\" + 0.009*\"COVID-19\" + 0.008*\"following\" + 0.008*\"model\" + 0.008*\"analysis\" + 0.007*\"dynamics\"'),\n",
       " (5,\n",
       "  '0.016*\"Virus\" + 0.014*\"Vaccines\" + 0.014*\"Influenza\" + 0.013*\"Novel\" + 0.013*\"Outcomes\" + 0.012*\"A(H1N1\" + 0.009*\"Viral\" + 0.009*\"Vaccine\" + 0.009*\"Specific\" + 0.008*\"Associated\"'),\n",
       " (6,\n",
       "  '0.015*\"Critical\" + 0.010*\"adults\" + 0.009*\"Evidence\" + 0.009*\"management\" + 0.009*\"infection\" + 0.008*\"respiratory\" + 0.008*\"specific\" + 0.008*\"Translation\" + 0.008*\"Infants\" + 0.008*\"Rhinovirus\"'),\n",
       " (7,\n",
       "  '0.022*\"Diseases\" + 0.014*\"Clinical\" + 0.013*\"Bacterial\" + 0.010*\"respiratory_viruses\" + 0.009*\"Factor\" + 0.009*\"BMC_Infectious\" + 0.009*\"Vivo\" + 0.009*\"derivatives\" + 0.008*\"PCR_assay\" + 0.008*\"Translational\"'),\n",
       " (8,\n",
       "  '0.010*\"RNA\" + 0.009*\"health\" + 0.009*\"sequencing\" + 0.009*\"protein\" + 0.009*\"cross_-\" + 0.009*\"MERS\" + 0.008*\"sectional_study\" + 0.008*\"impact\" + 0.008*\"virus\" + 0.008*\"multiple\"'),\n",
       " (9,\n",
       "  '0.017*\"Disease_Outbreaks\" + 0.016*\"Respiratory_Syndrome\" + 0.014*\"Novel\" + 0.013*\"Analysis\" + 0.012*\"4\" + 0.012*\"molecules\" + 0.012*\"Anti_-\" + 0.012*\"Binding\" + 0.011*\"Severe_Acute\" + 0.011*\"Inhibitors\"')]"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "ldamodel = LdaModel(corpus=corpus, num_topics=10, id2word=dictionary)\n",
    "ldamodel.show_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "2      0.111834 -0.006462       1        1  11.958412\n",
       "5     -0.230039 -0.081522       2        1  11.682705\n",
       "3      0.105576 -0.161383       3        1  11.053086\n",
       "8      0.101494 -0.004144       4        1  10.493456\n",
       "4      0.114333  0.055882       5        1   9.924843\n",
       "7     -0.115166  0.231970       6        1   9.750125\n",
       "6      0.002873  0.100782       7        1   9.191604\n",
       "1      0.063678 -0.017112       8        1   9.053710\n",
       "9     -0.210648 -0.101872       9        1   8.565844\n",
       "0      0.056065 -0.016139      10        1   8.326215, topic_info=                    Term        Freq        Total Category  logprob  loglift\n",
       "563             Diseases  426.000000   426.000000  Default  30.0000  30.0000\n",
       "1060               Novel  614.000000   614.000000  Default  29.0000  29.0000\n",
       "14581  Disease_Outbreaks  281.000000   281.000000  Default  28.0000  28.0000\n",
       "3207            Vaccines  332.000000   332.000000  Default  27.0000  27.0000\n",
       "1438         MACROPHAGES  284.000000   284.000000  Default  26.0000  26.0000\n",
       "...                  ...         ...          ...      ...      ...      ...\n",
       "344             diseases   90.718301   218.205195  Topic10  -5.2044   1.6081\n",
       "379                 gene   64.272980   261.603980  Topic10  -5.5490   1.0821\n",
       "933                China   66.296238   347.333567  Topic10  -5.5180   0.8296\n",
       "717               Health   60.577452   296.701437  Topic10  -5.6083   0.8970\n",
       "51                 virus   66.232691  1168.552385  Topic10  -5.5190  -0.3846\n",
       "\n",
       "[543 rows x 6 columns], token_table=       Topic      Freq       Term\n",
       "term                             \n",
       "1708       9  0.970821          0\n",
       "9107       3  0.995026        0_0\n",
       "5509       4  0.996679         10\n",
       "10778      2  0.995393         13\n",
       "7625       1  0.992308  2009_H1N1\n",
       "...      ...       ...        ...\n",
       "700        9  0.029678    viruses\n",
       "700       10  0.133552    viruses\n",
       "2792      10  0.991982      water\n",
       "9440       4  0.978266    workers\n",
       "8910       3  0.961136   zoonotic\n",
       "\n",
       "[1347 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[3, 6, 4, 9, 5, 8, 7, 2, 10, 1])"
      ],
      "text/html": "\n<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n\n\n<div id=\"ldavis_el908361404542728321124526208110\"></div>\n<script type=\"text/javascript\">\n\nvar ldavis_el908361404542728321124526208110_data = {\"mdsDat\": {\"x\": [0.1118340795232429, -0.23003919756130137, 0.10557636011641855, 0.10149350276485881, 0.11433298336247047, -0.11516613780719956, 0.0028726762899759133, 0.06367793267135788, -0.21064753140990972, 0.056065332050085875], \"y\": [-0.006462012271572289, -0.08152236892793621, -0.16138326353482105, -0.004144009470654665, 0.05588242513389999, 0.23196966502365227, 0.10078214151538986, -0.017111883313644393, -0.10187161488638999, -0.01613907926792398], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [11.95841163844411, 11.682704860186096, 11.053086268048716, 10.493456428112683, 9.92484314179784, 9.750125023100122, 9.191604106531063, 9.0537098552202, 8.565843867455945, 8.326214811103226]}, \"tinfo\": {\"Term\": [\"Diseases\", \"Novel\", \"Disease_Outbreaks\", \"Vaccines\", \"MACROPHAGES\", \"Critical\", \"respiratory_syndrome\", \"Outcomes\", \"0_0\", \"Respiratory_Syndrome\", \"A(H1N1\", \"vaccination\", \"respiratory_syncytial\", \"Reporting\", \"Singapore\", \"Pandemic\", \"Bacterial\", \"Systematic_Review\", \"Clinical\", \"Viral\", \"Binding\", \"Virus\", \"Global\", \"Different\", \"Anti_-\", \"Evidence\", \"molecules\", \"Severe_Acute\", \"4\", \"Analysis\", \"2009_H1N1\", \"Regions\", \"experimental_infection\", \"decision_making\", \"Kaposi_sarcoma\", \"HEALTH\", \"2_0\", \"in\", \"cysteine\", \"associated_herpesvirus\", \"prognosis\", \"follow_up\", \"type\", \"dogs\", \"United_States\", \"evidence\", \"isolation\", \"canine\", \"Patient\", \"Prevention\", \"Asthma\", \"Europe\", \"Immune_Response\", \"Therapy\", \"Respiratory_Infections\", \"vaccine_development\", \"protease\", \"roles\", \"themed_issue\", \"review_comes\", \"Human_Coronavirus\", \"trial\", \"Delivery\", \"emergence\", \"multi_-\", \"antiviral\", \"antigen\", \"pregnant_women\", \"T_cells\", \"nuclear\", \"PCR\", \"virus_infection\", \"strains\", \"virus\", \"pigs\", \"cells\", \"human\", \"respiratory\", \"derived\", \"potential\", \"patients\", \"infection\", \"Outcomes\", \"hybridization\", \"Glycosylation\", \"Fatal\", \"Live_Attenuated\", \"Rational_Design\", \"13\", \"Issue\", \"probes_detection\", \"paper\", \"Mannose\", \"A(H1N1\", \"Modelling\", \"transcription\", \"Sudan\", \"Channel\", \"Mammalian\", \"Adaptation\", \"Vaccines\", \"Supplementary\", \"Potent\", \"CD8_T\", \"Mouse_Hepatitis\", \"Type\", \"Antibody\", \"Epithelial_Cells\", \"Patterns\", \"2017\", \"mammalian\", \"Reported\", \"Peptide\", \"Protection\", \"porcine_epidemic\", \"Types\", \"Vaccine\", \"Respiratory_Syncytial\", \"Gene\", \"Insights\", \"Specific\", \"Associated\", \"Virus\", \"Influenza\", \"Novel\", \"Influenza_Virus\", \"Non_-\", \"Viral\", \"Effect\", \"Activity\", \"Virus_Infection\", \"Characterization\", \"Data\", \"Infection\", \"High\", \"0_0\", \"Environment\", \"Coding\", \"kidney\", \"Catalysis\", \"Identifies\", \"marine\", \"Analogues\", \"Variability\", \"Necessary\", \"descriptive_study\", \"mesenchymal_stem\", \"including\", \"Green\", \"Important\", \"insights\", \"sub_-\", \"S2\", \"quarantine\", \"Involved\", \"efficiency\", \"Tissue\", \"zoonotic\", \"expressed\", \"mechanism\", \"Challenge\", \"S\", \"France\", \"chemical\", \"Clinical_Medicine\", \"practices\", \"HLA\", \"evolution\", \"bovine_respiratory\", \"H7N9\", \"Immunity\", \"Virology_Journal\", \"severe_acute\", \"respiratory_syndrome\", \"induce\", \"porcine_reproductive\", \"immune_responses\", \"immune_response\", \"development\", \"inhibitors\", \"coronavirus\", \"host\", \"human\", \"virus\", \"disease\", \"protein\", \"Effects\", \"respiratory_syncytial\", \"infection\", \"virus_infection\", \"HIV-1\", \"Localization\", \"10\", \"behaviors\", \"Simultaneous\", \"Sufficient\", \"ferrets\", \"elements\", \"invasive\", \"PUBLIC\", \"7b\", \"Residues\", \"neutropenic\", \"sectional_study\", \"Flavivirus\", \"seeking\", \"H\", \"Strategies\", \"MERS\", \"clusters\", \"containing\", \"sequencing\", \"Africa\", \"outcome\", \"plant\", \"framework\", \"Intestinal\", \"social\", \"One_Health\", \"workers\", \"ER\", \"male\", \"health_care\", \"monoclonal_antibodies\", \"community_based\", \"feline_coronavirus\", \"outbreak\", \"multiple\", \"information\", \"public_health\", \"cross_-\", \"serum\", \"pandemic\", \"health\", \"impact\", \"sequences\", \"diagnosis\", \"RNA\", \"protein\", \"Health\", \"Evidence\", \"virus\", \"analysis\", \"Reporting\", \"measures\", \"stimulated\", \"8\", \"Supplemental\", \"Cloning\", \"encephalitis_virus\", \"Know\", \"Clinical_characteristics\", \"Brief\", \"nonlinear\", \"Sense\", \"Clostridium_difficile\", \"material\", \"bias\", \"hierarchical\", \"genomes\", \"correlates\", \"Genetic_Diversity\", \"farms\", \"Virulence\", \"enhancer\", \"following\", \"OC43\", \"inhibitor\", \"cytokine\", \"Time\", \"Plant\", \"prevention\", \"silico\", \"long\", \"neonatal_calves\", \"Intracellular\", \"Singapore\", \"characterisation\", \"Pandemic\", \"findings\", \"response\", \"Required\", \"infectious\", \"dynamics\", \"COVID-19\", \"model\", \"infection\", \"Infections\", \"analysis\", \"PCR_assay\", \"Translational\", \"derivatives\", \"Adult_Patients\", \"salmon\", \"pseudorabies_virus\", \"Parasitic\", \"prophylactic\", \"Asymmetric\", \"Manifestations\", \"time_RT\", \"Research_Article\", \"multiplex_real\", \"BMC_Infectious\", \"Factor\", \"Articles\", \"Diseases\", \"comparison\", \"Testing\", \"Laboratory\", \"viral_infections\", \"prospective\", \"Zoonotic\", \"Survey\", \"Effective\", \"e\", \"Wild\", \"Multiple_Sclerosis\", \"Animals\", \"recognition\", \"Vivo\", \"Adenovirus\", \"Vietnam\", \"Diagnosis\", \"Illness\", \"Small\", \"Viruses\", \"Bacterial\", \"respiratory_viruses\", \"Acute_Respiratory\", \"Children\", \"Clinical\", \"Use\", \"Study\", \"Rapid\", \"Detection\", \"viruses\", \"Analysis\", \"novel\", \"Viral\", \"Critical\", \"Translation\", \"adults\", \"Ebolavirus\", \"Efficiency\", \"administered\", \"Informatics\", \"predictors\", \"Lactobacillus\", \"Activate\", \"Biologically\", \"exosomes\", \"Rhinovirus\", \"Infants\", \"Genomic\", \"future\", \"2010\", \"genomic\", \"care\", \"Spread\", \"inhibitory\", \"Phylogenetic\", \"asthma\", \"Equine\", \"algorithm\", \"Immediate\", \"Hajj\", \"Swine\", \"degradation\", \"Yeast\", \"adaptation\", \"Structure\", \"frame\", \"MERS_Coronavirus\", \"Beijing\", \"management\", \"open_reading\", \"Assessment\", \"three\", \"Genes\", \"cancer\", \"Taiwan\", \"infections\", \"surveillance\", \"specific\", \"respiratory\", \"Evidence\", \"infection\", \"Clinical\", \"gene\", \"MACROPHAGES\", \"Active\", \"accessory\", \"Human_Papillomavirus\", \"morpholino_oligomers\", \"cathepsin\", \"tick\", \"Lambda\", \"quantitation\", \"activator\", \"HEPATITIS\", \"outcomes\", \"VIRUS\", \"Chronic\", \"Wuhan_China\", \"Care\", \"epithelial_cells\", \"3C_like\", \"required\", \"NL63\", \"Cytokine\", \"Gene_Expression\", \"contact\", \"2018\", \"chain\", \"platform\", \"isothermal_amplification\", \"CORONAVIRUS\", \"equipment\", \"Systemic\", \"loop_mediated\", \"rate\", \"reverse_transcription\", \"L\", \"genus\", \"transmissible_gastroenteritis\", \"Murine\", \"Receptor\", \"Report\", \"inhibits\", \"Transplantation\", \"Lectin\", \"replication\", \"Inhibition\", \"respiratory_syncytial\", \"pneumonia\", \"associated\", \"patients\", \"expression\", \"virus\", \"infection\", \"human\", \"Disease_Outbreaks\", \"Nucleotide\", \"borne\", \"Immunomodulatory\", \"Meta\", \"aminopeptidase\", \"2_7\", \"Aspergillus\", \"Codon_Usage\", \"Kawasaki_Disease\", \"Definitive\", \"Biological\", \"Binding\", \"apoptosis\", \"Zika_Virus\", \"Bodies\", \"diarrheic\", \"Knowledge\", \"Comprehensive\", \"Inhibitors\", \"0\", \"Dengue_Virus\", \"Etiology\", \"Expressed\", \"Essential\", \"REVIEW\", \"Conserved\", \"Nuclear\", \"disaster\", \"Activities\", \"Species\", \"Anti_-\", \"Innate_Immune\", \"molecules\", \"Severe_Acute\", \"Plasma\", \"4\", \"Sequence\", \"Age\", \"Natural\", \"Respiratory_Syndrome\", \"Complete_Genome\", \"Response\", \"Multiple\", \"Bat\", \"Coronavirus\", \"Analysis\", \"Novel\", \"Protein\", \"Viral\", \"Middle_East\", \"water\", \"affects\", \"disinfection\", \"Affinity\", \"clustering\", \"modulate\", \"Biomedical\", \"Microbiological\", \"Different\", \"adipose\", \"Epidemics\", \"emerging\", \"experimental\", \"Design\", \"regulates\", \"Growth\", \"Effectiveness\", \"Regulates\", \"targeting\", \"Acid\", \"vaccination\", \"Original_Article\", \"determinants\", \"relationship\", \"trends\", \"nucleocapsid\", \"HHS_Public\", \"Department\", \"Pigs\", \"Cats\", \"Stability\", \"Hemagglutinin\", \"enhances\", \"field\", \"Diarrhea_Virus\", \"epidemiological\", \"air\", \"Porcine_Epidemic\", \"Global\", \"Systematic_Review\", \"novel\", \"role\", \"anti_-\", \"High\", \"SARS\", \"transmission\", \"diseases\", \"gene\", \"China\", \"Health\", \"virus\"], \"Freq\": [426.0, 614.0, 281.0, 332.0, 284.0, 274.0, 467.0, 293.0, 284.0, 341.0, 277.0, 233.0, 361.0, 237.0, 262.0, 264.0, 328.0, 286.0, 480.0, 576.0, 204.0, 676.0, 337.0, 193.0, 214.0, 485.0, 221.0, 218.0, 237.0, 492.0, 183.47304533350535, 171.65300925091773, 168.83217621632213, 161.79922690892423, 160.65930546773808, 164.5107395452024, 160.9893766259158, 163.1962545496294, 159.43852051445776, 158.50499987381355, 158.92978739164909, 157.21692808280574, 86.96744238401014, 107.60625756496167, 66.19017053245317, 63.41122744838501, 59.23732249912558, 55.58423601177742, 49.673974938070224, 42.81153772425856, 40.517209557701335, 37.06442044324047, 35.03770962808171, 34.82047731475175, 175.45810146591828, 32.917696460033994, 60.30355540787677, 32.155082840647765, 32.253182321969355, 32.253182321969355, 174.26618213519288, 51.516906299256824, 44.34097108082197, 38.726637872942085, 52.535604277333, 212.539401614962, 169.5601466698713, 129.13539437507598, 142.5604054688141, 119.9465376692107, 168.70651741166765, 219.20757031677783, 119.04211463660691, 289.0896849054031, 71.39732778727293, 122.56600730237844, 148.65714973209253, 103.65588720138871, 88.65630776549443, 94.29727235476516, 103.68796747995721, 93.83257228649298, 292.1255981353005, 145.15082276684436, 144.3774187735583, 146.7366878547338, 139.75740293171737, 139.69906010504664, 140.67358820448064, 137.1502577789597, 137.385852286215, 137.89309816049374, 147.7851319770808, 275.4639401603534, 155.40027384826328, 156.6660735993836, 100.63656266984482, 132.8821767461703, 151.00679782785136, 139.8805099653528, 327.39965387284553, 161.19909322853627, 170.13013240936766, 42.71790324229809, 33.02354811058362, 39.548220893260066, 67.6788984606889, 32.5704576754898, 155.35094253847566, 28.7743561884824, 28.767149673034865, 134.49858386134017, 172.88380374742098, 165.8065469079198, 112.74394031113658, 126.32803036755642, 207.98423399560633, 158.21291396266307, 181.51929755119394, 152.998051668651, 197.46271064996822, 193.70338640446786, 380.79372240739013, 327.2069261261408, 295.59625165793943, 149.2437134360848, 161.78557569797158, 215.13661558842884, 160.7161838459598, 160.70740975826402, 154.80359271600815, 170.07761739002254, 150.17389623866526, 158.26155456936024, 148.6509822621281, 283.396573570077, 162.91320281793526, 157.62408774161287, 149.74900359263313, 145.91621724519746, 142.5952886488439, 144.81805620417146, 141.06353416278097, 142.4913242438665, 139.2382553445744, 141.67322995437513, 138.34105947922762, 155.12728790374908, 141.23982030027213, 152.41669836725856, 161.27611570549763, 140.46094495396662, 38.36029236359339, 33.87667679248227, 153.34856945547938, 28.930507940976007, 28.256490427799115, 27.13347166295485, 25.64941858221043, 37.51961986839525, 24.762448640217084, 54.715980314703124, 24.091979313126387, 24.029372973593972, 23.847675214158375, 28.221261279471346, 160.4282166528822, 196.56074847410878, 27.13779074990294, 152.459348074545, 40.885224977612545, 151.88919796557485, 113.82793345153644, 372.78846086950415, 130.17770946093708, 176.84181916940463, 143.31030685620533, 134.85043058718318, 167.38196706052156, 148.86204896655093, 231.58844151704207, 170.8878301788699, 240.1038226481205, 255.58404290652066, 201.2052066389546, 174.28155019567956, 148.6817602709424, 153.6760951104058, 160.08127042260054, 152.9817227512927, 144.0213020842345, 158.89530223966773, 157.56287357819056, 146.61169500716355, 141.2667613570897, 139.45252034973637, 143.58507129220658, 145.72000676758304, 145.31922350184487, 137.8512387707299, 137.06453257898937, 141.25524532081695, 117.7990375471324, 167.63508195642984, 145.39126658398217, 94.54229566434383, 145.41932965217208, 40.314010301853955, 178.57828266731167, 139.17586405746, 34.806771303856834, 196.78152998297276, 31.646019209356936, 31.43843974745061, 28.758860219033938, 28.390965027424038, 30.646838270930836, 28.198102924163422, 27.834892180810098, 27.630089944823084, 24.996538210372822, 58.43412259555865, 40.259522389489355, 149.28891171248134, 135.09693829399723, 144.20936065924175, 118.53494910076752, 164.01361813217898, 153.42723036672118, 161.1034731140683, 180.81910169841814, 160.29058441898374, 132.34154243670994, 196.82825329512315, 166.51637852032442, 138.68208976818266, 149.71186431740787, 201.95081499898745, 186.37304946811582, 156.00092876281613, 156.0807655260673, 164.12903052937037, 142.36016441047468, 236.58561328432222, 137.03096388213774, 129.9235776366727, 133.29807515749243, 125.17480708656082, 124.25487215155309, 123.1615176816167, 122.71555946863946, 126.23862109963545, 123.04217313569198, 118.39089892892831, 118.41187613727708, 129.46611313152863, 123.83321009335042, 118.9680838827691, 118.03625758446226, 139.8122226502449, 124.55071680146469, 123.00596728089772, 129.66882557759115, 133.68106507198223, 97.56240138411115, 162.4908494602446, 125.55258020404432, 38.90079345020599, 30.34797853122034, 29.862820993813408, 30.946571606343618, 26.659393615597427, 26.29159796620202, 125.7743628395148, 125.57999360131826, 130.18345480915764, 246.33508258865749, 137.706626193878, 239.15658654076543, 128.56802639909742, 190.55173467475188, 127.58214344850724, 143.13368120677373, 145.38775008971396, 181.9137090152097, 155.54667836470287, 194.2798833571261, 141.35390993847423, 148.995912440943, 158.33555478693907, 157.1388901627002, 171.51252020399832, 152.13676718327412, 151.5282389931279, 150.6524472970729, 149.26648340738447, 145.26457426099392, 142.6626168122208, 151.42867606405716, 153.23505779708142, 145.2328067152338, 154.9334821148857, 179.35790608115119, 183.54140453575548, 155.16760901720744, 420.7870205872062, 57.2589566528484, 39.414296658425314, 41.91175536438331, 69.12530589897632, 32.71351743199073, 32.6618174851058, 32.01671203288647, 31.442797587666863, 29.949027149328188, 30.90611791255567, 29.58141035366242, 30.441829421782604, 27.830031869878496, 174.60655486499516, 45.58218588791105, 139.57322974226796, 58.13978323920107, 144.70184645933375, 41.133585597651006, 135.09579817500816, 246.95266837834905, 185.76981635784597, 144.10881924629362, 73.80056632222576, 270.0852782014049, 76.59554291067555, 94.21353376514983, 69.00999880489604, 102.81744973409032, 107.07693990504795, 132.10662253520195, 143.9568808483839, 124.0287400153765, 273.9650220737722, 144.3893290692407, 180.4514788383913, 139.56201181663496, 137.41941593438827, 135.8605260141256, 135.73032487335604, 132.61412436061104, 132.9084142613501, 131.1632740461363, 117.78837162524333, 115.39602967846974, 139.93975135944282, 142.391589810284, 55.23990577782514, 42.933193588552335, 37.69552282537457, 31.958141289646548, 43.09956218459176, 33.72726648414745, 23.94994113613243, 23.39859653437782, 27.917015104276715, 22.645968305225722, 137.51356424851718, 82.60556778933767, 21.32957312731589, 32.97013377441767, 23.312805496419653, 20.45036305022408, 136.4354183081744, 74.78183269601227, 122.87684685924006, 127.52800113626562, 134.79248437517293, 169.42814453630947, 111.24616762452713, 133.13160893894684, 135.6786231160751, 133.15343083354563, 137.5402090365526, 133.04538273674814, 135.55504496277132, 113.46099652519867, 145.82062458867222, 152.17510934978142, 171.35996808796253, 167.204892554143, 121.94838892663347, 118.45461020026247, 283.5798723224362, 170.34020984097808, 151.29891030674818, 149.36990693026408, 148.23108218994972, 145.23009617036192, 144.47558003450575, 146.04275623262055, 144.51746507239753, 142.15106079502212, 138.28214787404056, 171.51866144022438, 172.6994046729336, 53.87108799276598, 156.77690119098787, 32.72650593901501, 33.45284920144098, 30.466775526613766, 29.87909072504005, 29.206454485541787, 29.084307912879993, 30.393602164689835, 27.307250707701517, 25.55676474161692, 25.66346578837484, 28.817621159538184, 44.32346135070697, 23.269336091548332, 21.783006882007964, 30.88695781293642, 37.07818197619647, 147.18347115236054, 37.563761543376785, 151.85887177977722, 130.43443235299196, 51.1853395741561, 40.26101831105596, 79.09537770067836, 137.95467010673784, 68.94205309802817, 34.65549467915402, 112.5162172153639, 190.06898268683483, 143.88592768996799, 195.2829506325427, 154.35121553410434, 167.3070573166515, 188.43815929655622, 133.57609455815074, 224.11009378002814, 132.3102597967852, 112.99899078609599, 280.9876815160865, 158.14599208092037, 151.45102298607915, 145.6615322437334, 144.20814571957408, 140.96510413968463, 142.1981309578202, 142.47953112535885, 144.38631360024164, 142.2648687400884, 134.68026300962399, 52.40177231562748, 200.6370533837599, 48.4562036091868, 37.167995175967654, 38.18000947007656, 32.311314099842136, 29.8857727662467, 28.171439060405714, 178.7327700202432, 31.84312291928752, 155.6494894304011, 25.964645170190277, 25.144082613991362, 23.770951161234613, 23.286091361003933, 30.321081701814908, 22.21556281322802, 22.54728337937808, 21.859225073656532, 172.43112179396044, 200.68582877976428, 27.895120982108462, 202.37767982922995, 194.10341342158213, 144.72368994418116, 205.2929197292236, 91.12168205653133, 140.83110632074587, 154.3774359651056, 276.1371725374437, 39.13691801178855, 96.5924710062955, 143.21790389498273, 55.905486102342806, 159.02969400742558, 223.90525950971858, 242.65526871585996, 132.98450296508594, 174.22114084518313, 96.92411046475428, 162.35100465133422, 152.92875610711522, 146.2158670525495, 146.29265979958907, 144.2130254487205, 143.55750274647724, 142.26685541542312, 141.1137027905068, 192.47882066251628, 140.63991676965892, 154.08756973581973, 76.93097244595175, 40.71038957981801, 40.00774440514506, 38.97669178652683, 32.538226813016, 36.018128910684744, 30.82606929297045, 63.661987220274014, 34.1769566528118, 225.86096007693828, 30.744059730958938, 27.908802516773356, 27.374672195354663, 27.335879705401318, 27.558870182744915, 30.915630064858377, 25.523985451495342, 25.224111851032653, 28.149995121107057, 145.4518315222579, 148.12936324876512, 38.52153862121817, 46.48982064425229, 39.273656046387785, 150.38696540622448, 148.91010489349682, 46.8712961529211, 208.16512372997602, 173.11088070478417, 231.67785683356306, 80.04198630764103, 66.00981162560743, 129.1713384213143, 137.69625633245758, 140.07460093212725, 90.71830102256553, 64.2729795129634, 66.29623841917788, 60.57745162698256, 66.23269057502313], \"Total\": [426.0, 614.0, 281.0, 332.0, 284.0, 274.0, 467.0, 293.0, 284.0, 341.0, 277.0, 233.0, 361.0, 237.0, 262.0, 264.0, 328.0, 286.0, 480.0, 576.0, 204.0, 676.0, 337.0, 193.0, 214.0, 485.0, 221.0, 218.0, 237.0, 492.0, 184.4184942198261, 172.60010351813872, 169.7776409832325, 162.74668751435897, 161.60528715145037, 165.47995916951834, 161.94107081419173, 164.16541115800308, 160.39142751030857, 159.45239660344498, 159.89096516955723, 158.19848103709379, 88.00186468491333, 108.95140549733046, 67.13449717706037, 64.35547052509631, 60.18157725353165, 56.52844742892082, 50.74864307366862, 43.799756645806916, 41.46151694817061, 38.00867694097582, 35.982035154975215, 35.764707261687406, 180.31616054600488, 33.86196317199078, 62.073400507613975, 33.09935328346408, 33.221606740318634, 33.221606740318634, 181.48784033477412, 53.43309759809915, 46.187910129472876, 40.27837280017427, 56.217949349928574, 262.103771907529, 205.35208363234, 154.0520156247509, 172.38237437726488, 170.95565374525663, 271.4584533200707, 512.6642305247474, 231.20003634062388, 1168.5523845942403, 99.14867450392872, 399.68321774477056, 689.2275151606264, 323.6672222122548, 202.96418864245123, 257.06935054138785, 500.29595965054733, 895.2177007768491, 293.7102372225981, 146.11319357587277, 145.33940249431555, 147.72985494633159, 140.71936114942227, 140.66077590754844, 141.6525236195424, 138.11254857122523, 138.35000941573168, 138.86339935761026, 148.85595135842726, 277.5174098912422, 156.6173107985918, 158.41541970432945, 102.05276067668702, 134.75716933107364, 153.14973257596648, 141.9922271496211, 332.475260178098, 163.7354979678557, 173.61930580367468, 43.67922537985818, 33.984815663454995, 40.73011960862329, 69.7552878515499, 33.578380986343, 160.2739614572914, 29.73570247377281, 29.728501988048535, 139.2529466792588, 187.3425227614324, 182.48472776193557, 122.10675866196611, 140.57635556476984, 239.16822573811947, 180.75609397864045, 210.54841032613353, 175.21656330135042, 241.00432435848774, 249.5586236301774, 676.6171130603675, 560.68197326409, 614.4522639610934, 176.08389804694946, 226.01065346048037, 576.8728947961669, 226.1191872912603, 229.0237564088645, 207.95576893215977, 426.4667447026062, 194.8448120788926, 344.567026758258, 344.3950246344794, 284.4147003065033, 163.88155571578332, 158.5842155566765, 150.70759623541937, 146.87921237955786, 143.56407593279724, 145.80626313983356, 142.02732871569256, 143.46524401441906, 140.20153469760626, 142.66518574446283, 139.31122219429741, 156.26248463042018, 142.295602101539, 155.08453913452968, 164.41264496495714, 143.90982175503763, 39.318783444905975, 34.83500153682763, 157.98762267654962, 29.90178852571712, 29.214742208133043, 28.091758167273312, 26.607590480887644, 38.956864113139225, 25.724618213089627, 56.86782148545952, 25.050230020954977, 24.98759256072647, 24.80603528968875, 29.357418898868975, 170.58026574014156, 211.39461785182198, 28.26548428678006, 171.10184058499814, 43.44383656841886, 176.1397356685458, 130.31676339232786, 467.84113107088695, 158.16596550129037, 224.71377215561893, 186.73208848085326, 183.81419523390954, 243.267093008312, 213.46104178136514, 410.76184865170734, 274.4018298613237, 689.2275151606264, 1168.5523845942403, 569.8948726400635, 500.66628688446934, 233.33940166849771, 361.36877937572086, 895.2177007768491, 512.6642305247474, 255.31728277420237, 159.85626489415066, 158.52640006293726, 147.57265303255318, 142.2359982282369, 140.41395407722376, 144.57753486607209, 146.7287566839996, 146.32917654141144, 138.81380513044357, 138.02727151339818, 142.4006848124628, 118.92327934064639, 169.3672253241321, 147.08664740506464, 95.87194681562693, 148.05366818154258, 41.274841958446906, 183.01898999708695, 142.91256884725993, 35.767634993784704, 202.67004429896284, 32.60683784085901, 32.399318856397294, 29.719714248421315, 29.35174636852013, 31.69100708321407, 29.158916891877304, 28.795700018485842, 28.622072723903376, 25.95743511376108, 60.72570255496142, 41.92486110395929, 162.80867106641523, 150.17947022357862, 166.879964089027, 136.1184896538343, 194.12352276550175, 182.53716000165974, 195.669904525941, 224.9616268454839, 201.98146455316802, 168.64753558858862, 273.34718835718104, 230.36402085282893, 185.08935261900058, 220.40704232664126, 492.4846889183274, 500.66628688446934, 296.70143654028595, 485.0518299368442, 1168.5523845942403, 474.9371042713307, 237.56760204100235, 138.01271514670532, 130.89974402668756, 134.32220184569644, 126.15119123619067, 125.23083970087919, 124.13791137772643, 123.69188117081023, 127.2476214915473, 124.05579258351383, 119.36752451152041, 119.39041594535462, 130.54025978992303, 124.86642981506301, 119.96725058309096, 119.03113404567047, 141.05240141398178, 125.72741960707063, 124.17424667004329, 131.0730563798888, 135.3928345037428, 98.90781522489324, 165.00191738666012, 128.18898236064143, 39.87590423583339, 31.323036080108764, 30.837895273659296, 31.981332226308147, 27.634422949318868, 27.280616079741623, 131.05538173827395, 130.9013206700888, 136.4334153792729, 262.4204281231297, 146.89066324958273, 264.94038930124384, 141.7269539269087, 283.72311294732464, 141.42473526700653, 183.73649301845376, 194.68642799037366, 343.4508369268549, 283.97530353257474, 895.2177007768491, 238.08870729861073, 474.9371042713307, 159.29422621814766, 158.110576982398, 172.57794818474173, 153.0956120493513, 152.4853352529553, 151.61272674288492, 150.2511072581943, 146.22444525804855, 143.63413999385, 152.60897683805388, 154.4676696355396, 146.45022329537528, 156.3878430827626, 181.21229275081737, 185.69133469282238, 157.10535553064008, 426.58404285417225, 58.569573261135055, 40.368243520456815, 43.02815806384249, 71.05323032609721, 33.66757077825104, 33.615788103539586, 32.970706609430486, 32.396764272780885, 30.90297220784152, 31.89463448721548, 30.56717644203707, 31.459376905178992, 28.78402017359775, 183.94046148622053, 48.113912853697535, 160.183457927398, 62.635217448540224, 168.22685080155918, 43.374492291905156, 165.0470604847499, 328.18623423998804, 238.35702606060082, 188.85837421251622, 86.98427280951341, 480.8476856144893, 99.57542381773091, 151.30817215262596, 89.04627940880368, 241.31687728964715, 269.5581522940384, 492.3788219786346, 660.9358962990129, 576.8728947961669, 274.93347027668653, 145.3566215217527, 181.69931277840993, 140.52936363765792, 138.3872992447676, 136.82843677045292, 136.69987940255834, 133.58709711336044, 133.91310015794556, 132.27277507237457, 118.87547205099361, 116.4867334288938, 141.29536383073727, 143.96444627253453, 56.33292010155806, 43.90004283677599, 38.66239236725444, 32.97476546879558, 44.60699331406123, 35.051435619188766, 24.925409008679914, 24.371683940603837, 29.17178533399005, 23.674453500990914, 143.77635734494928, 86.47743234594721, 22.342682846359804, 34.604849234375095, 24.48046804467549, 21.476607298213057, 144.15303178836234, 78.65065496874041, 131.3660117716522, 137.84323364365483, 147.41149897362476, 207.11904059379629, 131.20891325303853, 164.59511136535528, 168.70441234789732, 169.70580660921817, 179.40324173171754, 204.38979426033706, 227.31800364202323, 180.52037271971682, 283.20768152508106, 323.6672222122548, 485.0518299368442, 895.2177007768491, 480.8476856144893, 261.6039801895201, 284.56691934185676, 171.29624405401938, 152.25560627835594, 150.33135674318655, 149.18695808294882, 146.18685999851886, 145.43554100469458, 147.02472138072125, 145.50076843527563, 143.1954932232605, 139.35471367061945, 173.0209545426268, 174.67062761450086, 55.075229366494575, 161.15391794377163, 33.68192304799859, 34.45561568774708, 31.42222178036478, 30.834495386448637, 30.16183894122862, 30.039749908837205, 31.410708798915625, 28.26269996014593, 26.512208808445827, 26.630336310142617, 29.904073617988043, 46.06864114022148, 24.22894428549287, 22.738418307608367, 32.26323008376378, 38.792617543045466, 158.23462983900237, 39.44819206339767, 169.17559633143924, 146.81668491757128, 55.06733256273984, 42.907825623768325, 89.9685650124245, 168.04691093620573, 78.5426481913352, 36.568384345580355, 155.8507538409327, 303.73771754059317, 231.0069497047682, 361.36877937572086, 268.12887664446555, 387.5454724389699, 500.29595965054733, 308.13964299818434, 1168.5523845942403, 895.2177007768491, 689.2275151606264, 281.9538721641197, 159.14488664951037, 152.41124067156562, 146.6235797610277, 145.16798843238803, 141.92373379314589, 143.1657600252549, 143.45217436784338, 145.37382846935716, 143.25595753899344, 135.68699788199729, 53.35966487330903, 204.54440724407024, 49.45793519614273, 38.12584476007207, 39.18065825922321, 33.329209107606054, 30.843677904510255, 29.12934589937228, 184.95009296796533, 32.96178012958272, 161.3764593622265, 26.94793002346981, 26.101992857515704, 24.72880269062138, 24.243938107323412, 31.61573872332224, 23.175427585860803, 23.532720875628414, 22.817041698067687, 183.11021377383895, 214.93541373614968, 29.209917028033768, 221.26039801944887, 218.22481441724392, 162.55914530312657, 237.90331946181865, 101.49293566797145, 164.2073717978537, 181.3061944316453, 341.201036838654, 41.59317651856442, 118.9950402491809, 188.93091356293758, 63.795641523762086, 258.3945916517491, 492.3788219786346, 614.4522639610934, 245.69658275762197, 576.8728947961669, 236.38812195804343, 163.3094837965122, 153.88777991550813, 147.17435149761914, 147.2523197025951, 145.1735029895092, 144.5188537089181, 143.23066523548437, 142.07222785452635, 193.8498241309422, 141.64180752340278, 155.19966261119458, 78.10750792491045, 41.66890547894316, 40.966280357004344, 39.93520489341109, 33.49679625954382, 37.12541599014735, 31.784741121567823, 65.73479428181952, 35.29559293882151, 233.42086692426918, 31.77405447248632, 28.876020242937734, 28.333260258935606, 28.294368665978276, 28.54529674265783, 32.0380722574314, 26.482503045807743, 26.18262424949925, 29.250002786738577, 154.24110842586182, 158.3034753259099, 40.6517084624791, 49.86453588798088, 42.01153385407915, 185.65788758275704, 184.33067738977326, 51.78652878578849, 337.5293872690013, 286.5462343971656, 660.9358962990129, 135.36295723882233, 102.8251323540693, 344.3950246344794, 396.3301094841471, 434.982807397907, 218.20519451149246, 261.6039801895201, 347.3335667152924, 296.70143654028595, 1168.5523845942403], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -4.862100124359131, -4.928699970245361, -4.945300102233887, -4.987800121307373, -4.994900226593018, -4.971199989318848, -4.9928998947143555, -4.979300022125244, -5.002500057220459, -5.008399963378906, -5.00570011138916, -5.016600131988525, -5.608699798583984, -5.395699977874756, -5.881700038909912, -5.924600124359131, -5.992700099945068, -6.056300163269043, -6.168700218200684, -6.317399978637695, -6.372499942779541, -6.46150016784668, -6.5177998542785645, -6.52400016784668, -4.906799793243408, -6.5802001953125, -5.974800109863281, -6.603600025177002, -6.600599765777588, -6.600599765777588, -4.913599967956543, -6.132299900054932, -6.282299995422363, -6.417699813842773, -6.11269998550415, -4.715099811553955, -4.940999984741211, -5.2133002281188965, -5.1143999099731445, -5.287199974060059, -4.946000099182129, -4.684199810028076, -5.2947001457214355, -4.40749979019165, -5.8059000968933105, -5.265600204467773, -5.0725998878479, -5.43310022354126, -5.589399814605713, -5.5278000831604, -5.432799816131592, -5.532700061798096, -4.373700141906738, -5.0731000900268555, -5.078400135040283, -5.06220006942749, -5.111000061035156, -5.111400127410889, -5.104400157928467, -5.129799842834473, -5.1280999183654785, -5.1244001388549805, -5.055099964141846, -4.432400226593018, -5.004899978637695, -4.996799945831299, -5.4394001960754395, -5.161399841308594, -5.033599853515625, -5.110099792480469, -4.259699821472168, -4.968200206756592, -4.914299964904785, -6.296299934387207, -6.553699970245361, -6.3734002113342285, -5.836100101470947, -6.567500114440918, -5.005199909210205, -6.691400051116943, -6.6915998458862305, -5.1493000984191895, -4.8983001708984375, -4.940100193023682, -5.325799942016602, -5.211999893188477, -4.713399887084961, -4.9868998527526855, -4.8495001792907715, -5.020500183105469, -4.7652997970581055, -4.784599781036377, -4.10860013961792, -4.260300159454346, -4.3618998527526855, -5.045300006866455, -4.964600086212158, -4.679599761962891, -4.971199989318848, -4.97130012512207, -5.008699893951416, -4.914599895477295, -5.039100170135498, -4.986599922180176, -5.049300193786621, -4.348599910736084, -4.902299880981445, -4.935299873352051, -4.986499786376953, -5.012400150299072, -5.0355000495910645, -5.019999980926514, -5.046299934387207, -5.036200046539307, -5.059299945831299, -5.041999816894531, -5.065800189971924, -4.951200008392334, -5.045000076293945, -4.968900203704834, -4.912399768829346, -5.05049991607666, -6.348499774932861, -6.472799777984619, -4.962800025939941, -6.6305999755859375, -6.654200077056885, -6.694699764251709, -6.750999927520752, -6.37060022354126, -6.786200046539307, -5.993299961090088, -6.813600063323975, -6.816199779510498, -6.823800086975098, -6.655399799346924, -4.917600154876709, -4.7144999504089355, -6.6946001052856445, -4.968599796295166, -6.2846999168396, -4.972300052642822, -5.260799884796143, -4.07450008392334, -5.1265997886657715, -4.820199966430664, -5.0304999351501465, -5.091300010681152, -4.875199794769287, -4.992499828338623, -4.55049991607666, -4.854499816894531, -4.514400005340576, -4.451900005340576, -4.691199779510498, -4.834799766540527, -4.99370002746582, -4.960599899291992, -4.9197998046875, -4.965199947357178, -5.0254998207092285, -4.87529993057251, -4.883699893951416, -4.955699920654297, -4.9928998947143555, -5.005799770355225, -4.976600170135498, -4.9618000984191895, -4.964600086212158, -5.017300128936768, -5.023099899291992, -4.993000030517578, -5.174499988555908, -4.821700096130371, -4.964099884033203, -5.394499778747559, -4.963900089263916, -6.246799945831299, -4.758500099182129, -5.007800102233887, -6.393700122833252, -4.661399841308594, -6.488900184631348, -6.495500087738037, -6.58459997177124, -6.597400188446045, -6.520999908447266, -6.604300022125244, -6.617199897766113, -6.624599933624268, -6.724800109863281, -5.8755998611450195, -6.248199939727783, -4.937600135803223, -5.037499904632568, -4.972300052642822, -5.168300151824951, -4.843599796295166, -4.910299777984619, -4.861499786376953, -4.745999813079834, -4.866499900817871, -5.05810022354126, -4.661200046539307, -4.828400135040283, -5.011300086975098, -4.934800148010254, -4.635499954223633, -4.715799808502197, -4.893700122833252, -4.893099784851074, -4.842899799346924, -4.985199928283691, -4.421500205993652, -4.967599868774414, -5.020899772644043, -4.995200157165527, -5.05810022354126, -5.065499782562256, -5.0742998123168945, -5.077899932861328, -5.049600124359131, -5.075300216674805, -5.113800048828125, -5.11359977722168, -5.024400234222412, -5.068900108337402, -5.109000205993652, -5.116799831390381, -4.947500228881836, -5.0630998611450195, -5.0756001472473145, -5.022799968719482, -4.992400169372559, -5.307300090789795, -4.7972002029418945, -5.055099964141846, -6.226799964904785, -6.475100040435791, -6.491199970245361, -6.45550012588501, -6.604700088500977, -6.618599891662598, -5.053299903869629, -5.054900169372559, -5.018899917602539, -4.381100177764893, -4.962699890136719, -4.410699844360352, -5.031400203704834, -4.637899875640869, -5.039000034332275, -4.923999786376953, -4.908400058746338, -4.684299945831299, -4.84089994430542, -4.618500232696533, -4.936500072479248, -4.883900165557861, -4.805300235748291, -4.812900066375732, -4.725399971008301, -4.845300197601318, -4.849299907684326, -4.855100154876709, -4.864299774169922, -4.891499996185303, -4.909599781036377, -4.849899768829346, -4.838099956512451, -4.89169979095459, -4.827099800109863, -4.680699825286865, -4.657599925994873, -4.825500011444092, -3.827899932861328, -5.822500228881836, -6.195899963378906, -6.134500026702881, -5.634099960327148, -6.382299900054932, -6.383800029754639, -6.403800010681152, -6.421899795532227, -6.470600128173828, -6.4390997886657715, -6.482900142669678, -6.45419979095459, -6.543900012969971, -4.707499980926514, -6.05049991607666, -4.93149995803833, -5.807199954986572, -4.895400047302246, -6.153200149536133, -4.964099884033203, -4.360899925231934, -4.645500183105469, -4.899499893188477, -5.568699836730957, -4.271299839019775, -5.531499862670898, -5.32450008392334, -5.635799884796143, -5.237100124359131, -5.196499824523926, -4.986400127410889, -4.9004998207092285, -5.049499988555908, -4.1981000900268555, -4.838500022888184, -4.615600109100342, -4.872600078582764, -4.888000011444092, -4.899400234222412, -4.900400161743164, -4.923600196838379, -4.92140007019043, -4.934599876403809, -5.042200088500977, -5.062699794769287, -4.869800090789795, -4.852499961853027, -5.7993998527526855, -6.051400184631348, -6.18149995803833, -6.34660005569458, -6.047500133514404, -6.292799949645996, -6.6350998878479, -6.658400058746338, -6.481800079345703, -6.691100120544434, -4.88730001449585, -5.396999835968018, -6.750999927520752, -6.315499782562256, -6.662099838256836, -6.793099880218506, -4.895199775695801, -5.496500015258789, -4.999899864196777, -4.962699890136719, -4.907299995422363, -4.678599834442139, -5.099299907684326, -4.9197001457214355, -4.9008002281188965, -4.919600009918213, -4.8871002197265625, -4.920400142669678, -4.901700019836426, -5.079599857330322, -4.828700065612793, -4.785999774932861, -4.667300224304199, -4.691800117492676, -5.007500171661377, -5.036499977111816, -4.148499965667725, -4.658100128173828, -4.776700019836426, -4.7895002365112305, -4.7972002029418945, -4.817599773406982, -4.822800159454346, -4.811999797821045, -4.822500228881836, -4.839099884033203, -4.866600036621094, -4.651299953460693, -4.644400119781494, -5.8094000816345215, -4.741099834442139, -6.307799816131592, -6.285799980163574, -6.379300117492676, -6.398799896240234, -6.421599864959717, -6.4257001876831055, -6.381700038909912, -6.488800048828125, -6.554999828338623, -6.550899982452393, -6.434999942779541, -6.00439977645874, -6.648799896240234, -6.714799880981445, -6.365600109100342, -6.1828999519348145, -4.804299831390381, -6.169899940490723, -4.7729997634887695, -4.925099849700928, -5.860499858856201, -6.100599765777588, -5.425300121307373, -4.86899995803833, -5.562699794769287, -6.250500202178955, -5.072800159454346, -4.548600196838379, -4.826900005340576, -4.521500110626221, -4.756700038909912, -4.67609977722168, -4.557199954986572, -4.901299953460693, -4.383800029754639, -4.910799980163574, -5.068600177764893, -4.102200031280518, -4.677000045776367, -4.720300197601318, -4.759300231933594, -4.7692999839782715, -4.791999816894531, -4.783299922943115, -4.781400203704834, -4.768099784851074, -4.782899856567383, -4.837600231170654, -5.781599998474121, -4.4390997886657715, -5.859899997711182, -6.125100135803223, -6.098199844360352, -6.265100002288818, -6.343200206756592, -6.402200222015381, -4.554699897766113, -6.279699802398682, -4.69290018081665, -6.483799934387207, -6.515900135040283, -6.5721001625061035, -6.592700004577637, -6.328700065612793, -6.639800071716309, -6.624899864196777, -6.655900001525879, -4.59060001373291, -4.438799858093262, -6.412099838256836, -4.4303998947143555, -4.4721999168396, -4.765699863433838, -4.416100025177002, -5.228400230407715, -4.793000221252441, -4.701200008392334, -4.119699954986572, -6.073500156402588, -5.170100212097168, -4.776199817657471, -5.716899871826172, -4.671500205993652, -4.3292999267578125, -4.248899936676025, -4.850299835205078, -4.5802001953125, -5.166600227355957, -4.622399806976318, -4.682199954986572, -4.727099895477295, -4.726600170135498, -4.740900039672852, -4.7453999519348145, -4.754499912261963, -4.762599945068359, -4.452199935913086, -4.765999794006348, -4.674699783325195, -5.36929988861084, -6.00570011138916, -6.023099899291992, -6.049200057983398, -6.229800224304199, -6.128200054168701, -6.28380012512207, -5.558599948883057, -6.180600166320801, -4.292300224304199, -6.286499977111816, -6.383200168609619, -6.402599811553955, -6.4039998054504395, -6.395899772644043, -6.280900001525879, -6.472599983215332, -6.484399795532227, -6.374599933624268, -4.7322998046875, -4.714099884033203, -6.060999870300293, -5.872900009155273, -6.041600227355957, -4.698999881744385, -4.708799839019775, -5.864799976348877, -4.373799800872803, -4.558199882507324, -4.2667999267578125, -5.329599857330322, -5.52239990234375, -4.85099983215332, -4.787099838256836, -4.769999980926514, -5.204400062561035, -5.548999786376953, -5.51800012588501, -5.60830020904541, -5.519000053405762], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 2.1186, 2.1182, 2.1182, 2.1179, 2.1179, 2.1179, 2.1178, 2.1178, 2.1178, 2.1178, 2.1177, 2.1175, 2.1119, 2.1113, 2.1096, 2.109, 2.1079, 2.1069, 2.1023, 2.1009, 2.1007, 2.0986, 2.0971, 2.097, 2.0964, 2.0955, 2.0948, 2.0948, 2.0942, 2.0942, 2.0831, 2.0872, 2.0829, 2.0844, 2.056, 1.9141, 1.9322, 1.9473, 1.9338, 1.7694, 1.6481, 1.2741, 1.4599, 0.727, 1.7954, 0.9417, 0.5898, 0.9851, 1.2955, 1.1208, 0.5499, -0.1318, 2.1417, 2.1405, 2.1404, 2.1403, 2.1402, 2.1402, 2.1401, 2.1401, 2.1401, 2.14, 2.1398, 2.1396, 2.1393, 2.136, 2.1331, 2.133, 2.133, 2.1321, 2.1317, 2.1314, 2.1268, 2.1248, 2.1184, 2.1176, 2.1168, 2.1166, 2.1159, 2.1142, 2.1142, 2.1123, 2.0667, 2.0512, 2.0673, 2.0402, 2.0074, 2.0139, 1.9987, 2.0115, 1.9478, 1.8937, 1.5722, 1.6085, 1.4153, 1.9817, 1.8128, 1.1607, 1.8056, 1.7928, 1.8519, 1.2278, 1.8867, 1.369, 1.3069, 2.1989, 2.1965, 2.1964, 2.1961, 2.1959, 2.1957, 2.1957, 2.1957, 2.1956, 2.1956, 2.1955, 2.1955, 2.1952, 2.195, 2.1851, 2.1832, 2.1782, 2.1778, 2.1746, 2.1727, 2.1694, 2.1691, 2.1678, 2.1658, 2.1649, 2.1643, 2.1639, 2.1635, 2.1634, 2.1631, 2.163, 2.1411, 2.1297, 2.1617, 2.0871, 2.1418, 2.0543, 2.0672, 1.9753, 2.0077, 1.9629, 1.9378, 1.8927, 1.8286, 1.842, 1.6294, 1.7289, 1.148, 0.6825, 1.1613, 1.1472, 1.7518, 1.3474, 0.4811, 0.9932, 1.6299, 2.2484, 2.2483, 2.2479, 2.2476, 2.2475, 2.2475, 2.2475, 2.2475, 2.2475, 2.2474, 2.2463, 2.2449, 2.2441, 2.2428, 2.2405, 2.2365, 2.2309, 2.2299, 2.2279, 2.2272, 2.2249, 2.2245, 2.2243, 2.2216, 2.2211, 2.2209, 2.2209, 2.2205, 2.2191, 2.2167, 2.216, 2.2139, 2.1677, 2.1486, 2.1084, 2.1161, 2.0859, 2.0807, 2.06, 2.036, 2.0232, 2.012, 1.926, 1.9299, 1.9658, 1.8677, 1.363, 1.2662, 1.6116, 1.1205, 0.2916, 1.0496, 2.306, 2.303, 2.3026, 2.3025, 2.3024, 2.3023, 2.3022, 2.3022, 2.3022, 2.3019, 2.3019, 2.3019, 2.3019, 2.3018, 2.3018, 2.3017, 2.3013, 2.3007, 2.3007, 2.2994, 2.2974, 2.2964, 2.2948, 2.2893, 2.2854, 2.2785, 2.278, 2.2772, 2.2742, 2.2732, 2.269, 2.2686, 2.2632, 2.2469, 2.2456, 2.2077, 2.2127, 1.9121, 2.2071, 2.0604, 2.0181, 1.6746, 1.7082, 0.7824, 1.7888, 1.1509, 2.3219, 2.3217, 2.3217, 2.3216, 2.3216, 2.3215, 2.3213, 2.3213, 2.3211, 2.3201, 2.3199, 2.3195, 2.3185, 2.3176, 2.3162, 2.3155, 2.3142, 2.3053, 2.304, 2.3016, 2.3004, 2.2991, 2.2991, 2.2985, 2.298, 2.2965, 2.2964, 2.2951, 2.295, 2.2942, 2.2758, 2.2738, 2.1902, 2.2534, 2.1773, 2.2748, 2.1276, 2.0435, 2.0786, 2.0575, 2.1635, 1.7511, 2.0655, 1.8541, 2.073, 1.4747, 1.4047, 1.0123, 0.8037, 0.7908, 2.3834, 2.3802, 2.38, 2.38, 2.3799, 2.3798, 2.3798, 2.3796, 2.3793, 2.3785, 2.3777, 2.3775, 2.3772, 2.3759, 2.3673, 2.3646, 2.3616, 2.3556, 2.3525, 2.3484, 2.347, 2.3461, 2.3429, 2.3425, 2.3423, 2.3411, 2.3405, 2.3385, 2.338, 2.3379, 2.3319, 2.3364, 2.3201, 2.3091, 2.2974, 2.186, 2.2218, 2.1747, 2.169, 2.1443, 2.1212, 1.9575, 1.8699, 1.9225, 1.7231, 1.6322, 1.3464, 0.709, 1.0149, 1.5946, 2.3985, 2.3964, 2.3957, 2.3956, 2.3956, 2.3954, 2.3954, 2.3953, 2.3952, 2.3947, 2.3943, 2.3933, 2.3906, 2.3799, 2.3745, 2.3732, 2.3725, 2.3711, 2.3705, 2.3698, 2.3697, 2.3691, 2.3676, 2.3653, 2.365, 2.365, 2.3634, 2.3616, 2.3591, 2.3584, 2.3568, 2.3296, 2.353, 2.294, 2.2837, 2.3289, 2.3383, 2.2732, 2.2047, 2.2716, 2.3483, 2.0762, 1.9332, 1.9286, 1.7865, 1.8498, 1.562, 1.4256, 1.5661, 0.7506, 0.4901, 0.5938, 2.454, 2.4511, 2.4511, 2.4508, 2.4508, 2.4506, 2.4506, 2.4506, 2.4506, 2.4504, 2.4499, 2.4393, 2.4381, 2.4369, 2.4319, 2.4315, 2.4264, 2.4258, 2.424, 2.4232, 2.4229, 2.4213, 2.4202, 2.42, 2.4179, 2.4171, 2.4156, 2.4151, 2.4146, 2.4145, 2.3973, 2.3888, 2.4113, 2.3682, 2.3403, 2.3412, 2.31, 2.3496, 2.3038, 2.2966, 2.2458, 2.3965, 2.2488, 2.1804, 2.3254, 1.972, 1.6694, 1.5283, 1.8435, 1.2601, 1.5658, 2.4799, 2.4795, 2.4792, 2.4792, 2.4791, 2.4791, 2.479, 2.479, 2.4787, 2.4787, 2.4786, 2.4706, 2.4625, 2.4621, 2.4615, 2.4567, 2.4555, 2.4551, 2.4537, 2.4536, 2.4528, 2.4528, 2.4517, 2.4513, 2.4513, 2.4506, 2.4501, 2.4489, 2.4485, 2.4474, 2.4271, 2.4193, 2.4319, 2.4157, 2.4184, 2.2751, 2.2724, 2.386, 2.0024, 1.9818, 1.4375, 1.9604, 2.0425, 1.5051, 1.4286, 1.3526, 1.6081, 1.0821, 0.8296, 0.897, -0.3846]}, \"token.table\": {\"Topic\": [9, 3, 4, 2, 1, 7, 2, 8, 1, 9, 8, 2, 3, 6, 7, 8, 9, 4, 5, 2, 7, 10, 7, 8, 9, 2, 5, 7, 8, 9, 1, 6, 8, 2, 8, 4, 6, 6, 10, 4, 2, 8, 9, 3, 1, 3, 4, 5, 6, 7, 9, 10, 6, 9, 10, 2, 7, 6, 8, 9, 1, 2, 7, 10, 1, 2, 3, 5, 6, 8, 9, 1, 6, 6, 8, 1, 5, 6, 8, 10, 4, 5, 6, 8, 9, 2, 4, 7, 3, 7, 9, 9, 7, 10, 9, 5, 2, 8, 1, 2, 3, 4, 5, 6, 7, 8, 10, 8, 3, 10, 3, 2, 9, 1, 2, 3, 4, 5, 8, 9, 10, 1, 2, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 8, 1, 2, 4, 5, 6, 7, 8, 9, 10, 3, 5, 5, 5, 3, 9, 9, 10, 9, 9, 1, 2, 3, 4, 5, 7, 8, 9, 7, 8, 2, 6, 7, 10, 9, 1, 9, 3, 9, 10, 10, 10, 1, 2, 3, 5, 6, 7, 8, 9, 10, 1, 6, 8, 7, 10, 5, 10, 9, 1, 6, 8, 4, 7, 1, 2, 3, 4, 8, 9, 10, 6, 10, 3, 5, 8, 7, 3, 10, 2, 7, 9, 9, 1, 1, 2, 3, 4, 6, 7, 9, 9, 4, 6, 2, 4, 5, 3, 2, 3, 4, 5, 6, 7, 10, 8, 6, 7, 10, 5, 7, 3, 4, 6, 8, 9, 10, 2, 3, 10, 3, 4, 10, 1, 2, 3, 6, 1, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 10, 3, 4, 7, 1, 2, 4, 6, 8, 9, 10, 1, 10, 1, 2, 3, 5, 6, 7, 8, 9, 10, 1, 4, 7, 8, 3, 5, 6, 9, 2, 7, 1, 3, 9, 9, 3, 5, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 8, 10, 1, 2, 4, 5, 6, 7, 9, 10, 2, 5, 6, 7, 8, 9, 7, 1, 2, 3, 5, 7, 8, 10, 1, 7, 9, 9, 1, 2, 4, 6, 7, 4, 5, 7, 3, 10, 2, 1, 9, 5, 9, 7, 8, 9, 6, 7, 8, 8, 10, 2, 4, 8, 4, 5, 10, 1, 5, 7, 9, 1, 2, 6, 2, 9, 10, 1, 2, 3, 4, 5, 9, 10, 2, 2, 6, 8, 9, 10, 6, 1, 8, 10, 8, 3, 4, 8, 9, 3, 1, 2, 3, 5, 7, 8, 1, 2, 3, 4, 5, 6, 7, 9, 9, 9, 5, 7, 4, 10, 1, 2, 1, 3, 5, 6, 7, 8, 9, 6, 4, 1, 5, 6, 1, 1, 2, 9, 1, 2, 5, 6, 8, 7, 10, 5, 9, 10, 1, 7, 10, 2, 9, 1, 2, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 5, 6, 2, 3, 6, 7, 8, 1, 10, 2, 8, 9, 2, 6, 5, 2, 5, 7, 8, 6, 4, 1, 10, 2, 5, 6, 7, 1, 2, 3, 4, 5, 7, 9, 10, 4, 5, 7, 8, 9, 10, 7, 2, 3, 6, 3, 1, 2, 3, 4, 5, 7, 8, 9, 10, 5, 2, 3, 8, 9, 10, 1, 2, 9, 4, 5, 9, 10, 3, 5, 6, 1, 7, 8, 9, 10, 1, 2, 3, 4, 5, 7, 8, 9, 7, 7, 10, 4, 1, 6, 7, 10, 2, 3, 5, 6, 8, 10, 2, 4, 5, 1, 2, 9, 6, 7, 7, 10, 1, 8, 1, 5, 6, 2, 5, 6, 7, 10, 6, 1, 5, 3, 7, 6, 8, 10, 2, 2, 10, 1, 5, 6, 7, 8, 9, 6, 8, 1, 2, 3, 4, 7, 9, 10, 2, 4, 5, 8, 3, 4, 5, 6, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 3, 8, 9, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 5, 6, 1, 2, 3, 5, 6, 7, 8, 10, 5, 6, 7, 8, 6, 2, 8, 9, 7, 9, 6, 8, 8, 6, 7, 9, 10, 7, 7, 10, 2, 5, 10, 2, 6, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 4, 5, 6, 8, 10, 1, 3, 5, 7, 10, 1, 2, 5, 6, 7, 9, 9, 1, 2, 3, 4, 5, 7, 8, 9, 10, 1, 7, 4, 5, 9, 3, 1, 3, 6, 7, 9, 1, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 8, 1, 5, 10, 3, 10, 4, 8, 1, 4, 5, 6, 8, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 5, 1, 3, 4, 7, 10, 1, 5, 1, 7, 6, 1, 3, 5, 8, 3, 10, 1, 2, 3, 4, 6, 7, 8, 9, 10, 3, 4, 5, 6, 8, 9, 9, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 4, 5, 6, 7, 8, 9, 10, 10, 1, 2, 4, 5, 6, 7, 10, 6, 3, 4, 1, 2, 10, 5, 5, 9, 10, 3, 5, 8, 10, 8, 8, 1, 2, 3, 6, 8, 9, 7, 10, 1, 3, 2, 3, 4, 5, 7, 8, 10, 5, 7, 1, 4, 5, 7, 4, 5, 10, 3, 5, 7, 8, 1, 5, 7, 5, 7, 4, 7, 5, 6, 7, 8, 10, 5, 7, 1, 3, 8, 1, 2, 3, 4, 5, 7, 8, 9, 10, 4, 9, 5, 1, 2, 3, 7, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 1, 3, 8, 1, 3, 5, 10, 3, 4, 5, 6, 7, 8, 10, 1, 3, 3, 7, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 5, 6, 7, 8, 10, 1, 3, 5, 9, 10, 1, 2, 4, 7, 8, 10, 5, 1, 2, 3, 4, 6, 7, 9, 7, 1, 8, 10, 3, 5, 4, 1, 1, 8, 3, 5, 6, 10, 1, 8, 4, 7, 2, 2, 3, 7, 8, 9, 10, 3, 5, 5, 2, 3, 3, 1, 2, 3, 4, 5, 6, 7, 9, 10, 10, 2, 3, 6, 7, 8, 9, 10, 1, 4, 8, 1, 2, 1, 3, 4, 5, 6, 8, 10, 6, 8, 3, 5, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1, 3, 8, 10, 10, 5, 7, 3, 4, 5, 6, 7, 10, 4, 8, 1, 4, 6, 7, 8, 2, 1, 2, 3, 4, 6, 7, 8, 9, 1, 3, 5, 7, 10, 4, 8, 1, 2, 3, 4, 5, 6, 8, 10, 1, 2, 5, 6, 1, 2, 3, 4, 5, 8, 1, 2, 3, 4, 5, 6, 8, 9, 10, 3, 7, 1, 5, 5, 2, 1, 6, 6, 1, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 10, 6, 1, 3, 4, 7, 8, 8, 3, 3, 6, 8, 9, 6, 10, 10, 1, 3, 4, 5, 7, 8, 9, 10, 8, 1, 5, 6, 7, 8, 9, 10, 1, 3, 6, 8, 10, 1, 2, 3, 4, 5, 8, 9, 10, 1, 2, 5, 6, 7, 1, 3, 5, 6, 7, 8, 10, 1, 8, 1, 3, 4, 5, 7, 8, 10, 1, 6, 3, 4, 4, 1, 3, 4, 5, 7, 4, 5, 3, 4, 5, 9, 3, 4, 5, 6, 7, 9, 5, 4, 1, 2, 3, 4, 5, 6, 7, 8, 10, 5, 1, 2, 3, 4, 5, 7, 8, 10, 3, 4, 1, 3, 5, 7, 8, 10, 6, 10, 1, 1, 2, 3, 6, 7, 8, 8, 6, 2, 6, 1, 3, 8, 1, 2, 3, 4, 5, 7, 8, 10, 10, 1, 8, 1, 1, 3, 10, 1, 4, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 5, 6, 8, 10, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 4, 3], \"Freq\": [0.9708213535251533, 0.995025924099638, 0.9966794170388764, 0.9953934910380067, 0.9923082865098375, 0.9828672690256111, 0.9752586146427276, 0.9806802665086639, 0.9941888070181313, 0.9918572707255613, 0.9547383444014292, 0.012610164527281736, 0.012610164527281736, 0.037830493581845204, 0.02522032905456347, 0.05044065810912694, 0.8616945760309186, 0.9925574743155127, 0.9901564906804065, 0.990928821754899, 0.003603377533654178, 0.9632930677473762, 0.9903776489781956, 0.9924327351064942, 0.9641916025364112, 0.7029838411722444, 0.008732718523878812, 0.013099077785818218, 0.004366359261939406, 0.2663479149783038, 0.15355421818556605, 0.7624761178869487, 0.07942459561322383, 0.9859694633318073, 0.0070426390237986235, 0.04156801809242793, 0.9560644161258425, 0.9928436090709241, 0.991495416132497, 0.9813892459053913, 0.07916818750380802, 0.05480874519494402, 0.8586703413874562, 0.9927666828279996, 0.04265008782386509, 0.014216695941288363, 0.04671200094994748, 0.06295965345427704, 0.26808626632143767, 0.07920730595860659, 0.4549342701212276, 0.03249530500865912, 0.9536107498385087, 0.9351646455373961, 0.0604832855322694, 0.9748364904566745, 0.01433583074200992, 0.9865990849037003, 0.006365155386475486, 0.9898769441854559, 0.024302058346806637, 0.09720823338722655, 0.8080434400313207, 0.06683066045371824, 0.03205659609605498, 0.7773724553293333, 0.008014149024013746, 0.008014149024013746, 0.08815563926415121, 0.008014149024013746, 0.07613441572813058, 0.9888687876821406, 0.9955850329602896, 0.9877917070788378, 0.00551838942502144, 0.006094100822454024, 0.19501122631852877, 0.7526214515730719, 0.03961165534595115, 0.006094100822454024, 0.015675052027300768, 0.06270020810920307, 0.015675052027300768, 0.015675052027300768, 0.8778029135288431, 0.04070238781761208, 0.04070238781761208, 0.9158037258962718, 0.014666741762439315, 0.004888913920813105, 0.9826716980834341, 0.9745188640795016, 0.9926353852826927, 0.9914078089809816, 0.9698662985340406, 0.9914893729544868, 0.9844496926410377, 0.9492778442588313, 0.014558124373023017, 0.026204623871441433, 0.014558124373023017, 0.20963699097153146, 0.5299157271780378, 0.014558124373023017, 0.03202787362065064, 0.15140449347943938, 0.005823249749209207, 0.979754034618902, 0.994014044837837, 0.9572648660633528, 0.971831721385046, 0.9869604760934344, 0.007420755459349131, 0.023448487189718474, 0.398624282225214, 0.07738000772607095, 0.2673127539627906, 0.016413941032802932, 0.15007031801419823, 0.05627636925532434, 0.00937939487588739, 0.02299266218365467, 0.034488993275482, 0.8507285007952227, 0.08047431764279134, 0.16410737533675407, 0.08061414928823006, 0.03454892112352717, 0.19865629646028124, 0.04030707464411503, 0.0460652281647029, 0.03166984436323324, 0.09788860984999365, 0.1122839936514633, 0.19001906617939943, 0.9804770787364401, 0.008318642513352816, 0.07070846136349894, 0.004159321256676408, 0.029115248796734855, 0.5615083696513151, 0.2537185966572609, 0.014557624398367427, 0.02703558816839665, 0.029115248796734855, 0.9675064846003908, 0.9901953256420578, 0.9901714329807328, 0.9882008830654868, 0.9963160548190391, 0.9905496850167446, 0.937653799598427, 0.024042405117908384, 0.9612299602169709, 0.9488944814017474, 0.08514110090063519, 0.015480200163751854, 0.10449135110532501, 0.027090350286565743, 0.019350250204689817, 0.13158170139189074, 0.0038700500409379634, 0.6153379565091361, 0.996604741227952, 0.9653875311215115, 0.7698434379626441, 0.07185205420984678, 0.1129103709011878, 0.04105831669134102, 0.994936892313037, 0.9526302419109292, 0.02165068731615748, 0.024786762677830088, 0.9666837444353734, 0.006196690669457522, 0.9817803081161498, 0.9764127875759379, 0.30250681518798106, 0.10359822437944557, 0.008287857950355646, 0.08287857950355645, 0.4268246844433158, 0.004143928975177823, 0.05801500565248952, 0.008287857950355646, 0.008287857950355646, 0.015965459061774295, 0.9259966255829092, 0.03193091812354859, 0.02380298713856419, 0.9283164984040034, 0.00515863248513714, 0.9904574371463309, 0.996616921211976, 0.004688407908131013, 0.9869098646615782, 0.004688407908131013, 0.9631151880158797, 0.9962330745407569, 0.10171626864364275, 0.7120138805054992, 0.035379571702136604, 0.008844892925534151, 0.035379571702136604, 0.04864691109043783, 0.06191425047873906, 0.9568856858351616, 0.9696861042460501, 0.6385548215799507, 0.2699929782519255, 0.08999765941730849, 0.989975241569576, 0.9946207752792378, 0.9922701983302633, 0.9827751973337179, 0.9715113381196874, 0.970528185301192, 0.9648236423857333, 0.9734619296919437, 0.008246541406762278, 0.24533460685117775, 0.004123270703381139, 0.32161511486372885, 0.006184906055071708, 0.35253964513908737, 0.059787425199026516, 0.9577812750340094, 0.005385280910680285, 0.9908916875651723, 0.9950595297978413, 0.9858134817682113, 0.006798713667366974, 0.958075034836948, 0.8644092810678891, 0.07599202470926497, 0.004749501544329061, 0.009499003088658121, 0.004749501544329061, 0.018998006177316243, 0.018998006177316243, 0.9550882850830693, 0.1649914081282772, 0.7837091886093168, 0.04714040232236492, 0.9905435571261124, 0.9763385228538651, 0.014813524950984912, 0.22812828424516765, 0.05629139481374267, 0.0029627049901969826, 0.07999303473531853, 0.6162426379609723, 0.9907843126411096, 0.9908949954713675, 0.985168842545583, 0.00675430749053651, 0.9793745861277939, 0.00675430749053651, 0.011688944976640747, 0.0876670873248056, 0.8883598182246968, 0.005844472488320374, 0.9970995933771856, 0.9902786663261247, 0.9675987915536767, 0.062667124709102, 0.1096674682409285, 0.564004122381918, 0.027416867060232126, 0.003916695294318875, 0.0470003435318265, 0.14883442118411724, 0.0156667811772755, 0.019583476471594377, 0.9379748548624065, 0.052761085586010364, 0.9399050304033403, 0.08425978752079791, 0.030333523507487248, 0.5257810741297789, 0.05729665551414258, 0.016851957504159583, 0.07751900451913409, 0.2055938815507469, 0.05685282639228925, 0.9349131451176454, 0.0029036424119696304, 0.4326427193834749, 0.08420562994711928, 0.020325496883787413, 0.005807284823939261, 0.008710927235908892, 0.0667837754753015, 0.0029036424119696304, 0.3745698711440823, 0.958741917249321, 0.005510011018674259, 0.027550055093371294, 0.9911438520077955, 0.9960709116878147, 0.017833062829778628, 0.861931370105967, 0.11888708553185752, 0.03469113176254702, 0.9597879787638008, 0.9727076261599553, 0.9437472202858946, 0.02301822488502182, 0.9957470704095205, 0.9801105954742918, 0.012896192045714367, 0.9863546429455526, 0.006946159457363047, 0.017413157772085637, 0.4585464879982551, 0.026119736658128456, 0.002902192962014273, 0.005804385924028546, 0.24958859473322745, 0.07255482405035682, 0.05514166627827118, 0.1073811395945281, 0.002902192962014273, 0.058801612889775604, 0.32340887089376585, 0.004200115206412543, 0.004200115206412543, 0.5922162441041686, 0.01260034561923763, 0.008400230412825086, 0.04637209905044262, 0.5832183226728745, 0.00178354227117087, 0.00713416908468348, 0.09274419810088524, 0.1248479589819609, 0.08025940220268915, 0.06242397949098045, 0.8461875370357371, 0.005679110986817028, 0.07382844282862136, 0.017037332960451082, 0.005679110986817028, 0.04543288789453622, 0.9948801754206578, 0.004328874093519789, 0.10389297824447494, 0.09956410415095515, 0.008657748187039578, 0.09523523005743537, 0.6233578694668497, 0.06926198549631662, 0.005406864002891889, 0.021627456011567556, 0.967828656517648, 0.9585785530690631, 0.01141444599937879, 0.8732051189524775, 0.02853611499844698, 0.03424333799813637, 0.04565778399751516, 0.9781954836146536, 0.952845749984426, 0.03664791346093946, 0.968430294778466, 0.02531843907917558, 0.9919446235498907, 0.9962545337338925, 0.9912327727197553, 0.994406414032504, 0.9726466504052397, 0.04728814423285291, 0.8984747404242053, 0.04728814423285291, 0.9761049947265469, 0.9931813978104562, 0.9930302783702087, 0.7250526366740078, 0.2694885906221976, 0.9948879731719474, 0.9946435324588772, 0.9980077819896708, 0.9780405847658162, 0.0054639138813732755, 0.010927827762746551, 0.014509235942406112, 0.014509235942406112, 0.9285911003139912, 0.043527707827218334, 0.0065295575981758405, 0.985963197324552, 0.9894568663561562, 0.9942498008939782, 0.9919542287180482, 0.9924529384052156, 0.10152794396437467, 0.06768529597624978, 0.3257354868857021, 0.05922463397921856, 0.025381985991093668, 0.4103421068560143, 0.004230330998515611, 0.9896734863448674, 0.971021891858781, 0.005292940054867597, 0.22759642235930666, 0.7568904278460663, 0.005292940054867597, 0.9814449187640024, 0.023305771976617266, 0.9322308790646906, 0.023305771976617266, 0.9614798373702445, 0.03309318812194293, 0.03309318812194293, 0.07170190759754301, 0.8493918284632018, 0.9914299461828446, 0.07079312304540422, 0.7167803708347177, 0.1106142547584441, 0.030971991332364345, 0.06194398266472869, 0.004424570190337764, 0.0032549314524563918, 0.481729854963546, 0.055333834691758656, 0.004882397178684587, 0.0016274657262281959, 0.027666917345879328, 0.029294383072107525, 0.3954741714734516, 0.9492812988452509, 0.992806010462455, 0.9829237870499429, 0.0078009824369043084, 0.9723674014531672, 0.9756387881453219, 0.0034047161905429828, 0.994177127638551, 0.6225630402481362, 0.007367609943764926, 0.003683804971882463, 0.2983882027224795, 0.003683804971882463, 0.05894087955011941, 0.007367609943764926, 0.9918752471519259, 0.9941374337395417, 0.09436084874010801, 0.9020897139554326, 0.9916732243706904, 0.9852480179109053, 0.012478633346396352, 0.9670940843457173, 0.012478633346396352, 0.04804034806054614, 0.9234422460527202, 0.005337816451171793, 0.005337816451171793, 0.021351265804687172, 0.9437181302717217, 0.954831714413735, 0.9693154675557607, 0.8919830362642239, 0.10457732149304695, 0.01931004111390499, 0.05793012334171497, 0.9075719323535345, 0.9791537825420906, 0.017279184397801597, 0.9817406143994301, 0.9096651650573133, 0.07671874886025534, 0.010959821265750761, 0.004070060677182854, 0.19943297318195985, 0.11803175963830277, 0.012210182031548562, 0.03663054609464569, 0.07326109218929137, 0.004070060677182854, 0.004070060677182854, 0.5413180700653196, 0.008140121354365709, 0.9486907571774549, 0.0020305199785933606, 0.06903767927217426, 0.08122079914373442, 0.4101650356758589, 0.06497663931498754, 0.09746495897248131, 0.14010587852294187, 0.010152599892966803, 0.09340391901529459, 0.03248831965749377, 0.01123011547073278, 0.07861080829512945, 0.12353127017806058, 0.7748779674805618, 0.9953023442158264, 0.06668995997848148, 0.03334497998924074, 0.011114993329746914, 0.8780844730500063, 0.9965231566731033, 0.9753107593808487, 0.0535564739027937, 0.8211992665095035, 0.11901438645065267, 0.9622776623078727, 0.02872470633754844, 0.9976107767383855, 0.05656719091533875, 0.90507505464542, 0.007070898864417344, 0.021212696593252034, 0.990097500278642, 0.9901637775527031, 0.9705175591033698, 0.022183258493791307, 0.8741060758851678, 0.005532316935982075, 0.08298475403973112, 0.033193901615892446, 0.03516988140242528, 0.11137129110768006, 0.00879247035060632, 0.014654117251010535, 0.014654117251010535, 0.002930823450202107, 0.8089072722557815, 0.002930823450202107, 0.008403711599289815, 0.09244082759218797, 0.008403711599289815, 0.06722969279431852, 0.8151600251311121, 0.008403711599289815, 0.9908322269349967, 0.01758463703864037, 0.9671550371252202, 0.01758463703864037, 0.9664592001745458, 0.02270834282995598, 0.13120375857307898, 0.04541668565991196, 0.15138895219970652, 0.05803243167655417, 0.06307873008321105, 0.0025231492033284423, 0.17662044423299095, 0.348194590059325, 0.9883540405287555, 0.009852902504184576, 0.009852902504184576, 0.04926451252092288, 0.8966141278807964, 0.02955870751255373, 0.02291215145881643, 0.07790131495997586, 0.8889914766020774, 0.9913102291710038, 0.937427020294986, 0.015242715777154244, 0.04191746838717417, 0.02305502490427137, 0.02305502490427137, 0.9452560210751262, 0.010922383622304202, 0.02730595905576051, 0.005461191811152101, 0.9393249915181614, 0.016383575433456303, 0.029045121985395085, 0.8174127187318331, 0.03319442512616581, 0.008298606281541452, 0.008298606281541452, 0.029045121985395085, 0.03319442512616581, 0.04149303140770726, 0.9700030654774904, 0.0518668471826064, 0.9400866051847411, 0.9691133412520309, 0.01271445228774571, 0.02542890457549142, 0.9535839215809282, 0.01271445228774571, 0.31062433265396044, 0.03304514177169792, 0.006609028354339584, 0.6212486653079209, 0.006609028354339584, 0.01982708506301875, 0.9896841528861501, 0.9899301028411597, 0.990874511568937, 0.006107411113723906, 0.9832931893095489, 0.006107411113723906, 0.9705585136245506, 0.95362357386661, 0.3943517186248452, 0.6037420116999843, 0.03099503668429164, 0.9608461372130408, 0.8295511679578068, 0.15082748508323762, 0.011602114237172124, 0.04403350975800898, 0.1565635902506986, 0.009785224390668663, 0.650717421979466, 0.13699314146936128, 0.9661059436543616, 0.978618383310337, 0.9728290382263864, 0.9584202318309394, 0.990666943772151, 0.9929759475704043, 0.9571109204399425, 0.027346026298284073, 0.982074209070854, 0.8963100479720869, 0.0924764335209296, 0.9831011294525935, 0.010042638651786838, 0.7732831761875865, 0.1707248570803762, 0.010042638651786838, 0.03012791595536051, 0.005725061011442669, 0.9904355549795817, 0.004181157413004199, 0.8696807419048734, 0.029268101891029397, 0.008362314826008398, 0.012543472239012598, 0.016724629652016796, 0.05435504636905459, 0.983531826773622, 0.0030077425895217802, 0.0030077425895217802, 0.00902322776856534, 0.9897867666521949, 0.04994273505836004, 0.006242841882295005, 0.8739978635213007, 0.06242841882295005, 0.006242841882295005, 0.03120271408550084, 0.3726990849101489, 0.0017334841158611579, 0.012134388811028105, 0.005200452347583474, 0.21495203036678356, 0.04680407112825126, 0.00866742057930579, 0.30162623615984147, 0.005200452347583474, 0.011354621331801797, 0.8629512212169366, 0.11922352398391887, 0.005677310665900898, 0.9897126424094158, 0.007385915241861312, 0.008867644468614974, 0.5630954237570508, 0.042860281598305704, 0.0029558814895383244, 0.06207351128030481, 0.06355145202507398, 0.10049997064430304, 0.005911762979076649, 0.1330146670292246, 0.016257348192460785, 0.03366100414498994, 0.7453508060676344, 0.07213072316783559, 0.03366100414498994, 0.11060044219068123, 0.024235511909462902, 0.09088316966048589, 0.01817663393209718, 0.03029438988682863, 0.8179485269443729, 0.0060588779773657255, 0.012117755954731451, 0.0060588779773657255, 0.021746167035139524, 0.9513948077873542, 0.010873083517569762, 0.016309625276354642, 0.9719503138506234, 0.006205247832379173, 0.9742239096835301, 0.01861574349713752, 0.9312457839495013, 0.9704702999459535, 0.981681580641724, 0.9917533001966415, 0.9916513208876163, 0.0069370722737773964, 0.9434418292337259, 0.04162243364266437, 0.9954687988340114, 0.9939454342240076, 0.9906476653520296, 0.9942309914666677, 0.010850065915891658, 0.17902608761221236, 0.8083299107339286, 0.020865739370502887, 0.01391049291366859, 0.9598240110431328, 0.9934913367309499, 0.006316625871130307, 0.01473879369930405, 0.050533006969042456, 0.2989869579001679, 0.3137257515994719, 0.018949877613390922, 0.052638548926085896, 0.16212673069234454, 0.008422167828173743, 0.07158842653947682, 0.019450497696547338, 0.08752723963446302, 0.009725248848273669, 0.07780199078618935, 0.1556039815723787, 0.6418664239860622, 0.827846481968822, 0.11200275932519357, 0.00973937037610379, 0.014609055564155683, 0.034087796316363266, 0.8126552260192084, 0.03052226201011111, 0.02670697925884722, 0.06867508952275, 0.034337544761375, 0.02670697925884722, 0.9705217132425611, 0.028383765989505306, 0.030964108352187606, 0.0077410270880469015, 0.15740088412362033, 0.16256156884898493, 0.10837437923265662, 0.4309171745679442, 0.005160684725364601, 0.06708890142973982, 0.9971628108885056, 0.9598315522833385, 0.9961195179405845, 0.9919373780895226, 0.990740573560406, 0.9552286359596557, 0.08361052930376385, 0.1170547410252694, 0.02229614114767036, 0.7692168695946274, 0.01114807057383518, 0.9906516549992763, 0.9639744086146541, 0.9918812128632432, 0.3077437193736397, 0.01250990729161137, 0.2577040902071942, 0.052541610624767754, 0.10007925833289096, 0.002501981458322274, 0.005003962916644548, 0.16763275770759237, 0.002501981458322274, 0.09257331395792413, 0.9763301408288058, 0.013615569265977028, 0.939474279352415, 0.040846707797931085, 0.9604766822443436, 0.9919165483690644, 0.9726226399901778, 0.020991855539356355, 0.006658699744454133, 0.8989244655013079, 0.08656309667790373, 0.9732015588685093, 0.9553227412127469, 0.9785382792595011, 0.2093670585091777, 0.014607004082035652, 0.5648041578387119, 0.014607004082035652, 0.0998145278939103, 0.0024345006803392754, 0.02191050612305348, 0.051124514287124785, 0.024345006803392754, 0.9942143121258354, 0.12002046917337193, 0.04445202561976738, 0.8045816637177896, 0.004445202561976738, 0.02222601280988369, 0.991324801257105, 0.95776156319186, 0.9954119649022467, 0.939524520447333, 0.9966510890248675, 0.4385010015574003, 0.275910742552971, 0.2562028323706159, 0.02463488772794384, 0.9953374347007525, 0.9696627085184294, 0.02877495642109236, 0.004110708060156051, 0.6864882460460606, 0.04932849672187262, 0.041107080601560514, 0.04932849672187262, 0.020553540300780257, 0.008221416120312103, 0.10687840956405734, 0.07713002189288558, 0.6805590167019316, 0.1270376831176939, 0.07259296178153937, 0.036296480890769686, 0.004537060111346211, 0.9601187923987456, 0.9773625464541958, 0.12633909069309007, 0.007018838371838338, 0.35269662818487646, 0.20003689359739263, 0.17371624970299887, 0.005264128778878754, 0.05790541656766629, 0.010528257557757508, 0.007018838371838338, 0.059660126160625875, 0.05499410784818866, 0.05499410784818866, 0.05499410784818866, 0.3757930702959558, 0.009165684641364776, 0.01833136928272955, 0.009165684641364776, 0.013748526962047165, 0.4170386511820973, 0.9920206782929963, 0.9912676161175932, 0.010272929760152006, 0.06677404344098804, 0.7447874076110205, 0.07191050832106403, 0.02054585952030401, 0.08218343808121605, 0.9707804090244629, 0.9698416526174836, 0.9950333070321786, 0.9682615579701687, 0.02482721943513253, 0.9858207238416162, 0.990833490227945, 0.9908216026930826, 0.02459921213208012, 0.9593692731511247, 0.037703757654140864, 0.005386251093448695, 0.14542877952311475, 0.8079376640173043, 0.957753891239717, 0.9675255201298989, 0.978937757520276, 0.03311342583426926, 0.931906412764435, 0.018921957619582437, 0.009460978809791219, 0.009460978809791219, 0.9872368862519324, 0.9839471310500061, 0.9954196502040614, 0.9771647687781395, 0.012981127520886915, 0.1882263490528603, 0.09735845640665187, 0.22067916785507757, 0.019471691281330375, 0.43486777194971166, 0.02596225504177383, 0.991813295504617, 0.007629333042343208, 0.08389263550255374, 0.8628956794548385, 0.011984662214650535, 0.0359539866439516, 0.9960053623364994, 0.06016299854348201, 0.9224993110000574, 0.007055820874522705, 0.910200892813429, 0.07055820874522704, 0.01411164174904541, 0.9924241937771022, 0.9818067727078253, 0.0060605356339989225, 0.053286233673347665, 0.9363152488316804, 0.9539466459150832, 0.9794979052726116, 0.01146771542935485, 0.0611611489565592, 0.4510634735546241, 0.22553173677731206, 0.2446445958262368, 0.9925389330246633, 0.9704390477100432, 0.013622429910625481, 0.09535700937437837, 0.8854579441906563, 0.07682537408271942, 0.007316702293592325, 0.021950106880776976, 0.7206951759188441, 0.06950867178912709, 0.047558564908350115, 0.010975053440388488, 0.021950106880776976, 0.02560845802757314, 0.9540878358741299, 0.023852195896853248, 0.991337274453969, 0.13483875096131445, 0.014577162266088049, 0.6231736868752641, 0.06924152076391822, 0.13848304152783644, 0.02186574339913207, 0.21618405638561186, 0.0014508997072859857, 0.3482159297486366, 0.07834858419344322, 0.0507814897550095, 0.011607197658287886, 0.04787969034043753, 0.16395166692331636, 0.007254498536429928, 0.07399588507158526, 0.9923812932382816, 0.18496939236240104, 0.7344372932036511, 0.0761638674433416, 0.10175005353698587, 0.7658030345152094, 0.016065797926892503, 0.10710531951261669, 0.02170477829606176, 0.7249395950884627, 0.14325153675400762, 0.017363822636849408, 0.056432423569760574, 0.02170477829606176, 0.017363822636849408, 0.9929009944921868, 0.9919207439110794, 0.8219214518621544, 0.01896741811989587, 0.15173934495916697, 0.10500239206444308, 0.01228751396498802, 0.17872747585437118, 0.10835353223671254, 0.21670706447342508, 0.01228751396498802, 0.18654680292299994, 0.14745016757985624, 0.00446818689635928, 0.02680912137815568, 0.008798247248157115, 0.013197370872235673, 0.026394741744471345, 0.03519298899262846, 0.026394741744471345, 0.5982808128746838, 0.268346541068792, 0.01759649449631423, 0.1632773082100077, 0.02177030776133436, 0.7782885024677033, 0.01632773082100077, 0.01088515388066718, 0.0054783365753631064, 0.010956673150726213, 0.8381854960305553, 0.10408839493189902, 0.0054783365753631064, 0.02739168287681553, 0.9780342476836856, 0.037477564679900245, 0.014054086754962591, 0.698019642163142, 0.056216347019850364, 0.09369391169975061, 0.004684695584987531, 0.09837860728473813, 0.9628728656625994, 0.06365968190707884, 0.878503610317688, 0.05092774552566307, 0.9792434154581936, 0.012164514477741535, 0.9909165309829016, 0.9803664625047309, 0.02170673966606154, 0.9550965453067076, 0.9953048402794904, 0.961425607470513, 0.00763036196405169, 0.02289108589215507, 0.025778100662848275, 0.9537897245253862, 0.9551145159252056, 0.016467491653882855, 0.975494830235933, 0.033796989305915445, 0.10621910924716282, 0.8159558846713871, 0.009656282658832985, 0.01931256531766597, 0.009656282658832985, 0.9944703120258948, 0.9930611468883489, 0.9926621605434773, 0.025669417258426707, 0.9754378558202148, 0.990587820753818, 0.09155725753812795, 0.00352143298223569, 0.08099295859142087, 0.14790018525389897, 0.5493435452287677, 0.00352143298223569, 0.02112859789341414, 0.00704286596447138, 0.09860012350259932, 0.996409785328334, 0.013558684820481516, 0.013558684820481516, 0.04519561606827172, 0.004519561606827172, 0.004519561606827172, 0.9129514445790887, 0.004519561606827172, 0.0737061479674187, 0.9151846705954488, 0.9920438214023449, 0.9427593964714285, 0.05336373942291105, 0.015454077678282987, 0.10817854374798092, 0.84482291307947, 0.010302718452188659, 0.005151359226094329, 0.005151359226094329, 0.005151359226094329, 0.991125633198815, 0.006394358923863323, 0.030557369318535893, 0.9625571335338806, 0.9922363447613841, 0.9885435798629766, 0.11650146471264522, 0.0030260120704583174, 0.13011851902970764, 0.043877175021645606, 0.03782515088072897, 0.21787286907299885, 0.006052024140916635, 0.09229336814897869, 0.35101740017316485, 0.701936422522847, 0.04094629131383275, 0.2515272180706869, 0.005849470187690393, 0.9808971422657189, 0.1448072354913739, 0.8459791126075001, 0.007346540521740436, 0.8742383220871119, 0.05142578365218305, 0.014693081043480872, 0.029386162086961744, 0.007346540521740436, 0.9568102384312627, 0.9940992433816721, 0.18381531572227483, 0.7826974733980735, 0.011859052627243538, 0.005929526313621769, 0.011859052627243538, 0.9937823835394756, 0.2078769536189002, 0.019988168617201943, 0.01798935175548175, 0.24585447399158392, 0.02198698547892214, 0.1039384768094501, 0.37577757000339657, 0.005996450585160583, 0.716096310467435, 0.010085863527710353, 0.07060104469397246, 0.16137381644336565, 0.04034345411084141, 0.9757832715885031, 0.9697675430599455, 0.003729549806476023, 0.26106848645332165, 0.04102504787123626, 0.01118864941942807, 0.04102504787123626, 0.0634023467100924, 0.5743506701973076, 0.003729549806476023, 0.024568664608525408, 0.925419700254457, 0.008189554869508469, 0.032758219478033875, 0.13350316588172612, 0.004450105529390871, 0.787668678702184, 0.004450105529390871, 0.008900211058781742, 0.062301477411472184, 0.36566008278324924, 0.0038900008806728643, 0.16727003786893316, 0.0661300149714387, 0.1205900273008588, 0.04279000968740151, 0.05835001321009296, 0.015560003522691457, 0.15949003610758744, 0.953762321423929, 0.9956051360794056, 0.8373795011824181, 0.1557915351037057, 0.9770422942978622, 0.990242072108033, 0.994427670327636, 0.9916262615605228, 0.9801716974875334, 0.9665976007330281, 0.016109960012217137, 0.016109960012217137, 0.1118509503575227, 0.003994676798482953, 0.347536881468017, 0.37150494225891467, 0.01997338399241477, 0.0019973383992414767, 0.01198403039544886, 0.053928136779519874, 0.07589885917117611, 0.9959586061404725, 0.04088518374546146, 0.010221295936365365, 0.822814322877412, 0.11243425530001903, 0.010221295936365365, 0.9965583107177994, 0.976029811971018, 0.012639458265456321, 0.0063197291327281606, 0.9290001825110397, 0.044238103929097126, 0.9727619641429763, 0.9765819432776871, 0.9529436342040755, 0.0921847975507287, 0.13827719632609306, 0.049384712973604666, 0.006584628396480622, 0.013169256792961244, 0.6255396976656591, 0.006584628396480622, 0.07243091236128683, 0.972936304746035, 0.32131767711652554, 0.024716744393578887, 0.10813575672190763, 0.46961814347799885, 0.0710606401315393, 0.003089593049197361, 0.003089593049197361, 0.00830176863973313, 0.426157456839634, 0.022138049705955017, 0.5396149615826535, 0.002767256213244377, 0.07481172063662958, 0.006412433197425393, 0.7972791942132239, 0.008549910929900524, 0.029924688254651836, 0.04274955464950262, 0.03633712145207723, 0.002137477732475131, 0.06293080698274169, 0.004195387132182779, 0.04195387132182779, 0.780342006585997, 0.10908006543675226, 0.20442465683353805, 0.05991757183051978, 0.673191542331134, 0.007049126097708209, 0.03172106743968694, 0.0035245630488541044, 0.021147378293124627, 0.025349704199190873, 0.9632887595692532, 0.9632285473165854, 0.11820072733614283, 0.2585640910478124, 0.007387545458508927, 0.007387545458508927, 0.014775090917017853, 0.5910036366807142, 0.9667862609263335, 0.9968171676827139, 0.00590433006200708, 0.9919274504171894, 0.990905089084049, 0.005402795924509294, 0.010805591849018588, 0.7509886335067919, 0.005402795924509294, 0.22151463290488108, 0.9720232739940647, 0.024670641471930578, 0.14357753105788706, 0.7921518954917908, 0.009901898693647385, 0.04950949346823692, 0.8747915236107799, 0.015347219712469823, 0.08440970841858403, 0.007673609856234912, 0.007673609856234912, 0.007673609856234912, 0.9530576554430309, 0.9602551460956309, 0.13417715153547025, 0.031778799047874534, 0.15889399523937267, 0.05649564275177695, 0.014123910687944238, 0.007061955343972119, 0.5155227401099647, 0.05296466507979089, 0.028247821375888476, 0.9931264645826647, 0.51470580145013, 0.02595155301429227, 0.05622836486429992, 0.06920414137144605, 0.017301035342861513, 0.07352940020716144, 0.004325258835715378, 0.2378892359643458, 0.972831446058679, 0.013897592086552557, 0.060934950633406024, 0.0221581638666931, 0.23266072060027754, 0.6259681292340801, 0.027697704833366376, 0.027697704833366376, 0.015212643637595944, 0.9736091928061404, 0.9632285473165854, 0.08298538138486626, 0.005927527241776162, 0.04149269069243313, 0.05334774517598546, 0.806143704881558, 0.011855054483552323, 0.990129365939181, 0.9904985318998952, 0.9910651393218461, 0.0063125168109671725, 0.03631917339960748, 0.01815958669980374, 0.9261389216899909, 0.04827777016205134, 0.011494707181440796, 0.18391531490305274, 0.10805024750554348, 0.1517301347950185, 0.1632248419764593, 0.013793648617728955, 0.3218518010803423, 0.9542534883439668, 0.973179589757676, 0.01871499211072454, 0.988615415269887, 0.008568214257591966, 0.021420535643979912, 0.9682082111078921, 0.9745447962478513, 0.014073955475500867, 0.9711029278095598, 0.2473145438835849, 0.015403674013510478, 0.21907447485881568, 0.1403445854564288, 0.0333746270292727, 0.011124875676424234, 0.08215292807205588, 0.19169016550146373, 0.002567279002251746, 0.056480138049538414, 0.42718018336453534, 0.29844095002179866, 0.18725706668034425, 0.011703566667521516, 0.03511070000256455, 0.039011888891738386, 0.022258647898191194, 0.02967819719758826, 0.21516692968251488, 0.04451729579638239, 0.396945887517743, 0.07419549299397066, 0.051936845095779456, 0.02967819719758826, 0.13355188738914717, 0.9919815814362388, 0.9782659792005958, 0.9611359972283543], \"Term\": [\"0\", \"0_0\", \"10\", \"13\", \"2009_H1N1\", \"2010\", \"2017\", \"2018\", \"2_0\", \"2_7\", \"3C_like\", \"4\", \"4\", \"4\", \"4\", \"4\", \"4\", \"7b\", \"8\", \"A(H1N1\", \"A(H1N1\", \"Acid\", \"Activate\", \"Active\", \"Activities\", \"Activity\", \"Activity\", \"Activity\", \"Activity\", \"Activity\", \"Acute_Respiratory\", \"Acute_Respiratory\", \"Acute_Respiratory\", \"Adaptation\", \"Adaptation\", \"Adenovirus\", \"Adenovirus\", \"Adult_Patients\", \"Affinity\", \"Africa\", \"Age\", \"Age\", \"Age\", \"Analogues\", \"Analysis\", \"Analysis\", \"Analysis\", \"Analysis\", \"Analysis\", \"Analysis\", \"Analysis\", \"Analysis\", \"Animals\", \"Anti_-\", \"Anti_-\", \"Antibody\", \"Antibody\", \"Articles\", \"Articles\", \"Aspergillus\", \"Assessment\", \"Assessment\", \"Assessment\", \"Assessment\", \"Associated\", \"Associated\", \"Associated\", \"Associated\", \"Associated\", \"Associated\", \"Associated\", \"Asthma\", \"Asymmetric\", \"BMC_Infectious\", \"BMC_Infectious\", \"Bacterial\", \"Bacterial\", \"Bacterial\", \"Bacterial\", \"Bacterial\", \"Bat\", \"Bat\", \"Bat\", \"Bat\", \"Bat\", \"Beijing\", \"Beijing\", \"Beijing\", \"Binding\", \"Binding\", \"Binding\", \"Biological\", \"Biologically\", \"Biomedical\", \"Bodies\", \"Brief\", \"CD8_T\", \"CORONAVIRUS\", \"COVID-19\", \"COVID-19\", \"COVID-19\", \"COVID-19\", \"COVID-19\", \"COVID-19\", \"COVID-19\", \"COVID-19\", \"COVID-19\", \"Care\", \"Catalysis\", \"Cats\", \"Challenge\", \"Channel\", \"Channel\", \"Characterization\", \"Characterization\", \"Characterization\", \"Characterization\", \"Characterization\", \"Characterization\", \"Characterization\", \"Characterization\", \"Children\", \"Children\", \"Children\", \"Children\", \"China\", \"China\", \"China\", \"China\", \"China\", \"China\", \"China\", \"China\", \"China\", \"China\", \"Chronic\", \"Clinical\", \"Clinical\", \"Clinical\", \"Clinical\", \"Clinical\", \"Clinical\", \"Clinical\", \"Clinical\", \"Clinical\", \"Clinical_Medicine\", \"Clinical_characteristics\", \"Cloning\", \"Clostridium_difficile\", \"Coding\", \"Codon_Usage\", \"Complete_Genome\", \"Complete_Genome\", \"Comprehensive\", \"Conserved\", \"Coronavirus\", \"Coronavirus\", \"Coronavirus\", \"Coronavirus\", \"Coronavirus\", \"Coronavirus\", \"Coronavirus\", \"Coronavirus\", \"Critical\", \"Cytokine\", \"Data\", \"Data\", \"Data\", \"Data\", \"Definitive\", \"Delivery\", \"Delivery\", \"Dengue_Virus\", \"Dengue_Virus\", \"Dengue_Virus\", \"Department\", \"Design\", \"Detection\", \"Detection\", \"Detection\", \"Detection\", \"Detection\", \"Detection\", \"Detection\", \"Detection\", \"Detection\", \"Diagnosis\", \"Diagnosis\", \"Diagnosis\", \"Diarrhea_Virus\", \"Diarrhea_Virus\", \"Different\", \"Different\", \"Disease_Outbreaks\", \"Diseases\", \"Diseases\", \"Diseases\", \"ER\", \"Ebolavirus\", \"Effect\", \"Effect\", \"Effect\", \"Effect\", \"Effect\", \"Effect\", \"Effect\", \"Effective\", \"Effectiveness\", \"Effects\", \"Effects\", \"Effects\", \"Efficiency\", \"Environment\", \"Epidemics\", \"Epithelial_Cells\", \"Equine\", \"Essential\", \"Etiology\", \"Europe\", \"Evidence\", \"Evidence\", \"Evidence\", \"Evidence\", \"Evidence\", \"Evidence\", \"Evidence\", \"Expressed\", \"Factor\", \"Factor\", \"Fatal\", \"Flavivirus\", \"Flavivirus\", \"France\", \"Gene\", \"Gene\", \"Gene\", \"Gene\", \"Gene\", \"Gene\", \"Gene\", \"Gene_Expression\", \"Genes\", \"Genes\", \"Genes\", \"Genetic_Diversity\", \"Genomic\", \"Global\", \"Global\", \"Global\", \"Global\", \"Global\", \"Global\", \"Glycosylation\", \"Green\", \"Growth\", \"H\", \"H\", \"H\", \"H7N9\", \"H7N9\", \"H7N9\", \"H7N9\", \"HEALTH\", \"HEPATITIS\", \"HHS_Public\", \"HIV-1\", \"HIV-1\", \"HIV-1\", \"HIV-1\", \"HIV-1\", \"HIV-1\", \"HIV-1\", \"HIV-1\", \"HIV-1\", \"HLA\", \"HLA\", \"Hajj\", \"Health\", \"Health\", \"Health\", \"Health\", \"Health\", \"Health\", \"Health\", \"Hemagglutinin\", \"Hemagglutinin\", \"High\", \"High\", \"High\", \"High\", \"High\", \"High\", \"High\", \"High\", \"High\", \"Human_Coronavirus\", \"Human_Coronavirus\", \"Human_Coronavirus\", \"Human_Papillomavirus\", \"Identifies\", \"Illness\", \"Illness\", \"Illness\", \"Immediate\", \"Immediate\", \"Immune_Response\", \"Immunity\", \"Immunity\", \"Immunomodulatory\", \"Important\", \"Important\", \"Infants\", \"Infants\", \"Infection\", \"Infection\", \"Infection\", \"Infection\", \"Infection\", \"Infection\", \"Infection\", \"Infection\", \"Infection\", \"Infection\", \"Infections\", \"Infections\", \"Infections\", \"Infections\", \"Infections\", \"Infections\", \"Infections\", \"Influenza\", \"Influenza\", \"Influenza\", \"Influenza\", \"Influenza\", \"Influenza\", \"Influenza\", \"Influenza\", \"Influenza_Virus\", \"Influenza_Virus\", \"Influenza_Virus\", \"Influenza_Virus\", \"Influenza_Virus\", \"Influenza_Virus\", \"Informatics\", \"Inhibition\", \"Inhibition\", \"Inhibition\", \"Inhibition\", \"Inhibition\", \"Inhibition\", \"Inhibition\", \"Inhibitors\", \"Inhibitors\", \"Inhibitors\", \"Innate_Immune\", \"Insights\", \"Insights\", \"Insights\", \"Insights\", \"Insights\", \"Intestinal\", \"Intracellular\", \"Intracellular\", \"Involved\", \"Involved\", \"Issue\", \"Kaposi_sarcoma\", \"Kawasaki_Disease\", \"Know\", \"Knowledge\", \"L\", \"L\", \"L\", \"Laboratory\", \"Lactobacillus\", \"Lambda\", \"Lectin\", \"Lectin\", \"Live_Attenuated\", \"Localization\", \"MACROPHAGES\", \"MERS\", \"MERS\", \"MERS\", \"MERS_Coronavirus\", \"MERS_Coronavirus\", \"MERS_Coronavirus\", \"MERS_Coronavirus\", \"Mammalian\", \"Mammalian\", \"Manifestations\", \"Mannose\", \"Meta\", \"Microbiological\", \"Middle_East\", \"Middle_East\", \"Middle_East\", \"Middle_East\", \"Middle_East\", \"Middle_East\", \"Middle_East\", \"Modelling\", \"Mouse_Hepatitis\", \"Multiple\", \"Multiple\", \"Multiple\", \"Multiple\", \"Multiple_Sclerosis\", \"Murine\", \"Murine\", \"Murine\", \"NL63\", \"Natural\", \"Natural\", \"Natural\", \"Natural\", \"Necessary\", \"Non_-\", \"Non_-\", \"Non_-\", \"Non_-\", \"Non_-\", \"Non_-\", \"Novel\", \"Novel\", \"Novel\", \"Novel\", \"Novel\", \"Novel\", \"Novel\", \"Novel\", \"Nuclear\", \"Nucleotide\", \"OC43\", \"OC43\", \"One_Health\", \"Original_Article\", \"Outcomes\", \"Outcomes\", \"PCR\", \"PCR\", \"PCR\", \"PCR\", \"PCR\", \"PCR\", \"PCR\", \"PCR_assay\", \"PUBLIC\", \"Pandemic\", \"Pandemic\", \"Parasitic\", \"Patient\", \"Patterns\", \"Patterns\", \"Patterns\", \"Peptide\", \"Peptide\", \"Peptide\", \"Peptide\", \"Peptide\", \"Phylogenetic\", \"Pigs\", \"Plant\", \"Plasma\", \"Plasma\", \"Porcine_Epidemic\", \"Porcine_Epidemic\", \"Porcine_Epidemic\", \"Potent\", \"Potent\", \"Prevention\", \"Protection\", \"Protection\", \"Protection\", \"Protein\", \"Protein\", \"Protein\", \"Protein\", \"Protein\", \"Protein\", \"Protein\", \"Protein\", \"Protein\", \"Protein\", \"REVIEW\", \"RNA\", \"RNA\", \"RNA\", \"RNA\", \"RNA\", \"RNA\", \"RNA\", \"RNA\", \"RNA\", \"RNA\", \"Rapid\", \"Rapid\", \"Rapid\", \"Rapid\", \"Rational_Design\", \"Receptor\", \"Receptor\", \"Receptor\", \"Receptor\", \"Regions\", \"Regulates\", \"Report\", \"Report\", \"Report\", \"Reported\", \"Reported\", \"Reporting\", \"Required\", \"Required\", \"Required\", \"Required\", \"Research_Article\", \"Residues\", \"Respiratory_Infections\", \"Respiratory_Infections\", \"Respiratory_Syncytial\", \"Respiratory_Syncytial\", \"Respiratory_Syncytial\", \"Respiratory_Syncytial\", \"Respiratory_Syndrome\", \"Respiratory_Syndrome\", \"Respiratory_Syndrome\", \"Respiratory_Syndrome\", \"Respiratory_Syndrome\", \"Respiratory_Syndrome\", \"Respiratory_Syndrome\", \"Respiratory_Syndrome\", \"Response\", \"Response\", \"Response\", \"Response\", \"Response\", \"Response\", \"Rhinovirus\", \"S\", \"S\", \"S\", \"S2\", \"SARS\", \"SARS\", \"SARS\", \"SARS\", \"SARS\", \"SARS\", \"SARS\", \"SARS\", \"SARS\", \"Sense\", \"Sequence\", \"Sequence\", \"Sequence\", \"Sequence\", \"Sequence\", \"Severe_Acute\", \"Severe_Acute\", \"Severe_Acute\", \"Simultaneous\", \"Singapore\", \"Singapore\", \"Singapore\", \"Small\", \"Small\", \"Small\", \"Species\", \"Species\", \"Species\", \"Species\", \"Species\", \"Specific\", \"Specific\", \"Specific\", \"Specific\", \"Specific\", \"Specific\", \"Specific\", \"Specific\", \"Spread\", \"Stability\", \"Stability\", \"Strategies\", \"Structure\", \"Structure\", \"Structure\", \"Structure\", \"Study\", \"Study\", \"Study\", \"Study\", \"Study\", \"Study\", \"Sudan\", \"Sufficient\", \"Supplemental\", \"Supplementary\", \"Supplementary\", \"Supplementary\", \"Survey\", \"Swine\", \"Systematic_Review\", \"Systematic_Review\", \"Systemic\", \"Systemic\", \"T_cells\", \"T_cells\", \"T_cells\", \"Taiwan\", \"Taiwan\", \"Taiwan\", \"Taiwan\", \"Taiwan\", \"Testing\", \"Therapy\", \"Time\", \"Tissue\", \"Translation\", \"Translational\", \"Transplantation\", \"Transplantation\", \"Type\", \"Types\", \"Types\", \"United_States\", \"Use\", \"Use\", \"Use\", \"Use\", \"Use\", \"VIRUS\", \"VIRUS\", \"Vaccine\", \"Vaccine\", \"Vaccine\", \"Vaccine\", \"Vaccine\", \"Vaccine\", \"Vaccine\", \"Vaccines\", \"Vaccines\", \"Vaccines\", \"Vaccines\", \"Variability\", \"Vietnam\", \"Vietnam\", \"Vietnam\", \"Vietnam\", \"Vietnam\", \"Viral\", \"Viral\", \"Viral\", \"Viral\", \"Viral\", \"Viral\", \"Viral\", \"Viral\", \"Viral\", \"Viral\", \"Virology_Journal\", \"Virology_Journal\", \"Virology_Journal\", \"Virology_Journal\", \"Virulence\", \"Virulence\", \"Virus\", \"Virus\", \"Virus\", \"Virus\", \"Virus\", \"Virus\", \"Virus\", \"Virus\", \"Virus\", \"Virus\", \"Virus_Infection\", \"Virus_Infection\", \"Virus_Infection\", \"Virus_Infection\", \"Virus_Infection\", \"Viruses\", \"Viruses\", \"Viruses\", \"Viruses\", \"Viruses\", \"Viruses\", \"Viruses\", \"Viruses\", \"Vivo\", \"Vivo\", \"Vivo\", \"Vivo\", \"Wild\", \"Wuhan_China\", \"Wuhan_China\", \"Wuhan_China\", \"Yeast\", \"Zika_Virus\", \"Zoonotic\", \"accessory\", \"activator\", \"adaptation\", \"adaptation\", \"adaptation\", \"adipose\", \"administered\", \"adults\", \"affects\", \"air\", \"air\", \"air\", \"algorithm\", \"algorithm\", \"algorithm\", \"aminopeptidase\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"anti_-\", \"anti_-\", \"anti_-\", \"anti_-\", \"anti_-\", \"anti_-\", \"antigen\", \"antigen\", \"antigen\", \"antigen\", \"antigen\", \"antiviral\", \"antiviral\", \"antiviral\", \"antiviral\", \"antiviral\", \"antiviral\", \"apoptosis\", \"associated\", \"associated\", \"associated\", \"associated\", \"associated\", \"associated\", \"associated\", \"associated\", \"associated\", \"associated_herpesvirus\", \"asthma\", \"behaviors\", \"bias\", \"borne\", \"bovine_respiratory\", \"cancer\", \"cancer\", \"cancer\", \"cancer\", \"cancer\", \"canine\", \"care\", \"cathepsin\", \"cells\", \"cells\", \"cells\", \"cells\", \"cells\", \"cells\", \"cells\", \"cells\", \"cells\", \"cells\", \"chain\", \"characterisation\", \"characterisation\", \"characterisation\", \"chemical\", \"clustering\", \"clusters\", \"clusters\", \"community_based\", \"community_based\", \"community_based\", \"comparison\", \"contact\", \"containing\", \"coronavirus\", \"coronavirus\", \"coronavirus\", \"coronavirus\", \"coronavirus\", \"coronavirus\", \"coronavirus\", \"coronavirus\", \"coronavirus\", \"correlates\", \"cross_-\", \"cross_-\", \"cross_-\", \"cross_-\", \"cross_-\", \"cysteine\", \"cytokine\", \"decision_making\", \"degradation\", \"derivatives\", \"derived\", \"derived\", \"derived\", \"derived\", \"descriptive_study\", \"determinants\", \"development\", \"development\", \"development\", \"development\", \"development\", \"development\", \"development\", \"development\", \"development\", \"diagnosis\", \"diagnosis\", \"diagnosis\", \"diagnosis\", \"diagnosis\", \"diagnosis\", \"diarrheic\", \"disaster\", \"disease\", \"disease\", \"disease\", \"disease\", \"disease\", \"disease\", \"disease\", \"disease\", \"disease\", \"disease\", \"diseases\", \"diseases\", \"diseases\", \"diseases\", \"diseases\", \"diseases\", \"diseases\", \"diseases\", \"diseases\", \"disinfection\", \"dogs\", \"dynamics\", \"dynamics\", \"dynamics\", \"dynamics\", \"dynamics\", \"dynamics\", \"e\", \"efficiency\", \"elements\", \"emergence\", \"emergence\", \"emerging\", \"encephalitis_virus\", \"enhancer\", \"enhances\", \"enhances\", \"epidemiological\", \"epidemiological\", \"epidemiological\", \"epidemiological\", \"epithelial_cells\", \"equipment\", \"evidence\", \"evolution\", \"evolution\", \"evolution\", \"evolution\", \"evolution\", \"exosomes\", \"experimental\", \"experimental_infection\", \"expressed\", \"expression\", \"expression\", \"expression\", \"expression\", \"expression\", \"expression\", \"expression\", \"farms\", \"farms\", \"feline_coronavirus\", \"feline_coronavirus\", \"feline_coronavirus\", \"feline_coronavirus\", \"ferrets\", \"field\", \"field\", \"findings\", \"findings\", \"findings\", \"findings\", \"follow_up\", \"following\", \"following\", \"frame\", \"frame\", \"framework\", \"future\", \"gene\", \"gene\", \"gene\", \"gene\", \"gene\", \"genomes\", \"genomic\", \"genus\", \"genus\", \"genus\", \"health\", \"health\", \"health\", \"health\", \"health\", \"health\", \"health\", \"health\", \"health\", \"health_care\", \"health_care\", \"hierarchical\", \"host\", \"host\", \"host\", \"host\", \"host\", \"host\", \"human\", \"human\", \"human\", \"human\", \"human\", \"human\", \"human\", \"human\", \"human\", \"human\", \"hybridization\", \"immune_response\", \"immune_response\", \"immune_response\", \"immune_responses\", \"immune_responses\", \"immune_responses\", \"immune_responses\", \"impact\", \"impact\", \"impact\", \"impact\", \"impact\", \"impact\", \"impact\", \"in\", \"including\", \"induce\", \"induce\", \"induce\", \"infection\", \"infection\", \"infection\", \"infection\", \"infection\", \"infection\", \"infection\", \"infection\", \"infection\", \"infection\", \"infections\", \"infections\", \"infections\", \"infections\", \"infections\", \"infections\", \"infections\", \"infections\", \"infectious\", \"infectious\", \"infectious\", \"infectious\", \"infectious\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"inhibitor\", \"inhibitors\", \"inhibitors\", \"inhibitors\", \"inhibitors\", \"inhibitors\", \"inhibitors\", \"inhibitors\", \"inhibitory\", \"inhibits\", \"inhibits\", \"inhibits\", \"insights\", \"insights\", \"invasive\", \"isolation\", \"isothermal_amplification\", \"isothermal_amplification\", \"kidney\", \"long\", \"long\", \"long\", \"loop_mediated\", \"loop_mediated\", \"male\", \"male\", \"mammalian\", \"management\", \"management\", \"management\", \"management\", \"management\", \"management\", \"marine\", \"material\", \"measures\", \"mechanism\", \"mechanism\", \"mesenchymal_stem\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"modulate\", \"molecules\", \"molecules\", \"molecules\", \"molecules\", \"molecules\", \"molecules\", \"molecules\", \"monoclonal_antibodies\", \"monoclonal_antibodies\", \"morpholino_oligomers\", \"multi_-\", \"multi_-\", \"multiple\", \"multiple\", \"multiple\", \"multiple\", \"multiple\", \"multiple\", \"multiple\", \"multiplex_real\", \"multiplex_real\", \"neonatal_calves\", \"neonatal_calves\", \"neutropenic\", \"nonlinear\", \"novel\", \"novel\", \"novel\", \"novel\", \"novel\", \"novel\", \"novel\", \"novel\", \"novel\", \"nuclear\", \"nuclear\", \"nuclear\", \"nuclear\", \"nucleocapsid\", \"open_reading\", \"open_reading\", \"outbreak\", \"outbreak\", \"outbreak\", \"outbreak\", \"outbreak\", \"outbreak\", \"outcome\", \"outcomes\", \"pandemic\", \"pandemic\", \"pandemic\", \"pandemic\", \"pandemic\", \"paper\", \"patients\", \"patients\", \"patients\", \"patients\", \"patients\", \"patients\", \"patients\", \"patients\", \"pigs\", \"pigs\", \"pigs\", \"pigs\", \"pigs\", \"plant\", \"platform\", \"pneumonia\", \"pneumonia\", \"pneumonia\", \"pneumonia\", \"pneumonia\", \"pneumonia\", \"pneumonia\", \"pneumonia\", \"porcine_epidemic\", \"porcine_epidemic\", \"porcine_epidemic\", \"porcine_epidemic\", \"porcine_reproductive\", \"porcine_reproductive\", \"porcine_reproductive\", \"porcine_reproductive\", \"porcine_reproductive\", \"porcine_reproductive\", \"potential\", \"potential\", \"potential\", \"potential\", \"potential\", \"potential\", \"potential\", \"potential\", \"potential\", \"practices\", \"predictors\", \"pregnant_women\", \"pregnant_women\", \"prevention\", \"probes_detection\", \"prognosis\", \"prophylactic\", \"prospective\", \"protease\", \"protease\", \"protease\", \"protein\", \"protein\", \"protein\", \"protein\", \"protein\", \"protein\", \"protein\", \"protein\", \"protein\", \"pseudorabies_virus\", \"public_health\", \"public_health\", \"public_health\", \"public_health\", \"public_health\", \"quantitation\", \"quarantine\", \"rate\", \"rate\", \"rate\", \"rate\", \"recognition\", \"regulates\", \"relationship\", \"replication\", \"replication\", \"replication\", \"replication\", \"replication\", \"replication\", \"replication\", \"replication\", \"required\", \"respiratory\", \"respiratory\", \"respiratory\", \"respiratory\", \"respiratory\", \"respiratory\", \"respiratory\", \"respiratory_syncytial\", \"respiratory_syncytial\", \"respiratory_syncytial\", \"respiratory_syncytial\", \"respiratory_syncytial\", \"respiratory_syndrome\", \"respiratory_syndrome\", \"respiratory_syndrome\", \"respiratory_syndrome\", \"respiratory_syndrome\", \"respiratory_syndrome\", \"respiratory_syndrome\", \"respiratory_syndrome\", \"respiratory_viruses\", \"respiratory_viruses\", \"respiratory_viruses\", \"respiratory_viruses\", \"respiratory_viruses\", \"response\", \"response\", \"response\", \"response\", \"response\", \"response\", \"response\", \"reverse_transcription\", \"reverse_transcription\", \"review_comes\", \"role\", \"role\", \"role\", \"role\", \"role\", \"role\", \"roles\", \"salmon\", \"sectional_study\", \"sectional_study\", \"seeking\", \"sequences\", \"sequences\", \"sequences\", \"sequences\", \"sequences\", \"sequencing\", \"sequencing\", \"serum\", \"serum\", \"serum\", \"serum\", \"severe_acute\", \"severe_acute\", \"severe_acute\", \"severe_acute\", \"severe_acute\", \"severe_acute\", \"silico\", \"social\", \"specific\", \"specific\", \"specific\", \"specific\", \"specific\", \"specific\", \"specific\", \"specific\", \"specific\", \"stimulated\", \"strains\", \"strains\", \"strains\", \"strains\", \"strains\", \"strains\", \"strains\", \"strains\", \"sub_-\", \"sub_-\", \"surveillance\", \"surveillance\", \"surveillance\", \"surveillance\", \"surveillance\", \"surveillance\", \"targeting\", \"targeting\", \"themed_issue\", \"three\", \"three\", \"three\", \"three\", \"three\", \"three\", \"tick\", \"time_RT\", \"transcription\", \"transcription\", \"transmissible_gastroenteritis\", \"transmissible_gastroenteritis\", \"transmissible_gastroenteritis\", \"transmission\", \"transmission\", \"transmission\", \"transmission\", \"transmission\", \"transmission\", \"transmission\", \"transmission\", \"trends\", \"trial\", \"trial\", \"type\", \"vaccination\", \"vaccination\", \"vaccination\", \"vaccine_development\", \"viral_infections\", \"viral_infections\", \"virus\", \"virus\", \"virus\", \"virus\", \"virus\", \"virus\", \"virus\", \"virus\", \"virus\", \"virus\", \"virus_infection\", \"virus_infection\", \"virus_infection\", \"virus_infection\", \"virus_infection\", \"virus_infection\", \"viruses\", \"viruses\", \"viruses\", \"viruses\", \"viruses\", \"viruses\", \"viruses\", \"viruses\", \"viruses\", \"water\", \"workers\", \"zoonotic\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [3, 6, 4, 9, 5, 8, 7, 2, 10, 1]};\n\nfunction LDAvis_load_lib(url, callback){\n  var s = document.createElement('script');\n  s.src = url;\n  s.async = true;\n  s.onreadystatechange = s.onload = callback;\n  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n  document.getElementsByTagName(\"head\")[0].appendChild(s);\n}\n\nif(typeof(LDAvis) !== \"undefined\"){\n   // already loaded: just create the visualization\n   !function(LDAvis){\n       new LDAvis(\"#\" + \"ldavis_el908361404542728321124526208110\", ldavis_el908361404542728321124526208110_data);\n   }(LDAvis);\n}else if(typeof define === \"function\" && define.amd){\n   // require.js is available: use it to load d3/LDAvis\n   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n   require([\"d3\"], function(d3){\n      window.d3 = d3;\n      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n        new LDAvis(\"#\" + \"ldavis_el908361404542728321124526208110\", ldavis_el908361404542728321124526208110_data);\n      });\n    });\n}else{\n    // require.js not available: dynamically load d3 & LDAvis\n    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n                 new LDAvis(\"#\" + \"ldavis_el908361404542728321124526208110\", ldavis_el908361404542728321124526208110_data);\n            })\n         });\n}\n</script>"
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)"
   ]
  },
  {
   "source": [
    "# Topic Modelling overall"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   Unnamed: 0                                  paper_id  \\\n",
       "0           0  0001418189999fea7f7cbe3e82703d71c85a6fe5   \n",
       "1           1  000affa746a03f1fe4e3b3ef1a62fdfa9b9ac52a   \n",
       "2           2  000e754142ba65ef77c6fdffcbcbe824e141ea7b   \n",
       "3           3  000eec3f1e93c3792454ac59415c928ce3a6b4ad   \n",
       "4           4  001259ae6d9bfa9376894f61aa6b6c5f18be2177   \n",
       "\n",
       "                                               title  \\\n",
       "0  Absence of surface expression of feline infect...   \n",
       "1  Correlation between antimicrobial consumption ...   \n",
       "2  Laboratory-based surveillance of hospital-acqu...   \n",
       "3  Pneumonie virale sévère de l'immunocompétent V...   \n",
       "4  Microheterogeneity of S-glycoprotein of mouse ...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  Feline infectious peritonitis virus (FIPV) pos...   \n",
       "1  Objectives: This study was conducted to invest...   \n",
       "2  Of 7,772 laboratory-confirmed cases of respira...   \n",
       "3  Reçu et accepté le 7 février 2004 Les infectio...   \n",
       "4  IEF, isoelectric focusing; NC, nitrocellulose;...   \n",
       "\n",
       "                                           body_text  \\\n",
       "0  Feline infectious peritonitis (FIP) is a fatal...   \n",
       "1  The incidence of health-care-associated infect...   \n",
       "2  The human respiratory viruses include adenovir...   \n",
       "3  Les pathologies infectieuses respiratoires son...   \n",
       "4  (Accepted 10 January 1992) is a neurotropic co...   \n",
       "\n",
       "                                           doi  \\\n",
       "0  http://doi.org/10.1016/j.vetmic.2006.11.026   \n",
       "1    http://doi.org/10.1016/j.jmii.2013.10.008   \n",
       "2    http://doi.org/10.1016/j.ajic.2017.01.009   \n",
       "3  http://doi.org/10.1016/j.reaurg.2004.02.009   \n",
       "4  http://doi.org/10.1016/0166-0934(92)90173-B   \n",
       "\n",
       "                                 title_abstract_body  \n",
       "0  Absence of surface expression of feline infect...  \n",
       "1  Correlation between antimicrobial consumption ...  \n",
       "2  Laboratory-based surveillance of hospital-acqu...  \n",
       "3  Pneumonie virale sévère de l'immunocompétent V...  \n",
       "4  Microheterogeneity of S-glycoprotein of mouse ...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>paper_id</th>\n      <th>title</th>\n      <th>abstract</th>\n      <th>body_text</th>\n      <th>doi</th>\n      <th>title_abstract_body</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0001418189999fea7f7cbe3e82703d71c85a6fe5</td>\n      <td>Absence of surface expression of feline infect...</td>\n      <td>Feline infectious peritonitis virus (FIPV) pos...</td>\n      <td>Feline infectious peritonitis (FIP) is a fatal...</td>\n      <td>http://doi.org/10.1016/j.vetmic.2006.11.026</td>\n      <td>Absence of surface expression of feline infect...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>000affa746a03f1fe4e3b3ef1a62fdfa9b9ac52a</td>\n      <td>Correlation between antimicrobial consumption ...</td>\n      <td>Objectives: This study was conducted to invest...</td>\n      <td>The incidence of health-care-associated infect...</td>\n      <td>http://doi.org/10.1016/j.jmii.2013.10.008</td>\n      <td>Correlation between antimicrobial consumption ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>000e754142ba65ef77c6fdffcbcbe824e141ea7b</td>\n      <td>Laboratory-based surveillance of hospital-acqu...</td>\n      <td>Of 7,772 laboratory-confirmed cases of respira...</td>\n      <td>The human respiratory viruses include adenovir...</td>\n      <td>http://doi.org/10.1016/j.ajic.2017.01.009</td>\n      <td>Laboratory-based surveillance of hospital-acqu...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>000eec3f1e93c3792454ac59415c928ce3a6b4ad</td>\n      <td>Pneumonie virale sévère de l'immunocompétent V...</td>\n      <td>Reçu et accepté le 7 février 2004 Les infectio...</td>\n      <td>Les pathologies infectieuses respiratoires son...</td>\n      <td>http://doi.org/10.1016/j.reaurg.2004.02.009</td>\n      <td>Pneumonie virale sévère de l'immunocompétent V...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>001259ae6d9bfa9376894f61aa6b6c5f18be2177</td>\n      <td>Microheterogeneity of S-glycoprotein of mouse ...</td>\n      <td>IEF, isoelectric focusing; NC, nitrocellulose;...</td>\n      <td>(Accepted 10 January 1992) is a neurotropic co...</td>\n      <td>http://doi.org/10.1016/0166-0934(92)90173-B</td>\n      <td>Microheterogeneity of S-glycoprotein of mouse ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "my_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "health_data = my_df[[\"paper_id\",\"title\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_topics = 3 #@param {type:\"integer\"}\n",
    "\n",
    "no_top_words = 4 #@param {type:\"integer\"}\n",
    "\n",
    "no_top_documents = 3 #@param {type:\"integer\"}\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = health_data.title.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(H, W, feature_names, documents, no_top_words, no_top_documents):\n",
    "    for topic_idx, topic in enumerate(H):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "        top_doc_indices = np.argsort( W[:,topic_idx] )[::-1][0:no_top_documents]\n",
    "        for doc_index in top_doc_indices:\n",
    "            print(documents[doc_index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NMF is able to use tf-idf\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(titles)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run NMF\n",
    "nmf_model = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tfidf)\n",
    "nmf_W = nmf_model.transform(tfidf)\n",
    "nmf_H = nmf_model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "NMF Topics\nTopic 0:\nvirus infection porcine influenza\nTransmissible Gastroenteritis Virus of Pigs and Porcine Epidemic Diarrhea Virus\nVirus-Vectored Influenza Virus Vaccines\nMultiplex real-time RT-PCR for the simultaneous detection and quantification of transmissible gastroenteritis virus and porcine epidemic diarrhea virus\nTopic 1:\nrespiratory syndrome acute infections\nSevere Acute Respiratory Syndrome-associated Coronavirus Infection\nComparative Epidemiology of Human Infections with Middle East Respiratory Syndrome and Severe Acute Respiratory Syndrome Coronaviruses among Healthcare Personnel\nMiddle East respiratory syndrome\nTopic 2:\ncoronavirus sars protein cov\nPeptide Mimicrying Between SARS Coronavirus Spike Protein and Human Proteins Reacts with SARS Patient Serum\nCharacterization of protein-protein interactions between the nucleocapsid protein and membrane protein of the SARS coronavirus\nAntibody responses against SARS-coronavirus and its nucleocaspid in SARS patients\n--------------\n"
     ]
    }
   ],
   "source": [
    "print(\"NMF Topics\")\n",
    "display_topics(nmf_H, nmf_W, tfidf_feature_names, titles, no_top_words, no_top_documents)\n",
    "print(\"--------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "LDA Topics\nTopic 0:\nprotein coronavirus virus sars\nInterferon-induced HERC5 is evolving under positive selection and inhibits HIV-1 particle production by a novel mechanism targeting Rev/RRE-dependent RNA nuclear export Interferon-induced HERC5 is evolving under positive selection and inhibits HIV-1 particle production by a novel mechanism targeting Rev/RRE-dependent RNA nuclear export\nProteome and phosphoproteome analysis of honeybee (Apis mellifera) venom collected from electrical stimulation and manual extraction of the venom gland Proteome and phosphoproteome analysis of honeybee (Apis mellifera) venom collected from electrical stimulation and manual extraction of the venom gland\nFrom the Similarity Analysis of Protein Cavities to the Functional Classification of Protein Families Using Cavbase Keywords: protein binding pockets; classification of protein binding pockets; cluster analysis of protein binding pockets; protein kinases; SARS protease\nTopic 1:\nrespiratory health study disease\nAntivirals for influenza-like illness? Protocol for a randomized controlled trial of clinical and cost effectiveness in primary care (ALIC 4 E) Antivirals for influenza-Like Illness? A randomized Controlled trial of Clinical and Cost effectiveness in primary CarE (ALIC4E): The ALIC4E Protocol Antivirals for influenza-Like Illness? A randomized Controlled trial of Clinical and Cost effectiveness in primary CarE (ALIC 4 E): The ALIC 4 E Protocol Antivirals for influenza-Like Illness? A randomized Controlled trial of Clinical and Cost effectiveness in primary CarE (ALIC4E): The ALIC4E Protocol Antivirals for influenza-Like Illness? A randomized Controlled trial of Clinical and Cost effectiveness in primary CarE (ALIC 4 E): The ALIC 4 E Protocol\nNICEATM-ICCVAM # International Workshop on Alternative Methods to Reduce, Refine, and Replace the Use of Animals in Vaccine Potency and Safety Testing: Alternative methods to reduce, refine, and replace the use of animals in the development and testing of veterinary biologics in the United States; a strategic priority peer-review under responsibility of the National Toxicology Program Interagency Center for the Evaluation of Alternative Toxicological Methods (NICEATM). peer-review under responsibility of the National Toxicology Program Interagency Center for the Evaluation of Alternative Toxicological Methods (NICEATM)\nNICEATM-ICCVAM # International Workshop on Alternative Methods to Reduce, Refine, and Replace the Use of Animals in Vaccine Potency and Safety Testing: Non-animal replacement methods for veterinary vaccine potency testing: state of the science and future directions peer-review under responsibility of the National Toxicology Program Interagency Center for the Evaluation of Alternative Toxicological Methods (NICEATM). peer-review under responsibility of the National Toxicology Program Interagency Center for the Evaluation of Alternative Toxicological Methods (NICEATM)\nTopic 2:\nvirus human infection viruses\nEvaluation of NxTAG Respiratory Pathogen Panel and Comparison with xTAG Respiratory Viral Panel Fast v2 and Film Array Respiratory Panel for Detecting Respiratory Pathogens in Nasopharyngeal Aspirates and Swine/Avian-Origin Influenza A Subtypes in Culture Isolates\nA Single Injection of Human Neutralizing Antibody Protects against Zika Virus Infection and Microcephaly in Developing Mouse Embryos A Single Injection of Human Neutralizing Antibody Protects against Zika Virus Infection and Microcephaly in Developing Mouse Embryos\nRecently Discovered Human Coronaviruses Coronaviruses Human coronavirus 229E Human coronavirus NL63 Human coronavirus OC43 Human coronavirus HKU1 Severe acute respiratory syndrome-associated coronavirus Recently Discovered Human Coronaviruses\n"
     ]
    }
   ],
   "source": [
    "# LDA can only use raw term counts for LDA because it is a probabilistic graphical model\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(titles)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "# Run LDA\n",
    "lda_model = LatentDirichletAllocation(n_components=no_topics, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(tf)\n",
    "lda_W = lda_model.transform(tf)\n",
    "lda_H = lda_model.components_\n",
    "\n",
    "print(\"LDA Topics\")\n",
    "display_topics(lda_H, lda_W, tf_feature_names, titles, no_top_words, no_top_documents)"
   ]
  },
  {
   "source": [
    "# Zero-shot topic modelling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\n",
    "    Source code: https://github.com/Bjarten/early-stopping-pytorch \"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        if path is None:\n",
    "            self.path = 'checkpoint.pt'\n",
    "        else:\n",
    "            self.path = path\n",
    "        self.trace_func = trace_func\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        \"\"\"Saves model when validation loss decrease.\"\"\"\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "\n",
    "        model.save(self.path)\n",
    "\n",
    "        self.val_loss_min = val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"PyTorch class for feed foward inference network.\"\"\"\n",
    "\n",
    "from collections import OrderedDict\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "\n",
    "class ContextualInferenceNetwork(nn.Module):\n",
    "\n",
    "    \"\"\"Inference Network.\"\"\"\n",
    "\n",
    "    def __init__(self, input_size, bert_size, output_size, hidden_sizes,\n",
    "                 activation='softplus', dropout=0.2):\n",
    "        \"\"\"\n",
    "        Initialize InferenceNetwork.\n",
    "        Args\n",
    "            input_size : int, dimension of input\n",
    "            output_size : int, dimension of output\n",
    "            hidden_sizes : tuple, length = n_layers\n",
    "            activation : string, 'softplus' or 'relu', default 'softplus'\n",
    "            dropout : float, default 0.2, default 0.2\n",
    "        \"\"\"\n",
    "        super(ContextualInferenceNetwork, self).__init__()\n",
    "        assert isinstance(input_size, int), \"input_size must by type int.\"\n",
    "        assert isinstance(output_size, int), \"output_size must be type int.\"\n",
    "        assert isinstance(hidden_sizes, tuple), \\\n",
    "            \"hidden_sizes must be type tuple.\"\n",
    "        assert activation in ['softplus', 'relu'], \\\n",
    "            \"activation must be 'softplus' or 'relu'.\"\n",
    "        assert dropout >= 0, \"dropout must be >= 0.\"\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.dropout = dropout\n",
    "\n",
    "        if activation == 'softplus':\n",
    "            self.activation = nn.Softplus()\n",
    "        elif activation == 'relu':\n",
    "            self.activation = nn.ReLU()\n",
    "\n",
    "        self.input_layer = nn.Linear(input_size+input_size, hidden_sizes[0])\n",
    "        self.adapt_bert = nn.Linear(bert_size, hidden_sizes[0])\n",
    "\n",
    "        self.hiddens = nn.Sequential(OrderedDict([\n",
    "            ('l_{}'.format(i), nn.Sequential(nn.Linear(h_in, h_out), self.activation))\n",
    "            for i, (h_in, h_out) in enumerate(zip(hidden_sizes[:-1], hidden_sizes[1:]))]))\n",
    "\n",
    "        self.f_mu = nn.Linear(hidden_sizes[-1], output_size)\n",
    "        self.f_mu_batchnorm = nn.BatchNorm1d(output_size, affine=False)\n",
    "\n",
    "        self.f_sigma = nn.Linear(hidden_sizes[-1], output_size)\n",
    "        self.f_sigma_batchnorm = nn.BatchNorm1d(output_size, affine=False)\n",
    "\n",
    "        self.dropout_enc = nn.Dropout(p=self.dropout)\n",
    "\n",
    "    def forward(self, x, x_bert):\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        x_bert = self.adapt_bert(x_bert)\n",
    "\n",
    "        x = self.activation(x_bert)\n",
    "        x = self.hiddens(x)\n",
    "        x = self.dropout_enc(x)\n",
    "        mu = self.f_mu_batchnorm(self.f_mu(x))\n",
    "        log_sigma = self.f_sigma_batchnorm(self.f_sigma(x))\n",
    "\n",
    "        return mu, log_sigma\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CombinedInferenceNetwork(nn.Module):\n",
    "\n",
    "    \"\"\"Inference Network.\"\"\"\n",
    "\n",
    "    def __init__(self, input_size, bert_size, output_size, hidden_sizes,\n",
    "                 activation='softplus', dropout=0.2):\n",
    "        \"\"\"\n",
    "        Initialize InferenceNetwork.\n",
    "        Args\n",
    "            input_size : int, dimension of input\n",
    "            output_size : int, dimension of output\n",
    "            hidden_sizes : tuple, length = n_layers\n",
    "            activation : string, 'softplus' or 'relu', default 'softplus'\n",
    "            dropout : float, default 0.2, default 0.2\n",
    "        \"\"\"\n",
    "        super(CombinedInferenceNetwork, self).__init__()\n",
    "        assert isinstance(input_size, int), \"input_size must by type int.\"\n",
    "        assert isinstance(output_size, int), \"output_size must be type int.\"\n",
    "        assert isinstance(hidden_sizes, tuple), \\\n",
    "            \"hidden_sizes must be type tuple.\"\n",
    "        assert activation in ['softplus', 'relu'], \\\n",
    "            \"activation must be 'softplus' or 'relu'.\"\n",
    "        assert dropout >= 0, \"dropout must be >= 0.\"\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.dropout = dropout\n",
    "\n",
    "        if activation == 'softplus':\n",
    "            self.activation = nn.Softplus()\n",
    "        elif activation == 'relu':\n",
    "            self.activation = nn.ReLU()\n",
    "\n",
    "        self.input_layer = nn.Linear(input_size+input_size, hidden_sizes[0])\n",
    "        self.adapt_bert = nn.Linear(bert_size, input_size)\n",
    "        self.bert_layer = nn.Linear(hidden_sizes[0], hidden_sizes[0])\n",
    "\n",
    "        self.hiddens = nn.Sequential(OrderedDict([\n",
    "            ('l_{}'.format(i), nn.Sequential(nn.Linear(h_in, h_out), self.activation))\n",
    "            for i, (h_in, h_out) in enumerate(zip(hidden_sizes[:-1], hidden_sizes[1:]))]))\n",
    "\n",
    "        self.f_mu = nn.Linear(hidden_sizes[-1], output_size)\n",
    "        self.f_mu_batchnorm = nn.BatchNorm1d(output_size, affine=False)\n",
    "\n",
    "        self.f_sigma = nn.Linear(hidden_sizes[-1], output_size)\n",
    "        self.f_sigma_batchnorm = nn.BatchNorm1d(output_size, affine=False)\n",
    "\n",
    "        self.dropout_enc = nn.Dropout(p=self.dropout)\n",
    "\n",
    "    def forward(self, x, x_bert):\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        x_bert = self.adapt_bert(x_bert)\n",
    "        x = torch.cat((x, x_bert), 1)\n",
    "        x = self.input_layer(x)\n",
    "\n",
    "        x = self.activation(x)\n",
    "        x = self.hiddens(x)\n",
    "        x = self.dropout_enc(x)\n",
    "        mu = self.f_mu_batchnorm(self.f_mu(x))\n",
    "        log_sigma = self.f_sigma_batchnorm(self.f_sigma(x))\n",
    "\n",
    "        return mu, log_sigma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"PyTorch class for feed foward AVITM network.\"\"\"\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class DecoderNetwork(nn.Module):\n",
    "\n",
    "    \"\"\"AVITM Network.\"\"\"\n",
    "\n",
    "    def __init__(self, input_size, bert_size, infnet, n_components=10, model_type='prodLDA',\n",
    "                 hidden_sizes=(100,100), activation='softplus', dropout=0.2,\n",
    "                 learn_priors=True):\n",
    "        \"\"\"\n",
    "        Initialize InferenceNetwork.\n",
    "        Args\n",
    "            input_size : int, dimension of input\n",
    "            n_components : int, number of topic components, (default 10)\n",
    "            model_type : string, 'prodLDA' or 'LDA' (default 'prodLDA')\n",
    "            hidden_sizes : tuple, length = n_layers, (default (100, 100))\n",
    "            activation : string, 'softplus', 'relu', (default 'softplus')\n",
    "            learn_priors : bool, make priors learnable parameter\n",
    "        \"\"\"\n",
    "        super(DecoderNetwork, self).__init__()\n",
    "        assert isinstance(input_size, int), \"input_size must by type int.\"\n",
    "        assert isinstance(n_components, int) and n_components > 0, \\\n",
    "            \"n_components must be type int > 0.\"\n",
    "        assert model_type in ['prodLDA', 'LDA'], \\\n",
    "            \"model type must be 'prodLDA' or 'LDA'\"\n",
    "        assert isinstance(hidden_sizes, tuple), \\\n",
    "            \"hidden_sizes must be type tuple.\"\n",
    "        assert activation in ['softplus', 'relu'], \\\n",
    "            \"activation must be 'softplus' or 'relu'.\"\n",
    "        assert dropout >= 0, \"dropout must be >= 0.\"\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.n_components = n_components\n",
    "        self.model_type = model_type\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.activation = activation\n",
    "        self.dropout = dropout\n",
    "        self.learn_priors = learn_priors\n",
    "        self.topic_word_matrix = None\n",
    "\n",
    "        if infnet == \"zeroshot\":\n",
    "            self.inf_net = ContextualInferenceNetwork(\n",
    "                input_size, bert_size, n_components, hidden_sizes, activation)\n",
    "        elif infnet == \"combined\":\n",
    "            self.inf_net = CombinedInferenceNetwork(\n",
    "                input_size, bert_size, n_components, hidden_sizes, activation)\n",
    "        else:\n",
    "            raise Exception('Missing infnet parameter, options are zeroshot and combined')\n",
    "\n",
    "        # init prior parameters\n",
    "        # \\mu_1k = log \\alpha_k + 1/K \\sum_i log \\alpha_i;\n",
    "        # \\alpha = 1 \\forall \\alpha\n",
    "        topic_prior_mean = 0.0\n",
    "        self.prior_mean = torch.tensor(\n",
    "            [topic_prior_mean] * n_components)\n",
    "        if torch.cuda.is_available():\n",
    "            self.prior_mean = self.prior_mean.cuda()\n",
    "        if self.learn_priors:\n",
    "            self.prior_mean = nn.Parameter(self.prior_mean)\n",
    "\n",
    "        # \\Sigma_1kk = 1 / \\alpha_k (1 - 2/K) + 1/K^2 \\sum_i 1 / \\alpha_k;\n",
    "        # \\alpha = 1 \\forall \\alpha\n",
    "        topic_prior_variance = 1. - (1. / self.n_components)\n",
    "        self.prior_variance = torch.tensor(\n",
    "            [topic_prior_variance] * n_components)\n",
    "        if torch.cuda.is_available():\n",
    "            self.prior_variance = self.prior_variance.cuda()\n",
    "        if self.learn_priors:\n",
    "            self.prior_variance = nn.Parameter(self.prior_variance)\n",
    "\n",
    "        self.beta = torch.Tensor(n_components, input_size)\n",
    "        if torch.cuda.is_available():\n",
    "            self.beta = self.beta.cuda()\n",
    "        self.beta = nn.Parameter(self.beta)\n",
    "        nn.init.xavier_uniform_(self.beta)\n",
    "\n",
    "        self.beta_batchnorm = nn.BatchNorm1d(input_size, affine=False)\n",
    "\n",
    "        # dropout on theta\n",
    "        self.drop_theta = nn.Dropout(p=self.dropout)\n",
    "\n",
    "    @staticmethod\n",
    "    def reparameterize(mu, logvar):\n",
    "        \"\"\"Reparameterize the theta distribution.\"\"\"\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mu)\n",
    "\n",
    "    def forward(self, x, x_bert):\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        # batch_size x n_components\n",
    "        posterior_mu, posterior_log_sigma = self.inf_net(x, x_bert)\n",
    "        posterior_sigma = torch.exp(posterior_log_sigma)\n",
    "\n",
    "        # generate samples from theta\n",
    "        theta = F.softmax(\n",
    "            self.reparameterize(posterior_mu, posterior_log_sigma), dim=1)\n",
    "        theta = self.drop_theta(theta)\n",
    "\n",
    "        # prodLDA vs LDA\n",
    "        if self.model_type == 'prodLDA':\n",
    "            # in: batch_size x input_size x n_components\n",
    "            word_dist = F.softmax(\n",
    "                self.beta_batchnorm(torch.matmul(theta, self.beta)), dim=1)\n",
    "            # word_dist: batch_size x input_size\n",
    "            self.topic_word_matrix = self.beta\n",
    "        elif self.model_type == 'LDA':\n",
    "            # simplex constrain on Beta\n",
    "            beta = F.softmax(self.beta_batchnorm(self.beta), dim=1)\n",
    "            self.topic_word_matrix = beta\n",
    "            word_dist = torch.matmul(theta, beta)\n",
    "            # word_dist: batch_size x input_size\n",
    "\n",
    "        return self.prior_mean, self.prior_variance, \\\n",
    "            posterior_mu, posterior_sigma, posterior_log_sigma, word_dist\n",
    "\n",
    "    def get_theta(self, x, x_bert):\n",
    "        with torch.no_grad():\n",
    "            # batch_size x n_components\n",
    "            posterior_mu, posterior_log_sigma = self.inf_net(x, x_bert)\n",
    "            posterior_sigma = torch.exp(posterior_log_sigma)\n",
    "\n",
    "            # generate samples from theta\n",
    "            theta = F.softmax(\n",
    "                self.reparameterize(posterior_mu, posterior_log_sigma), dim=1)\n",
    "\n",
    "            return theta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import wordcloud\n",
    "from scipy.special import softmax\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class CTM:\n",
    "    \"\"\"Class to train the contextualized topic model. This is the more general class that we are keeping to\n",
    "    avoid braking code, user should use the two subclasses ZeroShotTM and CombinedTm to do topic modeling.\n",
    "        :param input_size: int, dimension of input\n",
    "        :param bert_input_size: int, dimension of input that comes from BERT embeddings\n",
    "        :param inference_type: string, you can choose between the contextual model and the combined model\n",
    "        :param n_components: int, number of topic components, (default 10)\n",
    "        :param model_type: string, 'prodLDA' or 'LDA' (default 'prodLDA')\n",
    "        :param hidden_sizes: tuple, length = n_layers, (default (100, 100))\n",
    "        :param activation: string, 'softplus', 'relu', (default 'softplus')\n",
    "        :param dropout: float, dropout to use (default 0.2)\n",
    "        :param learn_priors: bool, make priors a learnable parameter (default True)\n",
    "        :param batch_size: int, size of batch to use for training (default 64)\n",
    "        :param lr: float, learning rate to use for training (default 2e-3)\n",
    "        :param momentum: float, momentum to use for training (default 0.99)\n",
    "        :param solver: string, optimizer 'adam' or 'sgd' (default 'adam')\n",
    "        :param num_epochs: int, number of epochs to train for, (default 100)\n",
    "        :param reduce_on_plateau: bool, reduce learning rate by 10x on plateau of 10 epochs (default False)\n",
    "        :param num_data_loader_workers: int, number of data loader workers (default cpu_count). set it to 0 if you are using Windows\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, bert_input_size, inference_type=\"combined\", n_components=10, model_type='prodLDA',\n",
    "                 hidden_sizes=(100, 100), activation='softplus', dropout=0.2,\n",
    "                 learn_priors=True, batch_size=64, lr=2e-3, momentum=0.99,\n",
    "                 solver='adam', num_epochs=100, reduce_on_plateau=False, num_data_loader_workers=mp.cpu_count()):\n",
    "        warnings.simplefilter('always', DeprecationWarning)\n",
    "\n",
    "        if self.__class__.__name__ == \"CTM\":\n",
    "            warnings.warn(\n",
    "                \"Direct call to CTM is deprecated and will be removed in version 2, use CombinedTM or ZeroShotTM\",\n",
    "                DeprecationWarning)\n",
    "\n",
    "        assert isinstance(input_size, int) and input_size > 0, \\\n",
    "            \"input_size must by type int > 0.\"\n",
    "        assert isinstance(n_components, int) and input_size > 0, \\\n",
    "            \"n_components must by type int > 0.\"\n",
    "        assert model_type in ['LDA', 'prodLDA'], \\\n",
    "            \"model must be 'LDA' or 'prodLDA'.\"\n",
    "        assert isinstance(hidden_sizes, tuple), \\\n",
    "            \"hidden_sizes must be type tuple.\"\n",
    "        assert activation in ['softplus', 'relu'], \\\n",
    "            \"activation must be 'softplus' or 'relu'.\"\n",
    "        assert dropout >= 0, \"dropout must be >= 0.\"\n",
    "        assert isinstance(learn_priors, bool), \"learn_priors must be boolean.\"\n",
    "        assert isinstance(batch_size, int) and batch_size > 0, \\\n",
    "            \"batch_size must be int > 0.\"\n",
    "        assert lr > 0, \"lr must be > 0.\"\n",
    "        assert isinstance(momentum, float) and 0 < momentum <= 1, \\\n",
    "            \"momentum must be 0 < float <= 1.\"\n",
    "        assert solver in ['adam', 'sgd'], \"solver must be 'adam' or 'sgd'.\"\n",
    "        assert isinstance(reduce_on_plateau, bool), \\\n",
    "            \"reduce_on_plateau must be type bool.\"\n",
    "        assert isinstance(num_data_loader_workers, int) and num_data_loader_workers >= 0, \\\n",
    "            \"num_data_loader_workers must by type int >= 0. set 0 if you are using windows\"\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.n_components = n_components\n",
    "        self.model_type = model_type\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.activation = activation\n",
    "        self.dropout = dropout\n",
    "        self.learn_priors = learn_priors\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.bert_size = bert_input_size\n",
    "        self.momentum = momentum\n",
    "        self.solver = solver\n",
    "        self.num_epochs = num_epochs\n",
    "        self.reduce_on_plateau = reduce_on_plateau\n",
    "        self.num_data_loader_workers = num_data_loader_workers\n",
    "\n",
    "        self.model = DecoderNetwork(\n",
    "            input_size, self.bert_size, inference_type, n_components, model_type, hidden_sizes, activation,\n",
    "            dropout, learn_priors)\n",
    "        self.early_stopping = None\n",
    "\n",
    "        # init optimizer\n",
    "        if self.solver == 'adam':\n",
    "            self.optimizer = optim.Adam(\n",
    "                self.model.parameters(), lr=lr, betas=(self.momentum, 0.99))\n",
    "        elif self.solver == 'sgd':\n",
    "            self.optimizer = optim.SGD(\n",
    "                self.model.parameters(), lr=lr, momentum=self.momentum)\n",
    "\n",
    "        # init lr scheduler\n",
    "        if self.reduce_on_plateau:\n",
    "            self.scheduler = ReduceLROnPlateau(self.optimizer, patience=10)\n",
    "\n",
    "        # performance attributes\n",
    "        self.best_loss_train = float('inf')\n",
    "\n",
    "        # training attributes\n",
    "        self.model_dir = None\n",
    "        self.train_data = None\n",
    "        self.nn_epoch = None\n",
    "\n",
    "        # validation attributes\n",
    "        self.validation_data = None\n",
    "\n",
    "        # learned topics\n",
    "        self.best_components = None\n",
    "\n",
    "        # Use cuda if available\n",
    "        if torch.cuda.is_available():\n",
    "            self.USE_CUDA = True\n",
    "        else:\n",
    "            self.USE_CUDA = False\n",
    "\n",
    "        if self.USE_CUDA:\n",
    "            self.model = self.model.cuda()\n",
    "\n",
    "    def _loss(self, inputs, word_dists, prior_mean, prior_variance,\n",
    "              posterior_mean, posterior_variance, posterior_log_variance):\n",
    "\n",
    "        # KL term\n",
    "        # var division term\n",
    "        var_division = torch.sum(posterior_variance / prior_variance, dim=1)\n",
    "        # diff means term\n",
    "        diff_means = prior_mean - posterior_mean\n",
    "        diff_term = torch.sum(\n",
    "            (diff_means * diff_means) / prior_variance, dim=1)\n",
    "        # logvar det division term\n",
    "        logvar_det_division = \\\n",
    "            prior_variance.log().sum() - posterior_log_variance.sum(dim=1)\n",
    "        # combine terms\n",
    "        KL = 0.5 * (\n",
    "            var_division + diff_term - self.n_components + logvar_det_division)\n",
    "\n",
    "        # Reconstruction term\n",
    "        RL = -torch.sum(inputs * torch.log(word_dists + 1e-10), dim=1)\n",
    "\n",
    "        loss = KL + RL\n",
    "\n",
    "        return loss.sum()\n",
    "\n",
    "    def _train_epoch(self, loader):\n",
    "        \"\"\"Train epoch.\"\"\"\n",
    "        self.model.train()\n",
    "        train_loss = 0\n",
    "        samples_processed = 0\n",
    "\n",
    "        for batch_samples in loader:\n",
    "            # batch_size x vocab_size\n",
    "            X = batch_samples['X']\n",
    "            X = X.reshape(X.shape[0], -1)\n",
    "            X_bert = batch_samples['X_bert']\n",
    "            if self.USE_CUDA:\n",
    "                X = X.cuda()\n",
    "                X_bert = X_bert.cuda()\n",
    "\n",
    "            # forward pass\n",
    "            self.model.zero_grad()\n",
    "            prior_mean, prior_variance, posterior_mean, posterior_variance, posterior_log_variance, word_dists =\\\n",
    "                self.model(X, X_bert)\n",
    "\n",
    "            # backward pass\n",
    "            loss = self._loss(\n",
    "                X, word_dists, prior_mean, prior_variance,\n",
    "                posterior_mean, posterior_variance, posterior_log_variance)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # compute train loss\n",
    "            samples_processed += X.size()[0]\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= samples_processed\n",
    "\n",
    "        return samples_processed, train_loss\n",
    "\n",
    "    def fit(self, train_dataset, validation_dataset=None, save_dir=None, verbose=False, patience=5, delta=0):\n",
    "        \"\"\"\n",
    "        Train the CTM model.\n",
    "        :param train_dataset: PyTorch Dataset class for training data.\n",
    "        :param validation_dataset: PyTorch Dataset class for validation data. If not None, the training stops if\n",
    "        validation loss doesn't improve after a given patience\n",
    "        :param save_dir: directory to save checkpoint models to.\n",
    "        :param verbose: verbose\n",
    "        :param patience: How long to wait after last time validation loss improved. Default: 5\n",
    "        :param delta: Minimum change in the monitored quantity to qualify as an improvement. Default: 0\n",
    "        \"\"\"\n",
    "        # Print settings to output file\n",
    "        if verbose:\n",
    "            print(\"Settings: \\n\\\n",
    "                   N Components: {}\\n\\\n",
    "                   Topic Prior Mean: {}\\n\\\n",
    "                   Topic Prior Variance: {}\\n\\\n",
    "                   Model Type: {}\\n\\\n",
    "                   Hidden Sizes: {}\\n\\\n",
    "                   Activation: {}\\n\\\n",
    "                   Dropout: {}\\n\\\n",
    "                   Learn Priors: {}\\n\\\n",
    "                   Learning Rate: {}\\n\\\n",
    "                   Momentum: {}\\n\\\n",
    "                   Reduce On Plateau: {}\\n\\\n",
    "                   Save Dir: {}\".format(\n",
    "                self.n_components, 0.0,\n",
    "                1. - (1. / self.n_components), self.model_type,\n",
    "                self.hidden_sizes, self.activation, self.dropout, self.learn_priors,\n",
    "                self.lr, self.momentum, self.reduce_on_plateau, save_dir))\n",
    "\n",
    "        self.model_dir = save_dir\n",
    "        self.train_data = train_dataset\n",
    "        self.validation_data = validation_dataset\n",
    "        if self.validation_data is not None:\n",
    "            self.early_stopping = EarlyStopping(patience=patience, verbose=verbose, path=save_dir, delta=delta)\n",
    "        train_loader = DataLoader(\n",
    "            self.train_data, batch_size=self.batch_size, shuffle=True,\n",
    "            num_workers=self.num_data_loader_workers)\n",
    "\n",
    "        # init training variables\n",
    "        train_loss = 0\n",
    "        samples_processed = 0\n",
    "\n",
    "        # train loop\n",
    "        pbar = tqdm(self.num_epochs, position=0, leave=True)\n",
    "        for epoch in range(self.num_epochs):\n",
    "            self.nn_epoch = epoch\n",
    "            # train epoch\n",
    "            s = datetime.datetime.now()\n",
    "            sp, train_loss = self._train_epoch(train_loader)\n",
    "            samples_processed += sp\n",
    "            e = datetime.datetime.now()\n",
    "            pbar.update(1)\n",
    "            pbar.set_description(\"Epoch: [{}/{}]\\t Seen Samples: [{}/{}]\\tTrain Loss: {}\\tTime: {}\".format(\n",
    "                epoch + 1, self.num_epochs, samples_processed,\n",
    "                len(self.train_data) * self.num_epochs, train_loss, e - s))\n",
    "\n",
    "            if self.validation_data is not None:\n",
    "                self.best_loss_train = train_loss\n",
    "                self.best_components = self.model.beta\n",
    "\n",
    "                validation_loader = DataLoader(self.validation_data, batch_size=self.batch_size, shuffle=True,\n",
    "                                               num_workers=self.num_data_loader_workers)\n",
    "                # train epoch\n",
    "                s = datetime.datetime.now()\n",
    "                val_samples_processed, val_loss = self._validation(validation_loader)\n",
    "                e = datetime.datetime.now()\n",
    "\n",
    "                # report\n",
    "                if verbose:\n",
    "                    print(\"Epoch: [{}/{}]\\tSamples: [{}/{}]\\tValidation Loss: {}\\tTime: {}\".format(\n",
    "                        epoch + 1, self.num_epochs, val_samples_processed,\n",
    "                        len(self.validation_data) * self.num_epochs, val_loss, e - s))\n",
    "\n",
    "                self.early_stopping(val_loss, self)\n",
    "                if self.early_stopping.early_stop:\n",
    "                    print(\"Early stopping\")\n",
    "\n",
    "                    break\n",
    "            else:\n",
    "                # save best\n",
    "                if train_loss < self.best_loss_train:\n",
    "                    self.best_loss_train = train_loss\n",
    "                    self.best_components = self.model.beta\n",
    "\n",
    "                if save_dir is not None:\n",
    "                    self.save(save_dir)\n",
    "\n",
    "        pbar.close()\n",
    "\n",
    "    def _validation(self, loader):\n",
    "        \"\"\"Validation epoch.\"\"\"\n",
    "        self.model.eval()\n",
    "        val_loss = 0\n",
    "        samples_processed = 0\n",
    "        for batch_samples in loader:\n",
    "            # batch_size x vocab_size\n",
    "            X = batch_samples['X']\n",
    "            X = X.reshape(X.shape[0], -1)\n",
    "            X_bert = batch_samples['X_bert']\n",
    "\n",
    "            if self.USE_CUDA:\n",
    "                X = X.cuda()\n",
    "                X_bert = X_bert.cuda()\n",
    "\n",
    "            # forward pass\n",
    "            self.model.zero_grad()\n",
    "            prior_mean, prior_variance, posterior_mean, posterior_variance, posterior_log_variance, word_dists =\\\n",
    "                self.model(X, X_bert)\n",
    "            loss = self._loss(X, word_dists, prior_mean, prior_variance,\n",
    "                              posterior_mean, posterior_variance, posterior_log_variance)\n",
    "\n",
    "            # compute train loss\n",
    "            samples_processed += X.size()[0]\n",
    "            val_loss += loss.item()\n",
    "\n",
    "        val_loss /= samples_processed\n",
    "\n",
    "        return samples_processed, val_loss\n",
    "\n",
    "    def get_thetas(self, dataset, n_samples=20):\n",
    "        \"\"\"\n",
    "        Get the document-topic distribution for a dataset of topics. Includes multiple sampling to reduce variation via\n",
    "        the parameter n_sample.\n",
    "        :param dataset: a PyTorch Dataset containing the documents\n",
    "        :param n_samples: the number of sample to collect to estimate the final distribution (the more the better).\n",
    "        \"\"\"\n",
    "        warnings.warn(\"Call to `get_thetas` is deprecated and will be removed in version 2, \"\n",
    "                      \"use `get_doc_topic_distribution` instead\",\n",
    "                      DeprecationWarning)\n",
    "        return self.get_doc_topic_distribution(dataset, n_samples=n_samples)\n",
    "\n",
    "    def get_doc_topic_distribution(self, dataset, n_samples=20):\n",
    "        \"\"\"\n",
    "        Get the document-topic distribution for a dataset of topics. Includes multiple sampling to reduce variation via\n",
    "        the parameter n_sample.\n",
    "        :param dataset: a PyTorch Dataset containing the documents\n",
    "        :param n_samples: the number of sample to collect to estimate the final distribution (the more the better).\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "\n",
    "        loader = DataLoader(\n",
    "            dataset, batch_size=self.batch_size, shuffle=False,\n",
    "            num_workers=self.num_data_loader_workers)\n",
    "        pbar = tqdm(n_samples, position=0, leave=True)\n",
    "        final_thetas = []\n",
    "        for sample_index in range(n_samples):\n",
    "            with torch.no_grad():\n",
    "                collect_theta = []\n",
    "\n",
    "                for batch_samples in loader:\n",
    "                    # batch_size x vocab_size\n",
    "                    X = batch_samples['X']\n",
    "                    X = X.reshape(X.shape[0], -1)\n",
    "                    X_bert = batch_samples['X_bert']\n",
    "\n",
    "                    if self.USE_CUDA:\n",
    "                        X = X.cuda()\n",
    "                        X_bert = X_bert.cuda()\n",
    "\n",
    "                    # forward pass\n",
    "                    self.model.zero_grad()\n",
    "                    collect_theta.extend(self.model.get_theta(X, X_bert).cpu().numpy().tolist())\n",
    "\n",
    "                pbar.update(1)\n",
    "                pbar.set_description(\"Sampling: [{}/{}]\".format(sample_index + 1, n_samples))\n",
    "\n",
    "                final_thetas.append(np.array(collect_theta))\n",
    "        pbar.close()\n",
    "        return np.sum(final_thetas, axis=0) / n_samples\n",
    "\n",
    "    def get_most_likely_topic(self, doc_topic_distribution):\n",
    "        \"\"\" get the most likely topic for each document\n",
    "        :param doc_topic_distribution: ndarray representing the topic distribution of each document\n",
    "        \"\"\"\n",
    "        return np.argmax(doc_topic_distribution, axis=0)\n",
    "\n",
    "    def predict(self, dataset, k=10):\n",
    "        \"\"\"Predict input.\"\"\"\n",
    "        self.model.eval()\n",
    "\n",
    "        loader = DataLoader(\n",
    "            dataset, batch_size=self.batch_size, shuffle=False,\n",
    "            num_workers=self.num_data_loader_workers)\n",
    "\n",
    "        preds = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_samples in loader:\n",
    "                # batch_size x vocab_size\n",
    "                X = batch_samples['X']\n",
    "                X = X.reshape(X.shape[0], -1)\n",
    "                X_bert = batch_samples['X_bert']\n",
    "\n",
    "                if self.USE_CUDA:\n",
    "                    X = X.cuda()\n",
    "                    X_bert = X_bert.cuda()\n",
    "\n",
    "                # forward pass\n",
    "                self.model.zero_grad()\n",
    "                _, _, _, _, _, word_dists = self.model(X, X_bert)\n",
    "\n",
    "                _, indices = torch.sort(word_dists, dim=1)\n",
    "                preds += [indices[:, :k]]\n",
    "\n",
    "            preds = torch.cat(preds, dim=0)\n",
    "        return preds\n",
    "\n",
    "    def get_topics(self, k=10):\n",
    "        \"\"\"\n",
    "        Retrieve topic words.\n",
    "        :param k: int, number of words to return per topic, default 10.\n",
    "        \"\"\"\n",
    "        assert k <= self.input_size, \"k must be <= input size.\"\n",
    "        component_dists = self.best_components\n",
    "        topics = defaultdict(list)\n",
    "        for i in range(self.n_components):\n",
    "            _, idxs = torch.topk(component_dists[i], k)\n",
    "            component_words = [self.train_data.idx2token[idx]\n",
    "                               for idx in idxs.cpu().numpy()]\n",
    "            topics[i] = component_words\n",
    "        return topics\n",
    "\n",
    "    def get_topic_lists(self, k=10):\n",
    "        \"\"\"\n",
    "        Retrieve the lists of topic words.\n",
    "        :param k: (int) number of words to return per topic, default 10.\n",
    "        \"\"\"\n",
    "        assert k <= self.input_size, \"k must be <= input size.\"\n",
    "        # TODO: collapse this method with the one that just returns the topics\n",
    "        component_dists = self.best_components\n",
    "        topics = []\n",
    "        for i in range(self.n_components):\n",
    "            _, idxs = torch.topk(component_dists[i], k)\n",
    "            component_words = [self.train_data.idx2token[idx]\n",
    "                               for idx in idxs.cpu().numpy()]\n",
    "            topics.append(component_words)\n",
    "        return topics\n",
    "\n",
    "    def _format_file(self):\n",
    "        model_dir = \"contextualized_topic_model_nc_{}_tpm_{}_tpv_{}_hs_{}_ac_{}_do_{}_lr_{}_mo_{}_rp_{}\". \\\n",
    "            format(self.n_components, 0.0, 1 - (1. / self.n_components),\n",
    "                   self.model_type, self.hidden_sizes, self.activation,\n",
    "                   self.dropout, self.lr, self.momentum,\n",
    "                   self.reduce_on_plateau)\n",
    "        return model_dir\n",
    "\n",
    "    def save(self, models_dir=None):\n",
    "        \"\"\"\n",
    "        Save model. (Experimental Feature, not tested)\n",
    "        :param models_dir: path to directory for saving NN models.\n",
    "        \"\"\"\n",
    "        warnings.simplefilter('always', Warning)\n",
    "        warnings.warn(\"This is an experimental feature that we has not been fully tested. Refer to the following issue:\"\n",
    "                      \"https://github.com/MilaNLProc/contextualized-topic-models/issues/38\",\n",
    "                      Warning)\n",
    "\n",
    "        if (self.model is not None) and (models_dir is not None):\n",
    "\n",
    "            model_dir = self._format_file()\n",
    "            if not os.path.isdir(os.path.join(models_dir, model_dir)):\n",
    "                os.makedirs(os.path.join(models_dir, model_dir))\n",
    "\n",
    "            filename = \"epoch_{}\".format(self.nn_epoch) + '.pth'\n",
    "            fileloc = os.path.join(models_dir, model_dir, filename)\n",
    "            with open(fileloc, 'wb') as file:\n",
    "                torch.save({'state_dict': self.model.state_dict(),\n",
    "                            'dcue_dict': self.__dict__}, file)\n",
    "\n",
    "    def load(self, model_dir, epoch):\n",
    "        \"\"\"\n",
    "        Load a previously trained model. (Experimental Feature, not tested)\n",
    "        :param model_dir: directory where models are saved.\n",
    "        :param epoch: epoch of model to load.\n",
    "        \"\"\"\n",
    "\n",
    "        warnings.simplefilter('always', Warning)\n",
    "        warnings.warn(\"This is an experimental feature that we has not been fully tested. Refer to the following issue:\"\n",
    "                      \"https://github.com/MilaNLProc/contextualized-topic-models/issues/38\",\n",
    "                      Warning)\n",
    "\n",
    "        epoch_file = \"epoch_\" + str(epoch) + \".pth\"\n",
    "        model_file = os.path.join(model_dir, epoch_file)\n",
    "        with open(model_file, 'rb') as model_dict:\n",
    "            checkpoint = torch.load(model_dict)\n",
    "\n",
    "        for (k, v) in checkpoint['dcue_dict'].items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "        self.model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "    def get_topic_word_matrix(self):\n",
    "        \"\"\"\n",
    "        Return the topic-word matrix (dimensions: number of topics x length of the vocabulary).\n",
    "        If model_type is LDA, the matrix is normalized; otherwise the matrix is unnormalized.\n",
    "        \"\"\"\n",
    "        return self.model.topic_word_matrix.cpu().detach().numpy()\n",
    "\n",
    "    def get_topic_word_distribution(self):\n",
    "        \"\"\"\n",
    "        Return the topic-word distribution (dimensions: number of topics x length of the vocabulary).\n",
    "        \"\"\"\n",
    "        mat = self.get_topic_word_matrix()\n",
    "        return softmax(mat, axis=1)\n",
    "\n",
    "    def get_word_distribution_by_topic_id(self, topic):\n",
    "        \"\"\"\n",
    "        Return the word probability distribution of a topic sorted by probability.\n",
    "        :param topic: id of the topic (int)\n",
    "        :returns list of tuples (word, probability) sorted by the probability in descending order\n",
    "        \"\"\"\n",
    "        if topic >= self.n_components:\n",
    "            raise Exception('Topic id must be lower than the number of topics')\n",
    "        else:\n",
    "            wd = self.get_topic_word_distribution()\n",
    "            t = [(word, wd[topic][idx]) for idx, word in self.train_data.idx2token.items()]\n",
    "            t = sorted(t, key=lambda x: -x[1])\n",
    "        return t\n",
    "\n",
    "    def get_wordcloud(self, topic_id, n_words=5, background_color=\"black\"):\n",
    "        \"\"\"\n",
    "        Plotting the wordcloud. It is an adapted version of the code found here:\n",
    "        http://amueller.github.io/word_cloud/auto_examples/simple.html#sphx-glr-auto-examples-simple-py and\n",
    "        here https://github.com/ddangelov/Top2Vec/blob/master/top2vec/Top2Vec.py\n",
    "        :param topic_id: id of the topic\n",
    "        :param n_words: number of words to show in word cloud\n",
    "        :param background_color: color of the background\n",
    "        \"\"\"\n",
    "        word_score_list = self.get_word_distribution_by_topic_id(topic_id)[:n_words]\n",
    "        word_score_dict = {tup[0]: tup[1] for tup in word_score_list}\n",
    "        plt.figure(figsize=(10, 4), dpi=200)\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(wordcloud.WordCloud(width=1000, height=400, background_color=background_color\n",
    "                                       ).generate_from_frequencies(word_score_dict))\n",
    "        plt.title(\"Displaying Topic \" + str(topic_id), loc='center', fontsize=24)\n",
    "        plt.show()\n",
    "\n",
    "    def get_predicted_topics(self, dataset, n_samples):\n",
    "        \"\"\"\n",
    "        Return the a list containing the predicted topic for each document (length: number of documents).\n",
    "        :param dataset: CTMDataset to infer topics\n",
    "        :param n_samples: number of sampling of theta\n",
    "        :return: the predicted topics\n",
    "        \"\"\"\n",
    "        predicted_topics = []\n",
    "        thetas = self.get_doc_topic_distribution(dataset, n_samples)\n",
    "\n",
    "        for idd in range(len(dataset)):\n",
    "            predicted_topic = np.argmax(thetas[idd] / np.sum(thetas[idd]))\n",
    "            predicted_topics.append(predicted_topic)\n",
    "        return predicted_topics\n",
    "\n",
    "\n",
    "class ZeroShotTM(CTM):\n",
    "    \"\"\"\n",
    "    ZeroShotTM, as described in https://arxiv.org/pdf/2004.07737v1.pdf\n",
    "    :param input_size: int, dimension of input\n",
    "    :param bert_input_size: int, dimension of input that comes from BERT embeddings\n",
    "    :param n_components: int, number of topic components, (default 10)\n",
    "    :param model_type: string, 'prodLDA' or 'LDA' (default 'prodLDA')\n",
    "    :param hidden_sizes: tuple, length = n_layers, (default (100, 100))\n",
    "    :param activation: string, 'softplus', 'relu', (default 'softplus')\n",
    "    :param dropout: float, dropout to use (default 0.2)\n",
    "    :param learn_priors: bool, make priors a learnable parameter (default True)\n",
    "    :param batch_size: int, size of batch to use for training (default 64)\n",
    "    :param lr: float, learning rate to use for training (default 2e-3)\n",
    "    :param momentum: float, momentum to use for training (default 0.99)\n",
    "    :param solver: string, optimizer 'adam' or 'sgd' (default 'adam')\n",
    "    :param num_epochs: int, number of epochs to train for, (default 100)\n",
    "    :param reduce_on_plateau: bool, reduce learning rate by 10x on plateau of 10 epochs (default False)\n",
    "    :param num_data_loader_workers: int, number of data loader workers (default cpu_count). set it to 0 if you are using Windows\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, bert_input_size, n_components=10, model_type='prodLDA',\n",
    "                 hidden_sizes=(100, 100), activation='softplus', dropout=0.2,\n",
    "                 learn_priors=True, batch_size=64, lr=2e-3, momentum=0.99,\n",
    "                 solver='adam', num_epochs=100, reduce_on_plateau=False, num_data_loader_workers=mp.cpu_count()):\n",
    "        inference_type = \"zeroshot\"\n",
    "        super().__init__(input_size, bert_input_size, inference_type, n_components, model_type,\n",
    "                         hidden_sizes, activation, dropout,\n",
    "                         learn_priors, batch_size, lr, momentum,\n",
    "                         solver, num_epochs, reduce_on_plateau, num_data_loader_workers)\n",
    "\n",
    "\n",
    "class CombinedTM(CTM):\n",
    "    \"\"\"\n",
    "    CombinedTM, as described in https://arxiv.org/pdf/2004.03974.pdf\n",
    "    :param input_size: int, dimension of input\n",
    "    :param bert_input_size: int, dimension of input that comes from BERT embeddings\n",
    "    :param n_components: int, number of topic components, (default 10)\n",
    "    :param model_type: string, 'prodLDA' or 'LDA' (default 'prodLDA')\n",
    "    :param hidden_sizes: tuple, length = n_layers, (default (100, 100))\n",
    "    :param activation: string, 'softplus', 'relu', (default 'softplus')\n",
    "    :param dropout: float, dropout to use (default 0.2)\n",
    "    :param learn_priors: bool, make priors a learnable parameter (default True)\n",
    "    :param batch_size: int, size of batch to use for training (default 64)\n",
    "    :param lr: float, learning rate to use for training (default 2e-3)\n",
    "    :param momentum: float, momentum to use for training (default 0.99)\n",
    "    :param solver: string, optimizer 'adam' or 'sgd' (default 'adam')\n",
    "    :param num_epochs: int, number of epochs to train for, (default 100)\n",
    "    :param reduce_on_plateau: bool, reduce learning rate by 10x on plateau of 10 epochs (default False)\n",
    "    :param num_data_loader_workers: int, number of data loader workers (default cpu_count). set it to 0 if you are using Windows\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, bert_input_size, n_components=10, model_type='prodLDA',\n",
    "                 hidden_sizes=(100, 100), activation='softplus', dropout=0.2,\n",
    "                 learn_priors=True, batch_size=64, lr=2e-3, momentum=0.99,\n",
    "                 solver='adam', num_epochs=100, reduce_on_plateau=False, num_data_loader_workers=mp.cpu_count()):\n",
    "        inference_type = \"combined\"\n",
    "        super().__init__(input_size, bert_input_size, inference_type, n_components, model_type,\n",
    "                         hidden_sizes, activation, dropout,\n",
    "                         learn_priors, batch_size, lr, momentum,\n",
    "                         solver, num_epochs, reduce_on_plateau, num_data_loader_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import string\n",
    "from nltk.corpus import stopwords as stop_words\n",
    "import warnings\n",
    "\n",
    "class WhiteSpacePreprocessing():\n",
    "    \"\"\"\n",
    "    Provides a very simple preprocessing script that filters infrequent tokens from text\n",
    "    \"\"\"\n",
    "    def __init__(self, documents, stopwords_language=\"english\", vocabulary_size=2000):\n",
    "        \"\"\"\n",
    "        :param documents: list of strings\n",
    "        :param stopwords_language: string of the language of the stopwords (see nltk stopwords)\n",
    "        :param vocabulary_size: the number of most frequent words to include in the documents. Infrequent words will be discarded from the list of preprocessed documents\n",
    "        \"\"\"\n",
    "        self.documents = documents\n",
    "        self.stopwords = set(stop_words.words(stopwords_language))\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "\n",
    "    def preprocess(self):\n",
    "        \"\"\"\n",
    "        Note that if after filtering some documents do not contain words we remove them. That is why we return also the\n",
    "        list of unpreprocessed documents.\n",
    "        :return: preprocessed documents, unpreprocessed documents and the vocabulary list\n",
    "        \"\"\"\n",
    "        preprocessed_docs_tmp = self.documents\n",
    "        preprocessed_docs_tmp = [doc.lower() for doc in preprocessed_docs_tmp]\n",
    "        preprocessed_docs_tmp = [doc.translate(\n",
    "            str.maketrans(string.punctuation, ' ' * len(string.punctuation))) for doc in preprocessed_docs_tmp]\n",
    "        preprocessed_docs_tmp = [' '.join([w for w in doc.split() if len(w) > 0 and w not in self.stopwords])\n",
    "                             for doc in preprocessed_docs_tmp]\n",
    "\n",
    "        vectorizer = CountVectorizer(max_features=self.vocabulary_size, token_pattern=r'\\b[a-zA-Z]{2,}\\b')\n",
    "        vectorizer.fit_transform(preprocessed_docs_tmp)\n",
    "        vocabulary = set(vectorizer.get_feature_names())\n",
    "        preprocessed_docs_tmp = [' '.join([w for w in doc.split() if w in vocabulary])\n",
    "                                 for doc in preprocessed_docs_tmp]\n",
    "\n",
    "        preprocessed_docs, unpreprocessed_docs = [], []\n",
    "        for i, doc in enumerate(preprocessed_docs_tmp):\n",
    "            if len(doc) > 0:\n",
    "                preprocessed_docs.append(doc)\n",
    "                unpreprocessed_docs.append(self.documents[i])\n",
    "\n",
    "        return preprocessed_docs, unpreprocessed_docs, list(vocabulary)\n",
    "\n",
    "\n",
    "class SimplePreprocessing(WhiteSpacePreprocessing):\n",
    "    def __init__(self, documents, stopwords_language=\"english\"):\n",
    "        super().__init__(documents, stopwords_language)\n",
    "        warnings.simplefilter('always', DeprecationWarning)\n",
    "\n",
    "        if self.__class__.__name__ == \"CTM\":\n",
    "\n",
    "            warnings.warn(\"SimplePrepocessing is deprecated and will be removed in version 2.0, \"\n",
    "                          \"use WhiteSpacePreprocessing\", DeprecationWarning)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import scipy.sparse\n",
    "\n",
    "class CTMDataset(Dataset):\n",
    "\n",
    "    \"\"\"Class to load BOW dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, X, X_bert, idx2token):\n",
    "        \"\"\"\n",
    "        Args\n",
    "            X : array-like, shape=(n_samples, n_features)\n",
    "                Document word matrix.\n",
    "        \"\"\"\n",
    "        if X.shape[0] != len(X_bert):\n",
    "            raise Exception(\"Wait! BoW and Contextual Embeddings have different sizes! \"\n",
    "                            \"You might want to check if the BoW preparation method has removed some documents. \")\n",
    "\n",
    "        self.X = X\n",
    "        self.X_bert = X_bert\n",
    "        self.idx2token = idx2token\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return length of dataset.\"\"\"\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        \"\"\"Return sample from dataset at index i.\"\"\"\n",
    "        if type(self.X[i]) == scipy.sparse.csr.csr_matrix:\n",
    "            X = torch.FloatTensor(self.X[i].todense())\n",
    "            X_bert = torch.FloatTensor(self.X_bert[i])\n",
    "        else:\n",
    "            X = torch.FloatTensor(self.X[i])\n",
    "            X_bert = torch.FloatTensor(self.X_bert[i])\n",
    "\n",
    "        return {'X': X, 'X_bert': X_bert}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import scipy.sparse\n",
    "import warnings\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "def get_bag_of_words(data, min_length):\n",
    "    \"\"\"\n",
    "    Creates the bag of words\n",
    "    \"\"\"\n",
    "    vect = [np.bincount(x[x != np.array(None)].astype('int'), minlength=min_length)\n",
    "            for x in data if np.sum(x[x != np.array(None)]) != 0]\n",
    "\n",
    "    vect = scipy.sparse.csr_matrix(vect)\n",
    "    return vect\n",
    "\n",
    "\n",
    "def bert_embeddings_from_file(text_file, sbert_model_to_load, batch_size=200):\n",
    "    \"\"\"\n",
    "    Creates SBERT Embeddings from an input file\n",
    "    \"\"\"\n",
    "    model = SentenceTransformer(sbert_model_to_load)\n",
    "    with open(text_file, encoding=\"utf-8\") as filino:\n",
    "        train_text = list(map(lambda x: x, filino.readlines()))\n",
    "\n",
    "    return np.array(model.encode(train_text, show_progress_bar=True, batch_size=batch_size))\n",
    "\n",
    "\n",
    "def bert_embeddings_from_list(texts, sbert_model_to_load, batch_size=200):\n",
    "    \"\"\"\n",
    "    Creates SBERT Embeddings from a list\n",
    "    \"\"\"\n",
    "    model = SentenceTransformer(sbert_model_to_load)\n",
    "    return np.array(model.encode(texts, show_progress_bar=True, batch_size=batch_size))\n",
    "\n",
    "\n",
    "class TopicModelDataPreparation:\n",
    "\n",
    "    def __init__(self, contextualized_model=None):\n",
    "        self.contextualized_model = contextualized_model\n",
    "        self.vocab = []\n",
    "        self.id2token = {}\n",
    "        self.vectorizer = None\n",
    "\n",
    "    def load(self, contextualized_embeddings, bow_embeddings, id2token):\n",
    "        return CTMDataset(bow_embeddings, contextualized_embeddings, id2token)\n",
    "\n",
    "    def create_training_set(self, text_for_contextual, text_for_bow):\n",
    "\n",
    "        if self.contextualized_model is None:\n",
    "            raise Exception(\"You should define a contextualized model if you want to create the embeddings\")\n",
    "\n",
    "        # TODO: this count vectorizer removes tokens that have len = 1, might be unexpected for the users\n",
    "        self.vectorizer = CountVectorizer()\n",
    "\n",
    "        train_bow_embeddings = self.vectorizer.fit_transform(text_for_bow)\n",
    "        train_contextualized_embeddings = bert_embeddings_from_list(text_for_contextual, self.contextualized_model)\n",
    "        self.vocab = self.vectorizer.get_feature_names()\n",
    "        self.id2token = {k: v for k, v in zip(range(0, len(self.vocab)), self.vocab)}\n",
    "\n",
    "        return CTMDataset(train_bow_embeddings, train_contextualized_embeddings, self.id2token)\n",
    "\n",
    "    def create_test_set(self, text_for_contextual, text_for_bow=None):\n",
    "\n",
    "        if self.contextualized_model is None:\n",
    "            raise Exception(\"You should define a contextualized model if you want to create the embeddings\")\n",
    "\n",
    "        if text_for_bow is not None:\n",
    "            test_bow_embeddings = self.vectorizer.transform(text_for_bow)\n",
    "        else:\n",
    "            # dummy matrix\n",
    "            test_bow_embeddings = scipy.sparse.csr_matrix(np.zeros((len(text_for_contextual), 1)))\n",
    "        test_contextualized_embeddings = bert_embeddings_from_list(text_for_contextual, self.contextualized_model)\n",
    "\n",
    "        return CTMDataset(test_bow_embeddings, test_contextualized_embeddings, self.id2token)\n",
    "\n",
    "    def create_validation_set(self, text_for_contextual, text_for_bow=None):\n",
    "        return self.create_test_set(text_for_contextual=text_for_contextual, text_for_bow=text_for_bow)\n",
    "\n",
    "\n",
    "class QuickText:\n",
    "    \"\"\"\n",
    "    Integrated class to handle all the text preprocessing needed\n",
    "    \"\"\"\n",
    "    def __init__(self, bert_model, text_for_bow, text_for_bert=None):\n",
    "        \"\"\"\n",
    "        :param bert_model: string, bert model to use\n",
    "        :param text_for_bow: list, list of sentences with the preprocessed text\n",
    "        :param text_for_bert: list, list of sentences with the unpreprocessed text\n",
    "        \"\"\"\n",
    "        self.vocab_dict = {}\n",
    "        self.vocab = []\n",
    "        self.index_dd = None\n",
    "        self.idx2token = None\n",
    "        self.bow = None\n",
    "        self.bert_model = bert_model\n",
    "        self.text_handler = \"\"\n",
    "        self.data_bert = None\n",
    "        self.text_for_bow = text_for_bow\n",
    "        self.text_for_bert = text_for_bert\n",
    "        self.loaded_from_config = False\n",
    "\n",
    "    def prepare_bow(self):\n",
    "        indptr = [0]\n",
    "        indices = []\n",
    "        data = []\n",
    "        vocabulary = {}\n",
    "\n",
    "        if self.text_for_bow is not None:\n",
    "            docs = self.text_for_bow\n",
    "        else:\n",
    "            docs = self.text_for_bert\n",
    "\n",
    "        for d in docs:\n",
    "            for term in d.split():\n",
    "                index = vocabulary.setdefault(term, len(vocabulary))\n",
    "                indices.append(index)\n",
    "                data.append(1)\n",
    "            indptr.append(len(indices))\n",
    "\n",
    "        self.vocab_dict = vocabulary\n",
    "        self.vocab = list(vocabulary.keys())\n",
    "\n",
    "        warnings.simplefilter('always', DeprecationWarning)\n",
    "        if len(self.vocab) > 2000:\n",
    "            warnings.warn(\"The vocab you are using has more than 2000 words, reconstructing high-dimensional vectors requires\"\n",
    "                          \"significantly more training epochs and training samples. \"\n",
    "                          \"Consider reducing the number of vocabulary items. \"\n",
    "                          \"See https://github.com/MilaNLProc/contextualized-topic-models#preprocessing \"\n",
    "                          \"and https://github.com/MilaNLProc/contextualized-topic-models#tldr\", Warning)\n",
    "\n",
    "        self.idx2token = {v: k for (k, v) in self.vocab_dict.items()}\n",
    "        self.bow = scipy.sparse.csr_matrix((data, indices, indptr), dtype=int)\n",
    "\n",
    "    def load_configuration(self, bow_embeddings, contextualized_embeddings, vocab, id2token):\n",
    "        \"\"\"\n",
    "        This method defines a way to instantiate the model with pre-trained data.\n",
    "        \"\"\"\n",
    "\n",
    "        assert len(contextualized_embeddings) == bow_embeddings.shape[0]\n",
    "        assert len(vocab) == len(id2token)\n",
    "        self.data_bert = contextualized_embeddings\n",
    "        self.bow = bow_embeddings\n",
    "        self.vocab = vocab\n",
    "        self.idx2token = id2token\n",
    "        self.loaded_from_config = True\n",
    "\n",
    "    def load_pre_trained_contextualized(self, contextualized_embeddings):\n",
    "        \"\"\"\n",
    "        In case the contextualized embeddings have been already trained, it is possible to load them with this method\n",
    "        \"\"\"\n",
    "        self.data_bert = contextualized_embeddings\n",
    "\n",
    "    def load_dataset(self):\n",
    "        if self.loaded_from_config:\n",
    "            training_dataset = CTMDataset(self.bow, self.data_bert, self.idx2token)\n",
    "        else:\n",
    "            self.prepare_bow()\n",
    "\n",
    "            if self.data_bert is None:\n",
    "                if self.text_for_bert is not None:\n",
    "                    self.data_bert = bert_embeddings_from_list(self.text_for_bert, self.bert_model)\n",
    "                else:\n",
    "                    self.data_bert = bert_embeddings_from_list(self.text_for_bow, self.bert_model)\n",
    "\n",
    "            training_dataset = CTMDataset(self.bow, self.data_bert, self.idx2token)\n",
    "\n",
    "        return training_dataset\n",
    "\n",
    "class TextHandler:\n",
    "    \"\"\"\n",
    "    Class used to handle the text preparation and the BagOfWord\n",
    "    \"\"\"\n",
    "    def __init__(self, file_name=None, sentences=None):\n",
    "        self.file_name = file_name\n",
    "        self.sentences = sentences\n",
    "        self.vocab_dict = {}\n",
    "        self.vocab = []\n",
    "        self.index_dd = None\n",
    "        self.idx2token = None\n",
    "        self.bow = None\n",
    "\n",
    "        warnings.simplefilter('always', DeprecationWarning)\n",
    "        if len(self.vocab) > 2000:\n",
    "            warnings.warn(\"TextHandler class is deprecated and will be removed in version 2.0. Use QuickText.\", Warning)\n",
    "\n",
    "    def prepare(self):\n",
    "        indptr = [0]\n",
    "        indices = []\n",
    "        data = []\n",
    "        vocabulary = {}\n",
    "\n",
    "        if self.sentences is None and self.file_name is None:\n",
    "            raise Exception(\"Sentences and file_names cannot both be none\")\n",
    "\n",
    "        if self.sentences is not None:\n",
    "            docs = self.sentences\n",
    "        elif self.file_name is not None:\n",
    "            with open(self.file_name, encoding=\"utf-8\") as filino:\n",
    "                docs = filino.readlines()\n",
    "        else:\n",
    "            raise Exception(\"One parameter between sentences and file_name should be selected\")\n",
    "\n",
    "        for d in docs:\n",
    "            for term in d.split():\n",
    "                index = vocabulary.setdefault(term, len(vocabulary))\n",
    "                indices.append(index)\n",
    "                data.append(1)\n",
    "            indptr.append(len(indices))\n",
    "\n",
    "        self.vocab_dict = vocabulary\n",
    "        self.vocab = list(vocabulary.keys())\n",
    "\n",
    "        warnings.simplefilter('always', DeprecationWarning)\n",
    "        if len(self.vocab) > 2000:\n",
    "            warnings.warn(\"The vocab you are using has more than 2000 words, reconstructing high-dimensional vectors requires\"\n",
    "                          \"significantly more training epochs and training samples. \"\n",
    "                          \"Consider reducing the number of vocabulary items. \"\n",
    "                          \"See https://github.com/MilaNLProc/contextualized-topic-models#preprocessing \"\n",
    "                          \"and https://github.com/MilaNLProc/contextualized-topic-models#tldr\", Warning)\n",
    "\n",
    "        self.idx2token = {v: k for (k, v) in self.vocab_dict.items()}\n",
    "        self.bow = scipy.sparse.csr_matrix((data, indices, indptr), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/sdeshpande/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "titles = health_data.title.tolist()[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "sp = WhiteSpacePreprocessing(titles, stopwords_language='english')\n",
    "preprocessed_documents, unpreprocessed_corpus, vocab = sp.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['absence surface expression feline infectious peritonitis virus fipv antigens infected cells isolated cats fip',\n",
       " 'correlation antimicrobial consumption incidence health care associated infections due methicillin resistant staphylococcus aureus vancomycin resistant enterococci university hospital taiwan']"
      ]
     },
     "metadata": {},
     "execution_count": 66
    }
   ],
   "source": [
    "preprocessed_documents[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "0it [1:17:46, ?it/s]\n",
      "0it [00:15, ?it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:12<00:00, 12.28s/it]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['absence',\n",
       " 'absolute',\n",
       " 'accompanied',\n",
       " 'acid',\n",
       " 'acidification',\n",
       " 'acids',\n",
       " 'acquired',\n",
       " 'activate',\n",
       " 'activation',\n",
       " 'activities']"
      ]
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "source": [
    "tp = TopicModelDataPreparation(\"distiluse-base-multilingual-cased\")\n",
    "training_dataset = tp.create_training_set(text_for_contextual=unpreprocessed_corpus, text_for_bow=preprocessed_documents)\n",
    "tp.vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n0it [00:00, ?it/s]"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-0da690226975>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mctm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mZeroShotTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert_input_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mctm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_dataset\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# run the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-49-ae3d9cd7aca7>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_dataset, validation_dataset, save_dir, verbose, patience, delta)\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0;31m# train epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m             \u001b[0msp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m             \u001b[0msamples_processed\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m             \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-49-ae3d9cd7aca7>\u001b[0m in \u001b[0;36m_train_epoch\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0msamples_processed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_samples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m             \u001b[0;31m# batch_size x vocab_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_samples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'X'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_worker_number_rationality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_MultiProcessingDataLoaderIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m    912\u001b[0m             \u001b[0;31m#     before it starts, and __del__ tries to join but will get:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m             \u001b[0;31m#     AssertionError: can only join a started process.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 914\u001b[0;31m             \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    915\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index_queues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_queue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/multiprocessing/process.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    119\u001b[0m                \u001b[0;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mDefaultContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpopen_spawn_posix\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mclass\u001b[0m \u001b[0mForkServerProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseProcess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/multiprocessing/popen_spawn_posix.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mduplicate_for_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinalizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_launch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mduplicate_for_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/multiprocessing/popen_spawn_posix.py\u001b[0m in \u001b[0;36m_launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentinel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparent_r\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosefd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                 \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetbuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mfds_to_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ctm = ZeroShotTM(input_size=len(tp.vocab), bert_input_size=512, n_components=50, num_epochs=1)\n",
    "ctm.fit(training_dataset) # run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctm.get_topic_lists(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_sample how many times to sample the distribution (see the documentation)\n",
    "topics_predictions = ctm.get_thetas(titles[101:105], n_samples=5) # get all the topic predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_number = np.argmax(topics_predictions[0]) # get the topic id of the first document\n",
    "ctm.get_topic_lists(10)[topic_number] "
   ]
  }
 ]
}