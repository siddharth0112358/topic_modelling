{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('anaconda3': virtualenv)",
   "metadata": {
    "interpreter": {
     "hash": "17f60a5376c77425fc92555942c601d199d1b6f4d9fe48b17446cfeffaeccd2f"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# KeyBert"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "def mmr(doc_embedding: np.ndarray,\n",
    "        word_embeddings: np.ndarray,\n",
    "        words: List[str],\n",
    "        top_n: int = 5,\n",
    "        diversity: float = 0.8) -> List[Tuple[str, float]]:\n",
    "    \"\"\" Calculate Maximal Marginal Relevance (MMR)\n",
    "    between candidate keywords and the document.\n",
    "    MMR considers the similarity of keywords/keyphrases with the\n",
    "    document, along with the similarity of already selected\n",
    "    keywords and keyphrases. This results in a selection of keywords\n",
    "    that maximize their within diversity with respect to the document.\n",
    "    Arguments:\n",
    "        doc_embedding: The document embeddings\n",
    "        word_embeddings: The embeddings of the selected candidate keywords/phrases\n",
    "        words: The selected candidate keywords/keyphrases\n",
    "        top_n: The number of keywords/keyhprases to return\n",
    "        diversity: How diverse the select keywords/keyphrases are.\n",
    "                   Values between 0 and 1 with 0 being not diverse at all\n",
    "                   and 1 being most diverse.\n",
    "    Returns:\n",
    "         List[Tuple[str, float]]: The selected keywords/keyphrases with their distances\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract similarity within words, and between words and the document\n",
    "    word_doc_similarity = cosine_similarity(word_embeddings, doc_embedding)\n",
    "    word_similarity = cosine_similarity(word_embeddings)\n",
    "\n",
    "    # Initialize candidates and already choose best keyword/keyphras\n",
    "    keywords_idx = [np.argmax(word_doc_similarity)]\n",
    "    candidates_idx = [i for i in range(len(words)) if i != keywords_idx[0]]\n",
    "\n",
    "    for _ in range(top_n - 1):\n",
    "        # Extract similarities within candidates and\n",
    "        # between candidates and selected keywords/phrases\n",
    "        candidate_similarities = word_doc_similarity[candidates_idx, :]\n",
    "        target_similarities = np.max(word_similarity[candidates_idx][:, keywords_idx], axis=1)\n",
    "\n",
    "        # Calculate MMR\n",
    "        mmr = (1-diversity) * candidate_similarities - diversity * target_similarities.reshape(-1, 1)\n",
    "        mmr_idx = candidates_idx[np.argmax(mmr)]\n",
    "\n",
    "        # Update keywords & candidates\n",
    "        keywords_idx.append(mmr_idx)\n",
    "        candidates_idx.remove(mmr_idx)\n",
    "\n",
    "    return [(words[idx], round(float(word_doc_similarity.reshape(1, -1)[0][idx]), 4)) for idx in keywords_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "def max_sum_similarity(doc_embedding: np.ndarray,\n",
    "                       word_embeddings: np.ndarray,\n",
    "                       words: List[str],\n",
    "                       top_n: int,\n",
    "                       nr_candidates: int) -> List[Tuple[str, float]]:\n",
    "    \"\"\" Calculate Max Sum Distance for extraction of keywords\n",
    "    We take the 2 x top_n most similar words/phrases to the document.\n",
    "    Then, we take all top_n combinations from the 2 x top_n words and\n",
    "    extract the combination that are the least similar to each other\n",
    "    by cosine similarity.\n",
    "    NOTE:\n",
    "        This is O(n^2) and therefore not advised if you use a large top_n\n",
    "    Arguments:\n",
    "        doc_embedding: The document embeddings\n",
    "        word_embeddings: The embeddings of the selected candidate keywords/phrases\n",
    "        words: The selected candidate keywords/keyphrases\n",
    "        top_n: The number of keywords/keyhprases to return\n",
    "        nr_candidates: The number of candidates to consider\n",
    "    Returns:\n",
    "         List[Tuple[str, float]]: The selected keywords/keyphrases with their distances\n",
    "    \"\"\"\n",
    "    if nr_candidates < top_n:\n",
    "        raise Exception(\"Make sure that the number of candidates exceeds the number \"\n",
    "                        \"of keywords to return.\")\n",
    "\n",
    "    # Calculate distances and extract keywords\n",
    "    distances = cosine_similarity(doc_embedding, word_embeddings)\n",
    "    distances_words = cosine_similarity(word_embeddings, word_embeddings)\n",
    "\n",
    "    # Get 2*top_n words as candidates based on cosine similarity\n",
    "    words_idx = list(distances.argsort()[0][-nr_candidates:])\n",
    "    words_vals = [words[index] for index in words_idx]\n",
    "    candidates = distances_words[np.ix_(words_idx, words_idx)]\n",
    "\n",
    "    # Calculate the combination of words that are the least similar to each other\n",
    "    min_sim = 100_000\n",
    "    candidate = None\n",
    "    for combination in itertools.combinations(range(len(words_idx)), top_n):\n",
    "        sim = sum([candidates[i][j] for i in combination for j in combination if i != j])\n",
    "        if sim < min_sim:\n",
    "            candidate = combination\n",
    "            min_sim = sim\n",
    "\n",
    "    return [(words_vals[idx], round(float(distances[0][idx]), 4)) for idx in candidate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from typing import List, Union, Tuple\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Flair\n",
    "try:\n",
    "    from flair.embeddings import DocumentEmbeddings, TokenEmbeddings, DocumentPoolEmbeddings\n",
    "    from flair.data import Sentence\n",
    "    _HAS_FLAIR = True\n",
    "except ModuleNotFoundError as e:\n",
    "    DocumentEmbeddings, TokenEmbeddings, DocumentPoolEmbeddings = None, None, None\n",
    "    _HAS_FLAIR = False\n",
    "\n",
    "\n",
    "class KeyBERT:\n",
    "    \"\"\"\n",
    "    A minimal method for keyword extraction with BERT\n",
    "    The keyword extraction is done by finding the sub-phrases in\n",
    "    a document that are the most similar to the document itself.\n",
    "    First, document embeddings are extracted with BERT to get a\n",
    "    document-level representation. Then, word embeddings are extracted\n",
    "    for N-gram words/phrases. Finally, we use cosine similarity to find the\n",
    "    words/phrases that are the most similar to the document.\n",
    "    The most similar words could then be identified as the words that\n",
    "    best describe the entire document.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 model: Union[str,\n",
    "                              SentenceTransformer,\n",
    "                              DocumentEmbeddings,\n",
    "                              TokenEmbeddings] = 'distilbert-base-nli-mean-tokens'):\n",
    "        \"\"\" KeyBERT initialization\n",
    "        Arguments:\n",
    "            model: Use a custom embedding model. You can pass in a string related\n",
    "                   to one of the following models:\n",
    "                   https://www.sbert.net/docs/pretrained_models.html\n",
    "                   You can also pass in a SentenceTransformer() model or a Flair\n",
    "                   DocumentEmbedding model.\n",
    "        \"\"\"\n",
    "        self.model = self._select_embedding_model(model)\n",
    "\n",
    "    def extract_keywords(self,\n",
    "                         docs: Union[str, List[str]],\n",
    "                         keyphrase_ngram_range: Tuple[int, int] = (1, 1),\n",
    "                         stop_words: Union[str, List[str]] = 'english',\n",
    "                         top_n: int = 5,\n",
    "                         min_df: int = 1,\n",
    "                         use_maxsum: bool = False,\n",
    "                         use_mmr: bool = False,\n",
    "                         diversity: float = 0.5,\n",
    "                         nr_candidates: int = 20,\n",
    "                         vectorizer: CountVectorizer = None) -> Union[List[Tuple[str, float]],\n",
    "                                                                      List[List[Tuple[str, float]]]]:\n",
    "        \"\"\" Extract keywords/keyphrases\n",
    "        NOTE:\n",
    "            I would advise you to iterate over single documents as they\n",
    "            will need the least amount of memory. Even though this is slower,\n",
    "            you are not likely to run into memory errors.\n",
    "        Multiple Documents:\n",
    "            There is an option to extract keywords for multiple documents\n",
    "            that is faster than extraction for multiple single documents.\n",
    "            However...this method assumes that you can keep the word embeddings\n",
    "            for all words in the vocabulary in memory which might be troublesome.\n",
    "            I would advise against using this option and simply iterating\n",
    "            over documents instead if you have limited hardware.\n",
    "        Arguments:\n",
    "            docs: The document(s) for which to extract keywords/keyphrases\n",
    "            keyphrase_ngram_range: Length, in words, of the extracted keywords/keyphrases\n",
    "            stop_words: Stopwords to remove from the document\n",
    "            top_n: Return the top n keywords/keyphrases\n",
    "            min_df: Minimum document frequency of a word across all documents\n",
    "                    if keywords for multiple documents need to be extracted\n",
    "            use_maxsum: Whether to use Max Sum Similarity for the selection\n",
    "                        of keywords/keyphrases\n",
    "            use_mmr: Whether to use Maximal Marginal Relevance (MMR) for the\n",
    "                     selection of keywords/keyphrases\n",
    "            diversity: The diversity of the results between 0 and 1 if use_mmr\n",
    "                       is set to True\n",
    "            nr_candidates: The number of candidates to consider if use_maxsum is\n",
    "                           set to True\n",
    "            vectorizer: Pass in your own CountVectorizer from scikit-learn\n",
    "        Returns:\n",
    "            keywords: the top n keywords for a document with their respective distances\n",
    "                      to the input document\n",
    "        \"\"\"\n",
    "\n",
    "        if isinstance(docs, str):\n",
    "            return self._extract_keywords_single_doc(docs,\n",
    "                                                     keyphrase_ngram_range,\n",
    "                                                     stop_words,\n",
    "                                                     top_n,\n",
    "                                                     use_maxsum,\n",
    "                                                     use_mmr,\n",
    "                                                     diversity,\n",
    "                                                     nr_candidates,\n",
    "                                                     vectorizer)\n",
    "        elif isinstance(docs, list):\n",
    "            warnings.warn(\"Although extracting keywords for multiple documents is faster \"\n",
    "                          \"than iterating over single documents, it requires significantly more memory \"\n",
    "                          \"to hold all word embeddings. Use this at your own discretion!\")\n",
    "            return self._extract_keywords_multiple_docs(docs,\n",
    "                                                        keyphrase_ngram_range,\n",
    "                                                        stop_words,\n",
    "                                                        top_n,\n",
    "                                                        min_df,\n",
    "                                                        vectorizer)\n",
    "\n",
    "    def _extract_keywords_single_doc(self,\n",
    "                                     doc: str,\n",
    "                                     keyphrase_ngram_range: Tuple[int, int] = (1, 1),\n",
    "                                     stop_words: Union[str, List[str]] = 'english',\n",
    "                                     top_n: int = 5,\n",
    "                                     use_maxsum: bool = False,\n",
    "                                     use_mmr: bool = False,\n",
    "                                     diversity: float = 0.5,\n",
    "                                     nr_candidates: int = 20,\n",
    "                                     vectorizer: CountVectorizer = None) -> List[Tuple[str, float]]:\n",
    "        \"\"\" Extract keywords/keyphrases for a single document\n",
    "        Arguments:\n",
    "            doc: The document for which to extract keywords/keyphrases\n",
    "            keyphrase_ngram_range: Length, in words, of the extracted keywords/keyphrases\n",
    "            stop_words: Stopwords to remove from the document\n",
    "            top_n: Return the top n keywords/keyphrases\n",
    "            use_mmr: Whether to use Max Sum Similarity\n",
    "            use_mmr: Whether to use MMR\n",
    "            diversity: The diversity of results between 0 and 1 if use_mmr is True\n",
    "            nr_candidates: The number of candidates to consider if use_maxsum is set to True\n",
    "            vectorizer: Pass in your own CountVectorizer from scikit-learn\n",
    "        Returns:\n",
    "            keywords: the top n keywords for a document with their respective distances\n",
    "                      to the input document\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Extract Words\n",
    "            if vectorizer:\n",
    "                count = vectorizer.fit([doc])\n",
    "            else:\n",
    "                count = CountVectorizer(ngram_range=keyphrase_ngram_range, stop_words=stop_words).fit([doc])\n",
    "            words = count.get_feature_names()\n",
    "\n",
    "            # Extract Embeddings\n",
    "            doc_embedding = self._extract_embeddings([doc])\n",
    "            word_embeddings = self._extract_embeddings(words)\n",
    "            # doc_embedding = self.model.encode([doc])\n",
    "            # word_embeddings = self.model.encode(words)\n",
    "\n",
    "            # Calculate distances and extract keywords\n",
    "            if use_mmr:\n",
    "                keywords = mmr(doc_embedding, word_embeddings, words, top_n, diversity)\n",
    "            elif use_maxsum:\n",
    "                keywords = max_sum_similarity(doc_embedding, word_embeddings, words, top_n, nr_candidates)\n",
    "            else:\n",
    "                distances = cosine_similarity(doc_embedding, word_embeddings)\n",
    "                keywords = [(words[index], round(float(distances[0][index]), 4))\n",
    "                            for index in distances.argsort()[0][-top_n:]][::-1]\n",
    "\n",
    "            return keywords\n",
    "        except ValueError:\n",
    "            return []\n",
    "\n",
    "    def _extract_keywords_multiple_docs(self,\n",
    "                                        docs: List[str],\n",
    "                                        keyphrase_ngram_range: Tuple[int, int] = (1, 1),\n",
    "                                        stop_words: str = 'english',\n",
    "                                        top_n: int = 5,\n",
    "                                        min_df: int = 1,\n",
    "                                        vectorizer: CountVectorizer = None) -> List[List[Tuple[str, float]]]:\n",
    "        \"\"\" Extract keywords/keyphrases for a multiple documents\n",
    "        This currently does not use MMR as\n",
    "        Arguments:\n",
    "            docs: The document for which to extract keywords/keyphrases\n",
    "            keyphrase_ngram_range: Length, in words, of the extracted keywords/keyphrases\n",
    "            stop_words: Stopwords to remove from the document\n",
    "            top_n: Return the top n keywords/keyphrases\n",
    "            min_df: The minimum frequency of words\n",
    "            vectorizer: Pass in your own CountVectorizer from scikit-learn\n",
    "        Returns:\n",
    "            keywords: the top n keywords for a document with their respective distances\n",
    "                      to the input document\n",
    "        \"\"\"\n",
    "        # Extract words\n",
    "        if vectorizer:\n",
    "            count = vectorizer.fit(docs)\n",
    "        else:\n",
    "            count = CountVectorizer(ngram_range=keyphrase_ngram_range, stop_words=stop_words, min_df=min_df).fit(docs)\n",
    "        words = count.get_feature_names()\n",
    "        df = count.transform(docs)\n",
    "\n",
    "        # Extract embeddings\n",
    "        word_embeddings = self._extract_embeddings(words)\n",
    "        doc_embeddings = self._extract_embeddings(docs)\n",
    "        # word_embeddings = self.model.encode(words, show_progress_bar=True)\n",
    "        # doc_embeddings = self.model.encode(docs, show_progress_bar=True)\n",
    "\n",
    "        # Extract keywords\n",
    "        keywords = []\n",
    "        for index, doc in tqdm(enumerate(docs)):\n",
    "            doc_words = [words[i] for i in df[index].nonzero()[1]]\n",
    "\n",
    "            if doc_words:\n",
    "                doc_word_embeddings = np.array([word_embeddings[i] for i in df[index].nonzero()[1]])\n",
    "                distances = cosine_similarity([doc_embeddings[index]], doc_word_embeddings)[0]\n",
    "                doc_keywords = [(doc_words[i], round(float(distances[i]), 4)) for i in distances.argsort()[-top_n:]]\n",
    "                keywords.append(doc_keywords)\n",
    "            else:\n",
    "                keywords.append([\"None Found\"])\n",
    "\n",
    "        return keywords\n",
    "\n",
    "    def _extract_embeddings(self, documents: Union[List[str], str]) -> np.ndarray:\n",
    "        \"\"\" Extract sentence/document embeddings through pre-trained embeddings\n",
    "        For an overview of pre-trained models: https://www.sbert.net/docs/pretrained_models.html\n",
    "        Arguments:\n",
    "            documents: Dataframe with documents and their corresponding IDs\n",
    "        Returns:\n",
    "            embeddings: The extracted embeddings using the sentence transformer\n",
    "                        module. Typically uses pre-trained huggingface models.\n",
    "        \"\"\"\n",
    "        if isinstance(documents, str):\n",
    "            documents = [documents]\n",
    "\n",
    "        # Infer embeddings with SentenceTransformer\n",
    "        if isinstance(self.model, SentenceTransformer):\n",
    "            embeddings = self.model.encode(documents)\n",
    "\n",
    "        # Infer embeddings with Flair\n",
    "        elif isinstance(self.model, DocumentEmbeddings):\n",
    "            embeddings = []\n",
    "            for index, document in enumerate(documents):\n",
    "                try:\n",
    "                    sentence = Sentence(document) if document else Sentence(\"an empty document\")\n",
    "                    self.model.embed(sentence)\n",
    "                except RuntimeError:\n",
    "                    sentence = Sentence(\"an empty document\")\n",
    "                    self.model.embed(sentence)\n",
    "                embedding = sentence.embedding.detach().cpu().numpy()\n",
    "                embeddings.append(embedding)\n",
    "            embeddings = np.asarray(embeddings)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"An incorrect embedding model type was selected.\")\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    def _select_embedding_model(self, model: Union[str,\n",
    "                                                   SentenceTransformer,\n",
    "                                                   DocumentEmbeddings,\n",
    "                                                   TokenEmbeddings]) -> Union[SentenceTransformer,\n",
    "                                                                              DocumentEmbeddings]:\n",
    "        \"\"\" Select an embedding model based on language or a specific sentence transformer models.\n",
    "        When selecting a language, we choose distilbert-base-nli-stsb-mean-tokens for English and\n",
    "        xlm-r-bert-base-nli-stsb-mean-tokens for all other languages as it support 100+ languages.\n",
    "        Arguments:\n",
    "            model: Use a custom embedding model. You can pass in a string related\n",
    "                   to one of the following models:\n",
    "                   https://www.sbert.net/docs/pretrained_models.html\n",
    "                   You can also pass in a SentenceTransformer() model or a Flair\n",
    "                   DocumentEmbedding model.\n",
    "        Returns:\n",
    "            model: Either a Sentence-Transformer or Flair model\n",
    "        \"\"\"\n",
    "\n",
    "        # Sentence Transformer embeddings\n",
    "        if isinstance(model, SentenceTransformer):\n",
    "            return model\n",
    "\n",
    "        # Flair word embeddings\n",
    "        elif _HAS_FLAIR and isinstance(model, TokenEmbeddings):\n",
    "            return DocumentPoolEmbeddings([model])\n",
    "\n",
    "        # Flair document embeddings + disable fine tune to prevent CUDA OOM\n",
    "        # https://github.com/flairNLP/flair/issues/1719\n",
    "        elif _HAS_FLAIR and isinstance(model, DocumentEmbeddings):\n",
    "            if \"fine_tune\" in model.__dict__:\n",
    "                model.fine_tune = False\n",
    "            return model\n",
    "\n",
    "        # Select embedding model based on specific sentence transformer model\n",
    "        elif isinstance(model, str):\n",
    "            return SentenceTransformer(model)\n",
    "\n",
    "        return SentenceTransformer(\"xlm-r-bert-base-nli-stsb-mean-tokens\")"
   ]
  },
  {
   "source": [
    "# Yake keyword extraction"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Levenshtein(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def __ratio(distance, str_length):\n",
    "        return 1 - float(distance) / float(str_length)\n",
    "\n",
    "    @staticmethod\n",
    "    def ratio(seq1, seq2):\n",
    "        str_distance = Levenshtein.distance(seq1,seq2)\n",
    "        str_length = max(len(seq1),len(seq2))\n",
    "        return Levenshtein.__ratio(str_distance,str_length)\n",
    "\n",
    "    @staticmethod\n",
    "    def distance(seq1, seq2):  \n",
    "        size_x = len(seq1) + 1\n",
    "        size_y = len(seq2) + 1\n",
    "        matrix = np.zeros ((size_x, size_y))\n",
    "        for x in range(size_x):\n",
    "            matrix [x, 0] = x\n",
    "        for y in range(size_y):\n",
    "            matrix [0, y] = y\n",
    "\n",
    "        for x in range(1, size_x):\n",
    "            for y in range(1, size_y):\n",
    "                if seq1[x-1] == seq2[y-1]:\n",
    "                    matrix [x,y] = min(\n",
    "                        matrix[x-1, y] + 1,\n",
    "                        matrix[x-1, y-1],\n",
    "                        matrix[x, y-1] + 1\n",
    "                    )\n",
    "                else:\n",
    "                    matrix [x,y] = min(\n",
    "                        matrix[x-1,y] + 1,\n",
    "                        matrix[x-1,y-1] + 1,\n",
    "                        matrix[x,y-1] + 1\n",
    "                    )\n",
    "        return (matrix[size_x - 1, size_y - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segtok.segmenter import split_multi\n",
    "from segtok.tokenizer import web_tokenizer, split_contractions\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import string\n",
    "import os\n",
    "import math\n",
    "import jellyfish\n",
    "import re\n",
    "\n",
    "STOPWORD_WEIGHT = 'bi'\n",
    "\n",
    "class DataCore(object):\n",
    "    \n",
    "    def __init__(self, text, stopword_set, windowsSize, n, tagsToDiscard = set(['u', 'd']), exclude = set(string.punctuation)):\n",
    "        self.number_of_sentences = 0\n",
    "        self.number_of_words = 0\n",
    "        self.terms = {}\n",
    "        self.candidates = {}\n",
    "        self.sentences_obj = []\n",
    "        self.sentences_str = []\n",
    "        self.G = nx.DiGraph()\n",
    "        self.exclude = exclude\n",
    "        self.tagsToDiscard = tagsToDiscard\n",
    "        self.freq_ns = {}\n",
    "        for i in range(n):\n",
    "            self.freq_ns[i+1] = 0.\n",
    "        self.stopword_set = stopword_set\n",
    "        self._build(text, windowsSize, n)\n",
    "\n",
    "    def build_candidate(self, candidate_string):\n",
    "        sentences_str = [w for w in split_contractions(web_tokenizer(candidate_string.lower())) if not (w.startswith(\"'\") and len(w) > 1) and len(w) > 0]\n",
    "        candidate_terms = []\n",
    "        for (i, word) in enumerate(sentences_str):\n",
    "            tag = self.getTag(word, i)\n",
    "            term_obj = self.getTerm(word, save_non_seen=False)\n",
    "            if term_obj.tf == 0:\n",
    "                term_obj = None\n",
    "            candidate_terms.append( (tag, word, term_obj) )\n",
    "        if len([cand for cand in candidate_terms if cand[2] != None]) == 0:\n",
    "            invalid_virtual_cand = composed_word(None)\n",
    "            return invalid_virtual_cand\n",
    "        virtual_cand = composed_word(candidate_terms)\n",
    "        return virtual_cand\n",
    "\n",
    "    # Build the datacore features\n",
    "    def _build(self, text, windowsSize, n):\n",
    "        text = self.pre_filter(text)\n",
    "        self.sentences_str = [ [w for w in split_contractions(web_tokenizer(s)) if not (w.startswith(\"'\") and len(w) > 1) and len(w) > 0] for s in list(split_multi(text)) if len(s.strip()) > 0]\n",
    "        self.number_of_sentences = len(self.sentences_str)\n",
    "        pos_text = 0\n",
    "        block_of_word_obj = []\n",
    "        sentence_obj_aux = []\n",
    "        for (sentence_id, sentence) in enumerate(self.sentences_str):\n",
    "            sentence_obj_aux = []\n",
    "            block_of_word_obj = []\n",
    "            for (pos_sent, word) in enumerate(sentence):\n",
    "                if len([c for c in word if c in self.exclude]) == len(word): # If the word is based on exclude chars\n",
    "                    if len(block_of_word_obj) > 0:\n",
    "                        sentence_obj_aux.append( block_of_word_obj )\n",
    "                        block_of_word_obj = []\n",
    "                else:\n",
    "                    tag = self.getTag(word, pos_sent)\n",
    "                    term_obj = self.getTerm(word)\n",
    "                    term_obj.addOccur(tag, sentence_id, pos_sent, pos_text)\n",
    "                    pos_text += 1\n",
    "\n",
    "                    #Create co-occurrence matrix\n",
    "                    if tag not in self.tagsToDiscard:\n",
    "                        word_windows = list(range( max(0, len(block_of_word_obj)-windowsSize), len(block_of_word_obj) ))\n",
    "                        for w in word_windows:\n",
    "                            if block_of_word_obj[w][0] not in self.tagsToDiscard: \n",
    "                                self.addCooccur(block_of_word_obj[w][2], term_obj)\n",
    "                    #Generate candidate keyphrase list\n",
    "                    candidate = [ (tag, word, term_obj) ]\n",
    "                    cand = composed_word(candidate)\n",
    "                    self.addOrUpdateComposedWord(cand)\n",
    "                    word_windows = list(range( max(0, len(block_of_word_obj)-(n-1)), len(block_of_word_obj) ))[::-1]\n",
    "                    for w in word_windows:\n",
    "                        candidate.append(block_of_word_obj[w])\n",
    "                        self.freq_ns[len(candidate)] += 1.\n",
    "                        cand = composed_word(candidate[::-1])\n",
    "                        self.addOrUpdateComposedWord(cand)\n",
    "\n",
    "                    # Add term to the block of words' buffer\n",
    "                    block_of_word_obj.append( (tag, word, term_obj) )\n",
    "\n",
    "            if len(block_of_word_obj) > 0:\n",
    "                sentence_obj_aux.append( block_of_word_obj )\n",
    "\n",
    "            if len(sentence_obj_aux) > 0:\n",
    "                self.sentences_obj.append(sentence_obj_aux)\n",
    "\n",
    "        if len(block_of_word_obj) > 0:\n",
    "            sentence_obj_aux.append( block_of_word_obj )\n",
    "\n",
    "        if len(sentence_obj_aux) > 0:\n",
    "            self.sentences_obj.append(sentence_obj_aux)\n",
    "\n",
    "        self.number_of_words = pos_text\n",
    "\n",
    "    def build_single_terms_features(self, features=None):\n",
    "        validTerms = [ term for term in self.terms.values() if not term.stopword ]\n",
    "        validTFs = (np.array([ x.tf for x in validTerms ]))\n",
    "        avgTF = validTFs.mean()\n",
    "        stdTF = validTFs.std()\n",
    "        maxTF = max([ x.tf for x in self.terms.values()])\n",
    "        list(map(lambda x: x.updateH(maxTF=maxTF, avgTF=avgTF, stdTF=stdTF, number_of_sentences=self.number_of_sentences, features=features), self.terms.values()))\n",
    "\n",
    "    def build_mult_terms_features(self, features=None):\n",
    "        list(map(lambda x: x.updateH(features=features), [cand for cand in self.candidates.values() if cand.isValid()]))\n",
    "\n",
    "    def pre_filter(self, text):\n",
    "        prog = re.compile(\"^(\\\\s*([A-Z]))\")\n",
    "        parts = text.split('\\n')\n",
    "        buffer = ''\n",
    "        for part in parts:\n",
    "            sep = ' '\n",
    "            if prog.match(part):\n",
    "                sep = '\\n\\n'\n",
    "            buffer += sep + part.replace('\\t',' ')\n",
    "        return buffer\n",
    "\n",
    "    def getTag(self, word, i):\n",
    "        try:\n",
    "            w2 = word.replace(\",\",\"\")\n",
    "            float(w2)\n",
    "            return \"d\"\n",
    "        except:\n",
    "            cdigit = len([c for c in word if c.isdigit()])\n",
    "            calpha = len([c for c in word if c.isalpha()])\n",
    "            if ( cdigit > 0 and calpha > 0 ) or (cdigit == 0 and calpha == 0) or len([c for c in word if c in self.exclude]) > 1:\n",
    "                return \"u\"\n",
    "            if len(word) == len([c for c in word if c.isupper()]):\n",
    "                return \"a\"\n",
    "            if len([c for c in word if c.isupper()]) == 1 and len(word) > 1 and word[0].isupper() and i > 0:\n",
    "                return \"n\"\n",
    "        return \"p\"\n",
    "\n",
    "    def getTerm(self, str_word, save_non_seen=True):\n",
    "        unique_term = str_word.lower()\n",
    "        simples_sto = unique_term in self.stopword_set\n",
    "        if unique_term.endswith('s') and len(unique_term) > 3:\n",
    "            unique_term = unique_term[:-1]\n",
    "\n",
    "        if unique_term in self.terms:\n",
    "            return self.terms[unique_term]\n",
    "                \n",
    "        # Include this part\n",
    "        simples_unique_term = unique_term\n",
    "        for pontuation in self.exclude:\n",
    "            simples_unique_term = simples_unique_term.replace(pontuation, '')\n",
    "        # until here\n",
    "        isstopword = simples_sto or unique_term in self.stopword_set or len(simples_unique_term) < 3\n",
    "        \n",
    "        term_id = len(self.terms)\n",
    "        term_obj = single_word(unique_term, term_id, self.G)\n",
    "        term_obj.stopword = isstopword\n",
    "\n",
    "        if save_non_seen:\n",
    "            self.G.add_node(term_id)\n",
    "            self.terms[unique_term] = term_obj\n",
    "\n",
    "        return term_obj\n",
    "\n",
    "    def addCooccur(self, left_term, right_term):\n",
    "        if right_term.id not in self.G[left_term.id]:\n",
    "            self.G.add_edge(left_term.id, right_term.id, TF=0.)\n",
    "        self.G[left_term.id][right_term.id][\"TF\"]+=1.\n",
    "        \n",
    "    def addOrUpdateComposedWord(self, cand):\n",
    "        if cand.unique_kw not in self.candidates:\n",
    "            self.candidates[cand.unique_kw] = cand\n",
    "        else:\n",
    "            self.candidates[cand.unique_kw].uptadeCand(cand)\n",
    "        self.candidates[cand.unique_kw].tf += 1.\n",
    "\n",
    "\n",
    "class composed_word(object):\n",
    "    def __init__(self, terms): # [ (tag, word, term_obj) ]\n",
    "        if terms == None:\n",
    "             self.start_or_end_stopwords = True\n",
    "             self.tags = set()\n",
    "             return\n",
    "        self.tags = set([''.join([ w[0] for w in terms ])])\n",
    "        self.unique_kw = ' '.join( [ w[1].lower() for w in terms ] )\n",
    "        self.size = len(terms)\n",
    "        self.terms = [ w[2] for w in terms if w[2] != None ]\n",
    "        self.tf = 0.\n",
    "        self.integrity = 1.\n",
    "        self.H = 1.\n",
    "        self.start_or_end_stopwords = self.terms[0].stopword or self.terms[-1].stopword\n",
    "\n",
    "    def uptadeCand(self, cand):\n",
    "        for tag in cand.tags:\n",
    "            self.tags.add( tag )\n",
    "\n",
    "    def isValid(self):\n",
    "        isValid = False\n",
    "        for tag in self.tags:\n",
    "            isValid = isValid or ( \"u\" not in tag and \"d\" not in tag )\n",
    "        return isValid and not self.start_or_end_stopwords\n",
    "\n",
    "    def get_composed_feature(self, feature_name, discart_stopword=True):\n",
    "        list_of_features = [ getattr(term, feature_name) for term in self.terms if ( discart_stopword and not term.stopword ) or not discart_stopword ]\n",
    "        sum_f  = sum(list_of_features)\n",
    "        prod_f = np.prod(list_of_features)\n",
    "        return ( sum_f, prod_f, prod_f /(sum_f + 1) )\n",
    "\n",
    "    def build_features(self, doc_id=None, keys=None, rel=True, rel_approx=True, isVirtual=False, features=['WFreq', 'WRel', 'tf', 'WCase', 'WPos', 'WSpread'], _stopword=[True, False]):\n",
    "        columns = []\n",
    "        seen = set()\n",
    "        features_cand = []\n",
    "\n",
    "        if doc_id != None:\n",
    "            columns.append('doc_id')\n",
    "            features_cand.append(doc_id)\n",
    "\n",
    "        if keys != None:\n",
    "            if rel:\n",
    "                columns.append('rel')\n",
    "                if self.unique_kw in keys or isVirtual:\n",
    "                    features_cand.append(1)\n",
    "                    seen.add(self.unique_kw)\n",
    "                else:\n",
    "                    features_cand.append(0)\n",
    "\n",
    "            if rel_approx:\n",
    "                columns.append('rel_approx')\n",
    "                max_gold_ = ('', 0.)\n",
    "                for gold_key in keys:\n",
    "                    dist = 1.-jellyfish.levenshtein_distance(gold_key, self.unique_kw ) / max(len(gold_key), len(self.unique_kw)) # _tL\n",
    "                    if max_gold_[1] < dist:\n",
    "                        max_gold_ = ( gold_key, dist )\n",
    "                features_cand.append(max_gold_[1])\n",
    "\n",
    "        columns.append('kw')\n",
    "        features_cand.append(self.unique_kw)\n",
    "        columns.append('h')\n",
    "        features_cand.append(self.H)\n",
    "        columns.append('tf')\n",
    "        features_cand.append(self.tf)\n",
    "        columns.append('size')\n",
    "        features_cand.append(self.size)\n",
    "        columns.append('isVirtual')\n",
    "        features_cand.append(int(isVirtual))\n",
    "\n",
    "        for feature_name in features:\n",
    "\n",
    "            for discart_stopword in _stopword:\n",
    "                (f_sum, f_prod, f_sum_prod) = self.get_composed_feature(feature_name, discart_stopword=discart_stopword)\n",
    "                columns.append('%ss_sum_K%s' % ('n' if discart_stopword else '', feature_name) )\n",
    "                features_cand.append(f_sum)\n",
    "\n",
    "                columns.append('%ss_prod_K%s' % ('n' if discart_stopword else '', feature_name) )\n",
    "                features_cand.append(f_prod)\n",
    "\n",
    "                columns.append('%ss_sum_prod_K%s' % ('n' if discart_stopword else '', feature_name) )\n",
    "                features_cand.append(f_sum_prod)\n",
    "\n",
    "        return (features_cand, columns, seen)\n",
    "\n",
    "    def updateH(self, features=None, isVirtual=False):\n",
    "        sum_H  = 0.\n",
    "        prod_H = 1.\n",
    "\n",
    "        for (t, term_base) in enumerate(self.terms):\n",
    "            if not term_base.stopword:\n",
    "                sum_H += term_base.H\n",
    "                prod_H *= term_base.H\n",
    "\n",
    "            else:\n",
    "                if STOPWORD_WEIGHT == 'bi':\n",
    "                    prob_t1 = 0.\n",
    "                    if term_base.G.has_edge(self.terms[t-1].id, self.terms[ t ].id):\n",
    "                        prob_t1 = term_base.G[self.terms[t-1].id][self.terms[ t ].id][\"TF\"] / self.terms[t-1].tf\n",
    "\n",
    "                    prob_t2 = 0.\n",
    "                    if term_base.G.has_edge(self.terms[ t ].id, self.terms[t+1].id):\n",
    "                        prob_t2 = term_base.G[self.terms[ t ].id][self.terms[t+1].id][\"TF\"] / self.terms[t+1].tf\n",
    "\n",
    "                    prob = prob_t1 * prob_t2\n",
    "                    prod_H *= (1 + (1 - prob ) )\n",
    "                    sum_H -= (1 - prob)\n",
    "                elif STOPWORD_WEIGHT == 'h':\n",
    "                    sum_H += term_base.H\n",
    "                    prod_H *= term_base.H\n",
    "                elif STOPWORD_WEIGHT == 'none':\n",
    "                    pass\n",
    "\n",
    "        tf_used = 1.\n",
    "        if features == None or \"KPF\" in features:\n",
    "            tf_used = self.tf\n",
    "\n",
    "        if isVirtual:\n",
    "            tf_used = np.mean( [term_obj.tf for term_obj in self.terms] )\n",
    "\n",
    "        self.H = prod_H / ( ( sum_H + 1 ) * tf_used )\n",
    "\n",
    "    def updateH_old(self, features=None, isVirtual=False):\n",
    "        sum_H  = 0.\n",
    "        prod_H = 1.\n",
    "\n",
    "        for (t, term_base) in enumerate(self.terms):\n",
    "            if isVirtual and term_base.tf==0:\n",
    "                continue\n",
    "\n",
    "            if term_base.stopword:\n",
    "                prob_t1 = 0.\n",
    "                if term_base.G.has_edge(self.terms[t-1].id, self.terms[ t ].id):\n",
    "                    prob_t1 = term_base.G[self.terms[t-1].id][self.terms[ t ].id][\"TF\"] / self.terms[t-1].tf\n",
    "\n",
    "                prob_t2 = 0.\n",
    "                if term_base.G.has_edge(self.terms[ t ].id, self.terms[t+1].id):\n",
    "                    prob_t2 = term_base.G[self.terms[ t ].id][self.terms[t+1].id][\"TF\"] / self.terms[t+1].tf\n",
    "\n",
    "                prob = prob_t1 * prob_t2\n",
    "                prod_H *= (1 + (1 - prob ) )\n",
    "                sum_H -= (1 - prob)\n",
    "            else:\n",
    "                sum_H += term_base.H\n",
    "                prod_H *= term_base.H\n",
    "        tf_used = 1.\n",
    "        if features == None or \"KPF\" in features:\n",
    "            tf_used = self.tf\n",
    "        if isVirtual:\n",
    "            tf_used = np.mean( [term_obj.tf for term_obj in self.terms] )\n",
    "        self.H = prod_H / ( ( sum_H + 1 ) * tf_used )\n",
    "\n",
    "\n",
    "class single_word(object):\n",
    "\n",
    "    def __init__(self, unique, idx, graph):\n",
    "        self.unique_term = unique\n",
    "        self.id = idx\n",
    "        self.tf = 0.\n",
    "        self.WFreq = 0.0\n",
    "        self.WCase = 0.0\n",
    "        self.tf_a = 0.\n",
    "        self.tf_n = 0.\n",
    "        self.WRel = 1.0\n",
    "        self.PL = 0.\n",
    "        self.PR = 0.\n",
    "        self.occurs = {}\n",
    "        self.WPos = 1.0\n",
    "        self.WSpread = 0.0\n",
    "        self.H = 0.0\n",
    "        self.stopword = False\n",
    "        self.G = graph\n",
    "\n",
    "        self.pagerank = 1.\n",
    "\n",
    "    def updateH(self, maxTF, avgTF, stdTF, number_of_sentences, features=None):\n",
    "        \"\"\"if features == None or \"WRel\" in features:\n",
    "            self.PL = self.WDL / maxTF\n",
    "            self.PR = self.WDR / maxTF\n",
    "            self.WRel = ( (0.5 + (self.PWL * (self.tf / maxTF) + self.PL)) + (0.5 + (self.PWR * (self.tf / maxTF) + self.PR)) )\"\"\"\n",
    "\n",
    "        if features == None or \"WRel\" in features:\n",
    "            self.PL = self.WDL / maxTF\n",
    "            self.PR = self.WDR / maxTF\n",
    "            self.WRel = ( (0.5 + (self.PWL * (self.tf / maxTF))) + (0.5 + (self.PWR * (self.tf / maxTF))) )\n",
    "\n",
    "        if features == None or \"WFreq\" in features:\n",
    "            self.WFreq = self.tf / (avgTF + stdTF)\n",
    "        \n",
    "        if features == None or \"WSpread\" in features:\n",
    "            self.WSpread = len(self.occurs) / number_of_sentences\n",
    "        \n",
    "        if features == None or \"WCase\" in features:\n",
    "            self.WCase = max(self.tf_a, self.tf_n) / (1. + math.log(self.tf))\n",
    "        \n",
    "        if features == None or \"WPos\" in features:\n",
    "            self.WPos = math.log( math.log( 3. + np.median(list(self.occurs.keys())) ) )\n",
    "\n",
    "        self.H = (self.WPos * self.WRel) / (self.WCase + (self.WFreq / self.WRel) + (self.WSpread / self.WRel))\n",
    "        \n",
    "    @property\n",
    "    def WDR(self):\n",
    "        return len( self.G.out_edges(self.id) )\n",
    "\n",
    "    @property\n",
    "    def WIR(self):\n",
    "        return sum( [ d['TF'] for (u,v,d) in self.G.out_edges(self.id, data=True) ] )\n",
    "\n",
    "    @property\n",
    "    def PWR(self):\n",
    "        wir = self.WIR\n",
    "        if wir == 0:\n",
    "            return 0\n",
    "        return self.WDR / wir \n",
    "    \n",
    "    @property\n",
    "    def WDL(self):\n",
    "        return len( self.G.in_edges(self.id) )\n",
    "\n",
    "    @property\n",
    "    def WIL(self):\n",
    "        return sum( [ d['TF'] for (u,v,d) in self.G.in_edges(self.id, data=True) ] )\n",
    "\n",
    "    @property\n",
    "    def PWL(self):\n",
    "        wil = self.WIL\n",
    "        if wil == 0:\n",
    "            return 0\n",
    "        return self.WDL / wil \n",
    "\n",
    "    def addOccur(self, tag, sent_id, pos_sent, pos_text):\n",
    "        if sent_id not in self.occurs:\n",
    "            self.occurs[sent_id] = []\n",
    "\n",
    "        self.occurs[sent_id].append( (pos_sent, pos_text) )\n",
    "        self.tf += 1.\n",
    "\n",
    "        if tag == \"a\":\n",
    "            self.tf_a += 1.\n",
    "        if tag == \"n\":\n",
    "            self.tf_n += 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"Main module.\"\"\"\n",
    "\n",
    "import string\n",
    "import os\n",
    "import jellyfish\n",
    "\n",
    "class KeywordExtractor(object):\n",
    "\n",
    "    def __init__(self, lan=\"en\", n=3, dedupLim=0.9, dedupFunc='seqm', windowsSize=1, top=20, features=None):\n",
    "        self.lan = lan\n",
    "\n",
    "        # dir_path = os.path.dirname(os.path.realpath(\"/Users/sdeshpande/Desktop/bioinformatices/\"))\n",
    "        # print(dir_path)\n",
    "\n",
    "        # local_path = os.path.join(\"StopwordsList\", \"stopwords_%s.txt\" % lan[:2].lower())\n",
    "\n",
    "\n",
    "        # if os.path.exists(os.path.join(dir_path,local_path)) == False:\n",
    "        #     local_path = os.path.join(\"StopwordsList\", \"stopwords_noLang.txt\")\n",
    "        \n",
    "        resource_path = \"/Users/sdeshpande/Desktop/bioinformatices/StopwordsList/stopwords_noLang.txt\"\n",
    "\n",
    "        try:\n",
    "            with open(resource_path, encoding='utf-8') as stop_fil:\n",
    "                self.stopword_set = set( stop_fil.read().lower().split(\"\\n\") )\n",
    "        except:\n",
    "            print('Warning, read stopword list as ISO-8859-1')\n",
    "            with open(resource_path, encoding='ISO-8859-1') as stop_fil:\n",
    "                self.stopword_set = set( stop_fil.read().lower().split(\"\\n\") )\n",
    "\n",
    "        self.n = n\n",
    "        self.top = top\n",
    "        self.dedupLim = dedupLim\n",
    "        self.features = features\n",
    "        self.windowsSize = windowsSize\n",
    "        if dedupFunc == 'jaro_winkler' or dedupFunc == 'jaro':\n",
    "            self.dedu_function = self.jaro\n",
    "        elif dedupFunc.lower() == 'sequencematcher' or dedupFunc.lower() == 'seqm':\n",
    "            self.dedu_function = self.seqm\n",
    "        else:\n",
    "            self.dedu_function = self.levs\n",
    "\n",
    "    def jaro(self, cand1, cand2):\n",
    "        return jellyfish.jaro_winkler(cand1, cand2 )\n",
    "\n",
    "    def levs(self, cand1, cand2):\n",
    "        return 1.-jellyfish.levenshtein_distance(cand1, cand2 ) / max(len(cand1),len(cand2))\n",
    "\n",
    "    def seqm(self, cand1, cand2):\n",
    "        return Levenshtein.ratio(cand1, cand2)\n",
    "\n",
    "    def extract_keywords(self, text):\n",
    "        text = text.replace('\\n\\t',' ')\n",
    "        dc = DataCore(text=text, stopword_set=self.stopword_set, windowsSize=self.windowsSize, n=self.n)\n",
    "        dc.build_single_terms_features(features=self.features)\n",
    "        dc.build_mult_terms_features(features=self.features)\n",
    "        resultSet = []\n",
    "        todedup = sorted([cc for cc in dc.candidates.values() if cc.isValid()], key=lambda c: c.H)\n",
    "\n",
    "        if self.dedupLim >= 1.:\n",
    "            return ([ (cand.H, cand.unique_kw) for cand in todedup])[:self.top]\n",
    "\n",
    "        for cand in todedup:\n",
    "            toadd = True\n",
    "            for (h, candResult) in resultSet:\n",
    "                dist = self.dedu_function(cand.unique_kw, candResult.unique_kw)\n",
    "                if dist > self.dedupLim:\n",
    "                    toadd = False\n",
    "                    break\n",
    "            if toadd:\n",
    "                resultSet.append( (cand.H, cand) )\n",
    "            if len(resultSet) == self.top:\n",
    "                break\n",
    "\n",
    "        return [ (cand.unique_kw,h) for (h,cand) in resultSet]\n"
   ]
  },
  {
   "source": [
    "https://github.com/NC0DER/KeyphraseExtraction/tree/main/KeyExt/yake/StopwordsList"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Rake keyword extraction"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Implementation of Rapid Automatic Keyword Extraction algorithm.\n",
    "As described in the paper `Automatic keyword extraction from individual\n",
    "documents` by Stuart Rose, Dave Engel, Nick Cramer and Wendy Cowley.\n",
    "\"\"\"\n",
    "\n",
    "import string\n",
    "from collections import Counter, defaultdict\n",
    "from itertools import chain, groupby, product\n",
    "\n",
    "import nltk\n",
    "from enum import Enum\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "\n",
    "class Metric(Enum):\n",
    "    \"\"\"Different metrics that can be used for ranking.\"\"\"\n",
    "\n",
    "    DEGREE_TO_FREQUENCY_RATIO = 0  # Uses d(w)/f(w) as the metric\n",
    "    WORD_DEGREE = 1  # Uses d(w) alone as the metric\n",
    "    WORD_FREQUENCY = 2  # Uses f(w) alone as the metric\n",
    "\n",
    "\n",
    "class Rake(object):\n",
    "    \"\"\"Rapid Automatic Keyword Extraction Algorithm.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        stopwords=None,\n",
    "        punctuations=None,\n",
    "        language=\"english\",\n",
    "        ranking_metric=Metric.DEGREE_TO_FREQUENCY_RATIO,\n",
    "        max_length=100000,\n",
    "        min_length=1,\n",
    "    ):\n",
    "        \"\"\"Constructor.\n",
    "        :param stopwords: List of Words to be ignored for keyword extraction.\n",
    "        :param punctuations: Punctuations to be ignored for keyword extraction.\n",
    "        :param language: Language to be used for stopwords\n",
    "        :param max_length: Maximum limit on the number of words in a phrase\n",
    "                           (Inclusive. Defaults to 100000)\n",
    "        :param min_length: Minimum limit on the number of words in a phrase\n",
    "                           (Inclusive. Defaults to 1)\n",
    "        \"\"\"\n",
    "        # By default use degree to frequency ratio as the metric.\n",
    "        if isinstance(ranking_metric, Metric):\n",
    "            self.metric = ranking_metric\n",
    "        else:\n",
    "            self.metric = Metric.DEGREE_TO_FREQUENCY_RATIO\n",
    "\n",
    "        # If stopwords not provided we use language stopwords by default.\n",
    "        self.stopwords = stopwords\n",
    "        if self.stopwords is None:\n",
    "            self.stopwords = nltk.corpus.stopwords.words(language)\n",
    "\n",
    "        # If punctuations are not provided we ignore all punctuation symbols.\n",
    "        self.punctuations = punctuations\n",
    "        if self.punctuations is None:\n",
    "            self.punctuations = string.punctuation\n",
    "\n",
    "        # All things which act as sentence breaks during keyword extraction.\n",
    "        self.to_ignore = set(chain(self.stopwords, self.punctuations))\n",
    "\n",
    "        # Assign min or max length to the attributes\n",
    "        self.min_length = min_length\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # Stuff to be extracted from the provided text.\n",
    "        self.frequency_dist = None\n",
    "        self.degree = None\n",
    "        self.rank_list = None\n",
    "        self.ranked_phrases = None\n",
    "\n",
    "    def extract_keywords_from_text(self, text):\n",
    "        \"\"\"Method to extract keywords from the text provided.\n",
    "        :param text: Text to extract keywords from, provided as a string.\n",
    "        \"\"\"\n",
    "        sentences = nltk.tokenize.sent_tokenize(text)\n",
    "        self.extract_keywords_from_sentences(sentences)\n",
    "\n",
    "    def extract_keywords_from_sentences(self, sentences):\n",
    "        \"\"\"Method to extract keywords from the list of sentences provided.\n",
    "        :param sentences: Text to extraxt keywords from, provided as a list\n",
    "                          of strings, where each string is a sentence.\n",
    "        \"\"\"\n",
    "        phrase_list = self._generate_phrases(sentences)\n",
    "        self._build_frequency_dist(phrase_list)\n",
    "        self._build_word_co_occurance_graph(phrase_list)\n",
    "        self._build_ranklist(phrase_list)\n",
    "\n",
    "    def get_ranked_phrases(self):\n",
    "        \"\"\"Method to fetch ranked keyword strings.\n",
    "        :return: List of strings where each string represents an extracted\n",
    "                 keyword string.\n",
    "        \"\"\"\n",
    "        return self.ranked_phrases\n",
    "\n",
    "    def get_ranked_phrases_with_scores(self):\n",
    "        \"\"\"Method to fetch ranked keyword strings along with their scores.\n",
    "        :return: List of tuples where each tuple is formed of an extracted\n",
    "                 keyword string and its score. Ex: (5.68, 'Four Scoures')\n",
    "        \"\"\"\n",
    "        return self.rank_list\n",
    "\n",
    "    def get_word_frequency_distribution(self):\n",
    "        \"\"\"Method to fetch the word frequency distribution in the given text.\n",
    "        :return: Dictionary (defaultdict) of the format `word -> frequency`.\n",
    "        \"\"\"\n",
    "        return self.frequency_dist\n",
    "\n",
    "    def get_word_degrees(self):\n",
    "        \"\"\"Method to fetch the degree of words in the given text. Degree can be\n",
    "        defined as sum of co-occurances of the word with other words in the\n",
    "        given text.\n",
    "        :return: Dictionary (defaultdict) of the format `word -> degree`.\n",
    "        \"\"\"\n",
    "        return self.degree\n",
    "\n",
    "    def _build_frequency_dist(self, phrase_list):\n",
    "        \"\"\"Builds frequency distribution of the words in the given body of text.\n",
    "        :param phrase_list: List of List of strings where each sublist is a\n",
    "                            collection of words which form a contender phrase.\n",
    "        \"\"\"\n",
    "        self.frequency_dist = Counter(chain.from_iterable(phrase_list))\n",
    "\n",
    "    def _build_word_co_occurance_graph(self, phrase_list):\n",
    "        \"\"\"Builds the co-occurance graph of words in the given body of text to\n",
    "        compute degree of each word.\n",
    "        :param phrase_list: List of List of strings where each sublist is a\n",
    "                            collection of words which form a contender phrase.\n",
    "        \"\"\"\n",
    "        co_occurance_graph = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "        for phrase in phrase_list:\n",
    "            # For each phrase in the phrase list, count co-occurances of the\n",
    "            # word with other words in the phrase.\n",
    "            #\n",
    "            # Note: Keep the co-occurances graph as is, to help facilitate its\n",
    "            # use in other creative ways if required later.\n",
    "            for (word, coword) in product(phrase, phrase):\n",
    "                co_occurance_graph[word][coword] += 1\n",
    "        self.degree = defaultdict(lambda: 0)\n",
    "        for key in co_occurance_graph:\n",
    "            self.degree[key] = sum(co_occurance_graph[key].values())\n",
    "\n",
    "    def _build_ranklist(self, phrase_list):\n",
    "        \"\"\"Method to rank each contender phrase using the formula\n",
    "              phrase_score = sum of scores of words in the phrase.\n",
    "              word_score = d(w)/f(w) where d is degree and f is frequency.\n",
    "        :param phrase_list: List of List of strings where each sublist is a\n",
    "                            collection of words which form a contender phrase.\n",
    "        \"\"\"\n",
    "        self.rank_list = []\n",
    "        for phrase in phrase_list:\n",
    "            rank = 0.0\n",
    "            for word in phrase:\n",
    "                if self.metric == Metric.DEGREE_TO_FREQUENCY_RATIO:\n",
    "                    rank += 1.0 * self.degree[word] / self.frequency_dist[word]\n",
    "                elif self.metric == Metric.WORD_DEGREE:\n",
    "                    rank += 1.0 * self.degree[word]\n",
    "                else:\n",
    "                    rank += 1.0 * self.frequency_dist[word]\n",
    "            self.rank_list.append((rank, \" \".join(phrase)))\n",
    "        self.rank_list.sort(reverse=True)\n",
    "        self.ranked_phrases = [ph[1] for ph in self.rank_list]\n",
    "\n",
    "    def _generate_phrases(self, sentences):\n",
    "        \"\"\"Method to generate contender phrases given the sentences of the text\n",
    "        document.\n",
    "        :param sentences: List of strings where each string represents a\n",
    "                          sentence which forms the text.\n",
    "        :return: Set of string tuples where each tuple is a collection\n",
    "                 of words forming a contender phrase.\n",
    "        \"\"\"\n",
    "        phrase_list = set()\n",
    "        # Create contender phrases from sentences.\n",
    "        for sentence in sentences:\n",
    "            word_list = [word.lower() for word in wordpunct_tokenize(sentence)]\n",
    "            phrase_list.update(self._get_phrase_list_from_words(word_list))\n",
    "        return phrase_list\n",
    "\n",
    "    def _get_phrase_list_from_words(self, word_list):\n",
    "        \"\"\"Method to create contender phrases from the list of words that form\n",
    "        a sentence by dropping stopwords and punctuations and grouping the left\n",
    "        words into phrases. Only phrases in the given length range (both limits\n",
    "        inclusive) would be considered to build co-occurrence matrix. Ex:\n",
    "        Sentence: Red apples, are good in flavour.\n",
    "        List of words: ['red', 'apples', \",\", 'are', 'good', 'in', 'flavour']\n",
    "        List after dropping punctuations and stopwords.\n",
    "        List of words: ['red', 'apples', *, *, good, *, 'flavour']\n",
    "        List of phrases: [('red', 'apples'), ('good',), ('flavour',)]\n",
    "        List of phrases with a correct length:\n",
    "        For the range [1, 2]: [('red', 'apples'), ('good',), ('flavour',)]\n",
    "        For the range [1, 1]: [('good',), ('flavour',)]\n",
    "        For the range [2, 2]: [('red', 'apples')]\n",
    "        :param word_list: List of words which form a sentence when joined in\n",
    "                          the same order.\n",
    "        :return: List of contender phrases that are formed after dropping\n",
    "                 stopwords and punctuations.\n",
    "        \"\"\"\n",
    "        groups = groupby(word_list, lambda x: x not in self.to_ignore)\n",
    "        phrases = [tuple(group[1]) for group in groups if group[0]]\n",
    "        return list(\n",
    "            filter(\n",
    "                lambda x: self.min_length <= len(x) <= self.max_length, phrases\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "source": [
    "# Utilities"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "from string import punctuation\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Initialize all required stemmers once.\n",
    "stemmers = {\n",
    "    'english': SnowballStemmer('english'),\n",
    "    'french': SnowballStemmer('french'),\n",
    "    'spanish': SnowballStemmer('spanish'),\n",
    "    'portuguese': SnowballStemmer('portuguese')\n",
    "}\n",
    "\n",
    "def preprocess(lis, language):\n",
    "    \"\"\"\n",
    "    Function which applies stemming to a \n",
    "    lowercase version of each string of the list,\n",
    "    which has all punctuation removed.\n",
    "    \"\"\"\n",
    "    return list(map(stemmers[language].stem, \n",
    "           map(lambda s: s.translate(str.maketrans('', '', punctuation)),\n",
    "           map(str.lower, lis))))\n",
    "\n",
    "\n",
    "def rreplace(s, old, new, occurrence):\n",
    "    \"\"\"\n",
    "    Function which replaces a string occurence\n",
    "    in a string from the end of the string.\n",
    "    \"\"\"\n",
    "    return new.join(s.rsplit(old, occurrence))\n",
    "\n",
    "def clear_screen():\n",
    "    \"\"\"\n",
    "    Function which clears the output of the terminal \n",
    "    by using the platform specific system call.\n",
    "    \"\"\"\n",
    "    if platform.system() == 'Windows':\n",
    "        os.system('cls')\n",
    "    else:\n",
    "        os.system('clear') # Linux/OS X.\n",
    "    return\n"
   ]
  },
  {
   "source": [
    "# Keyphrase extraction models"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: rake-nltk in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (1.0.4)\n",
      "Requirement already satisfied: nltk in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from rake-nltk) (3.4.5)\n",
      "Requirement already satisfied: six in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from nltk->rake-nltk) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install rake-nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pke\n",
    "import spacy\n",
    "import pytextrank\n",
    "from string import printable\n",
    "from statistics import mean\n",
    "from operator import itemgetter\n",
    "from itertools import islice, combinations\n",
    "from nltk import sent_tokenize\n",
    "from rake_nltk import Rake\n",
    "from spacy.language import Language\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "\n",
    "def tfidfvectorizer(text, ngram_range = (1, 3), top_n = 10):\n",
    "    # Tokenize the text into sentences.\n",
    "    sentences = sent_tokenize(text)\n",
    "    vectorizer = TfidfVectorizer (\n",
    "        stop_words = 'english', \n",
    "        ngram_range = ngram_range\n",
    "    )\n",
    "    # Vectorizer fits and transform the sentences.\n",
    "    vectorizer.fit_transform(sentences)\n",
    "    results = {\n",
    "        key: val \n",
    "        for key, val in sorted (\n",
    "            vectorizer.vocabulary_.items(), \n",
    "            key = lambda item: item[1],\n",
    "            reverse = True\n",
    "        )\n",
    "    }\n",
    "    return list(islice(results, top_n))\n",
    "\n",
    "def keybert(text, ngram_range = (1, 3), top_n = 10, method = None, diversity = 0.5):\n",
    "    # Initialize the keybert model using the pretrained sentence transformer model.\n",
    "    # This call takes some time on the first execution.\n",
    "    model = KeyBERT('distiluse-base-multilingual-cased-v2')\n",
    "\n",
    "    # Returned the extracted keywords based on the specified arguments. \n",
    "    return [\n",
    "        keyphrase for (keyphrase, _) in \n",
    "        model.extract_keywords (\n",
    "            text, \n",
    "            keyphrase_ngram_range = ngram_range,\n",
    "            stop_words = 'english',\n",
    "            top_n = top_n,\n",
    "            nr_candidates = 2 * top_n,\n",
    "            use_maxsum = True if method == 'maxsum' else False,\n",
    "            use_mmr = True if method == 'mmr' else False,\n",
    "            diversity = diversity\n",
    "    )]\n",
    "\n",
    "\n",
    "def singlerank(text, top_n = 10):\n",
    "    # Clean the text from non-printable characters.\n",
    "    text = ''.join(word for word in text if word in printable)\n",
    "\n",
    "    # Initialize the keyphrase extraction model.\n",
    "    extractor = pke.unsupervised.SingleRank()\n",
    "\n",
    "    # Load the content of the document and preprocess it with spacy.\n",
    "    # Then, select the keyphrase candidates from the document,\n",
    "    # and weight them using a random walk algorithm.\n",
    "    extractor.load_document(input = text, language = 'en')\n",
    "    extractor.candidate_selection()\n",
    "    extractor.candidate_weighting()\n",
    "    \n",
    "    # Return the n-highest scored candidates.\n",
    "    return [\n",
    "        keyphrase for (keyphrase, score)\n",
    "        in extractor.get_n_best(n = top_n, redundancy_removal = True)\n",
    "    ]\n",
    "    \n",
    "def rake(text, top_n = 10):\n",
    "    # Clean the text from non-printable characters.\n",
    "    text = ''.join(word for word in text if word in printable)\n",
    "\n",
    "    # Uses all english stopwords and punctuation from NLTK.\n",
    "    r = Rake()\n",
    "    r.extract_keywords_from_text(text)\n",
    "    return [keyphrase for (score, keyphrase) in r.get_ranked_phrases_with_scores()[:top_n]]\n",
    "\n",
    "def yake(text, top_n = 10, n = 3, dedupLim = 0.9, dedupFunc = 'seqm', windowsSize = 1):\n",
    "    # Initialize the keyword extractor object and its parameters.\n",
    "    kw_extractor = KeywordExtractor (\n",
    "        top = top_n,\n",
    "        n = n,\n",
    "        dedupLim = dedupLim,\n",
    "        dedupFunc = dedupFunc,\n",
    "        windowsSize = windowsSize\n",
    "    )\n",
    "    # Return the extracted keywords, in a list.\n",
    "    return [keyword for (keyword, score) in kw_extractor.extract_keywords(text)]"
   ]
  },
  {
   "source": [
    "# Apply models on your own data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/sdeshpande/Desktop/bioinformatices/bioinformatics_title.txt\", 'r') as f:\n",
    "    bio_titles = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "20140"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "len(bio_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_titles_trial = bio_titles[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_titles_trial = [x.strip(\"\\n\") for x in bio_titles_trial]\n",
    "text = \". \".join(bio_titles_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"Absence of surface expression of feline infectious peritonitis virus (FIPV) antigens on infected cells isolated from cats with FIP. Correlation between antimicrobial consumption and incidence of health-care- associated infections due to methicillin- resistant Staphylococcus aureus and vancomycin-resistant enterococci at a university hospital in Taiwan from 2000 to 2010. Laboratory-based surveillance of hospital-acquired respiratory virus infection in a tertiary care hospital. Pneumonie virale sévère de l'immunocompétent Viral pneumonia in immunocompetent patients. Microheterogeneity of S-glycoprotein of mouse hepatitis virus temperature-sensitive mutants. immunity to pathogens taught by specialized human dendritic cell subsets. The RNA pseudoknots in foot-and-mouth disease virus are dispensable for genome replication but essential for the production of infectious virus. 2 3. Enhancement of feline infectious peritonitis virus Type I infection in cell cultures using low-speed centrifugation. Intervention time series analysis of crime rates: The case of sentence reform in Virginia. Changes in salivary analytes in canine parvovirus: A high-resolution quantitative proteomic study\""
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_extraction(text):\n",
    "    # Create all ngrams with range (1, 3) for the text.\n",
    "    ngrams = {\n",
    "        '1-tfidfvectorizer': tfidfvectorizer(text, top_n = 10),\n",
    "        '2-keybert-maxsum': keybert(text, top_n = 10, method = 'maxsum', diversity = 0.7),\n",
    "        '3-keybert-mmr': keybert(text, top_n = 10, method = 'mmr', diversity = 0.7),\n",
    "        '4-singlerank': singlerank(text, top_n = 10),\n",
    "        '5-rake': rake(text, top_n = 10),\n",
    "        '6-yake-seqm': yake(text, top_n = 10, dedupFunc = 'seqm'),\n",
    "    }\n",
    "\n",
    "    return ngrams\n",
    "\n",
    "    # # Write ngrams from each method to a json file.\n",
    "    # with open(r'C:\\Users\\USER\\Desktop\\ngrams.json', 'w',\n",
    "    #     encoding = 'utf-8-sig', errors = 'ignore') as file:\n",
    "    #     file.write(json.dumps(texts, indent = 4, separators = (',', ':')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'1-tfidfvectorizer': ['virus type infection',\n",
       "  'virus type',\n",
       "  'virus temperature sensitive',\n",
       "  'virus temperature',\n",
       "  'virus infection tertiary',\n",
       "  'virus infection',\n",
       "  'virus fipv antigens',\n",
       "  'virus fipv',\n",
       "  'virus dispensable genome',\n",
       "  'virus dispensable'],\n",
       " '2-keybert-maxsum': ['cells isolated cats',\n",
       "  'virus infection tertiary',\n",
       "  'respiratory virus infection',\n",
       "  'isolated cats',\n",
       "  'immunocompétent viral pneumonia',\n",
       "  'viral pneumonia immunocompetent',\n",
       "  'fip correlation antimicrobial',\n",
       "  'cats fip correlation',\n",
       "  'expression feline infectious',\n",
       "  'isolated cats fip'],\n",
       " '3-keybert-mmr': ['virus enhancement feline',\n",
       "  '2000',\n",
       "  'university hospital taiwan',\n",
       "  'absence surface expression',\n",
       "  'correlation',\n",
       "  'cells isolated',\n",
       "  'laboratory based surveillance',\n",
       "  'using low speed',\n",
       "  'antimicrobial consumption incidence',\n",
       "  'case sentence reform'],\n",
       " '4-singlerank': ['feline infectious peritonitis virus type',\n",
       "  'respiratory virus infection',\n",
       "  'mouse hepatitis virus temperature',\n",
       "  'infectious virus',\n",
       "  'specialized human dendritic cell subsets',\n",
       "  'mouth disease virus',\n",
       "  'infected cells',\n",
       "  'methicillin- resistant staphylococcus aureus',\n",
       "  'infections due',\n",
       "  'intervention time series analysis'],\n",
       " '5-rake': ['pneumonie virale svre de l',\n",
       "  'specialized human dendritic cell subsets',\n",
       "  'feline infectious peritonitis virus type',\n",
       "  'cell cultures using low',\n",
       "  'feline infectious peritonitis virus',\n",
       "  'resolution quantitative proteomic study',\n",
       "  'intervention time series analysis',\n",
       "  'mouse hepatitis virus temperature',\n",
       "  'acquired respiratory virus infection',\n",
       "  'mouth disease virus'],\n",
       " '6-yake-seqm': ['cats with fip',\n",
       "  'isolated from cats',\n",
       "  'from cats with',\n",
       "  'with fip',\n",
       "  'infected cells isolated',\n",
       "  'absence of surface',\n",
       "  'antigens on infected',\n",
       "  'cells isolated from',\n",
       "  'surface expression',\n",
       "  'cats with']}"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "keyword_extraction(text)"
   ]
  }
 ]
}